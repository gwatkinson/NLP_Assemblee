{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import CamembertModel, CamembertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deputies_df = pd.read_pickle(\"../data/2017-2022/deputies.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = deputies_df.groupe_sigle.unique()\n",
    "groups_to_int = dict(zip(groups, range(len(groups))))\n",
    "int_to_groups = dict(zip(range(len(groups)), groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "camembert_model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: tokenizer(\n",
    "    x, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob(\"../data/2017-2022/interventions/*.json\")\n",
    "# file_stats = [os.stat(file).st_size for file in files]\n",
    "# no_empty_files = [files[i] for i in range(len(files)) if file_stats[i] > 63]\n",
    "# slug_files = [file.split(\"/\")[-1].split(\".\")[0] for file in no_empty_files]\n",
    "# file_dict = dict(zip(slug_files, no_empty_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "\n",
    "# for dep in slug_files:\n",
    "#     tmp = deputies_df[deputies_df.slug == dep]\n",
    "\n",
    "#     with open(file_dict[dep], \"r\", encoding=\"utf-8\") as f:\n",
    "#         ints = json.load(f)\n",
    "\n",
    "#     ints_list = [\n",
    "#         {\n",
    "#             \"intervention\": bs4.BeautifulSoup(intervention[\"intervention\"]).text,\n",
    "#             \"nb_mots\": intervention[\"nb_mots\"],\n",
    "#             \"seance_id\": intervention[\"seance_id\"],\n",
    "#             \"section_id\": intervention[\"section_id\"],\n",
    "#             \"type\": intervention[\"type\"],\n",
    "#             \"date\": intervention[\"date\"],\n",
    "#         } for i, intervention in enumerate(ints[\"interventions\"]) if i < 500\n",
    "#     ]\n",
    "\n",
    "#     tokens = tokenize([x[\"intervention\"] for x in ints_list])\n",
    "\n",
    "#     value = {\n",
    "#         \"nom_circo\": tmp[\"nom_circo\"].iloc[0],\n",
    "#         \"ancien_depute\": tmp[\"ancien_depute\"].iloc[0],\n",
    "#         \"groupe_sigle\": tmp[\"groupe_sigle\"].iloc[0],\n",
    "#         \"slug\": tmp[\"slug\"].iloc[0],\n",
    "#         \"nom\": tmp[\"nom\"].iloc[0],\n",
    "#         \"nb_mandats\": tmp[\"nb_mandats\"].iloc[0],\n",
    "#         \"nb_interventions\": ints[\"last_result\"],\n",
    "#         \"nb_ints\": len(ints_list),\n",
    "#         \"interventions\": ints_list,\n",
    "#         \"tokens\": tokens,\n",
    "#     }\n",
    "\n",
    "#     data.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543904072"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"../data/2017-2022/tokenized.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(data, f)\n",
    "# os.stat(\"../data/2017-2022/tokenized.pkl\").st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [x[\"nb_ints\"] for x in data]\n",
    "# all_input_ids = [x[\"tokens\"][\"input_ids\"] for x in data]\n",
    "# all_masks = [x[\"tokens\"][\"attention_mask\"] for x in data]\n",
    "# labels = [groups_to_int[x[\"groupe_sigle\"]] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 512])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_input_ids[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_inputs = nn.utils.rnn.pad_sequence(all_input_ids, batch_first=True, padding_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_masks = nn.utils.rnn.pad_sequence(all_masks, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(padded_inputs, \"../data/2017-2022/padded_inputs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list = {\n",
    "#     \"tokens\": padded_inputs,\n",
    "#     \"masks\": padded_masks,\n",
    "#     \"interventions_masks\": torch.all(padded_masks== 0, dim=2),\n",
    "#     \"labels\": labels\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(tokens_list, \"../data/2017-2022/tokens_list.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/2017-2022/tokens_lists.pkl\", \"rb\") as f:\n",
    "#     tokens_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list = torch.load(\"../data/2017-2022/data_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_frac = 0.25\n",
    "# X, y = np.arange(len(tokens_list[\"labels\"])), tokens_list[\"labels\"]\n",
    "# train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "#     X, y,\n",
    "#     stratify=y,\n",
    "#     test_size=0.25\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokens_list = {\n",
    "#     \"tokens\": tokens_list[\"tokens\"][train_idx],\n",
    "#     \"masks\": tokens_list[\"masks\"][train_idx],\n",
    "#     \"interventions_masks\": tokens_list[\"interventions_masks\"][train_idx],\n",
    "#     \"labels\": tokens_list[\"labels\"][train_idx]\n",
    "# }\n",
    "# test_tokens_list = {\n",
    "#     \"tokens\": tokens_list[\"tokens\"][test_idx],\n",
    "#     \"masks\": tokens_list[\"masks\"][test_idx],\n",
    "#     \"interventions_masks\": tokens_list[\"interventions_masks\"][test_idx],\n",
    "#     \"labels\": tokens_list[\"labels\"][test_idx]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, count = np.unique(test_tokens_list[\"labels\"], return_counts=True)\n",
    "# test_party_count = pd.DataFrame({\n",
    "#     \"party\": unique,\n",
    "#     \"count\": count\n",
    "# })\n",
    "# test_party_count[\"freq\"] = test_party_count[\"count\"]/test_party_count[\"count\"].sum()\n",
    "# test_party_count[\"party_name\"] = test_party_count[\"party\"].apply(lambda x: int_to_groups[x])\n",
    "# test_party_count[\"type\"] = \"test\"\n",
    "\n",
    "\n",
    "# unique, count = np.unique(train_tokens_list[\"labels\"], return_counts=True)\n",
    "# train_party_count = pd.DataFrame({\n",
    "#     \"party\": unique,\n",
    "#     \"count\": count\n",
    "# })\n",
    "# train_party_count[\"freq\"] = train_party_count[\"count\"]/train_party_count[\"count\"].sum()\n",
    "# train_party_count[\"party_name\"] = train_party_count[\"party\"].apply(lambda x: int_to_groups[x])\n",
    "# train_party_count[\"type\"] = \"train\"\n",
    "\n",
    "\n",
    "# unique, count = np.unique(tokens_list[\"labels\"], return_counts=True)\n",
    "# party_count = pd.DataFrame({\n",
    "#     \"party\": unique,\n",
    "#     \"count\": count\n",
    "# })\n",
    "# party_count[\"freq\"] = party_count[\"count\"]/party_count[\"count\"].sum()\n",
    "# party_count[\"party_name\"] = party_count[\"party\"].apply(lambda x: int_to_groups[x])\n",
    "# party_count[\"type\"] = \"total\"\n",
    "\n",
    "# party_count = pd.concat([party_count, train_party_count, test_party_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='party_name', ylabel='freq'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA65klEQVR4nO3deVyVdf7//+cBBGQVEUEThNxJzYSx0CntU4LamKamqankko5Lmls5zri1WOM+Nm6TgFZjWGrZfJ0RWlxSx200p9RcCRfMpcQtQeH6/eFwfh0P6EEPHLh83G+366bnfd7Xdb2uw+HwPO9rsxiGYQgAAMAk3FxdAAAAgDMRbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKl4uLqA0pafn6+TJ0/K399fFovF1eUAAAAHGIahixcvqnr16nJzu/XYzD0Xbk6ePKnw8HBXlwEAAO7AsWPHVKNGjVv2uefCjb+/v6QbL05AQICLqwEAAI64cOGCwsPDrX/Hb+WeCzcFu6ICAgIINwAAlDOOHFLCAcUAAMBUCDcAAMBUCDcAAMBU7rljbgAAKEl5eXm6du2aq8solzw9PW97mrcjCDcAADiBYRg6deqUzp8/7+pSyi03NzdFRUXJ09PzrpZDuAEAwAkKgk3VqlXl4+PDhWKLqeAiu1lZWYqIiLir149wAwDAXcrLy7MGm+DgYFeXU26FhITo5MmTun79uipUqHDHy+GAYgAA7lLBMTY+Pj4urqR8K9gdlZeXd1fLIdwAAOAk7Iq6O856/Qg3AADAVAg3AADAVAg3AADAVAg3AACUM61atdKIESNcXUaZRbgBAACmQrgBAKAcSUxM1Pr16zVnzhxZLBZZLBZ5eHho+vTpNv2+/fZbubm56fDhw5JunIk0f/58tW3bVhUrVlRUVJQ++ugjm3lOnDihbt26KSgoSMHBwerQoYMyMjJKa9Ochov4/U/MmKUO913lP83hvhET/nsn5QAAUKg5c+bowIEDatiwoaZMmSJJWrx4sZKTkzV69Ghrv6SkJD366KOqVauWte1Pf/qT3nrrLc2ZM0fvvfeeunfvroYNG6pBgwa6cuWKHn/8cT366KPasGGDPDw89Prrr6tNmzbas2fPXd8SoTQRbkpYi7ktHO67adimEqwEAGAGgYGB8vT0lI+Pj8LCwiRJffv21cSJE7Vt2zY1a9ZM165d0/vvv69p02y/jD/77LPq37+/JOm1115Tenq65s6dq3nz5unDDz+Um5ub3n33Xev1ZpKTk1WpUiWtW7dO8fHxpbuhd4HdUgAAlHPVqlXTU089paSkJEnSP/7xD129elXPPvusTb+4uDi7x/v27ZMk7dy5U4cOHZK/v7/8/Pzk5+enypUr6+rVq9ZdW+UFIzcAAJhA//791atXL82aNUvJycnq1q2bQ7eDKBilyc/PV0xMjD744AO7PiEhIU6vtyQRbgAAKGc8PT3t7r/Url07+fr6av78+frnP/+pDRs22M3373//W71797Z5/NBDD0mSmjZtqtTUVFWtWlUBAQEluwEljN1SAACUM5GRkdq6dasyMjJ09uxZ5efny93dXYmJiRo3bpxq165ttwtKkj766CMlJSXpwIED1mN0hg4dKknq2bOnqlSpog4dOmjjxo06evSo1q9fr+HDh+v48eOlvYl3hXADAEA5M3r0aLm7uys6OlohISHKzMyUJPXr10+5ubnq27dvofNNnjxZH374oRo3bqwlS5bogw8+UHR0tKQbdzTfsGGDIiIi1KlTJzVo0EB9+/bVL7/8Uu5GctgtBQBAOVO3bl1t2bLFrj0rK0seHh42u55+rXr16kpLSytyuWFhYVqyZInT6nQVwg0AAOVcTk6Ojh07pj/96U/q2rWrQkNDXV2SS7FbCgCAcm7ZsmWqV6+esrOz9ec//9nV5bgcIzcAAJRziYmJSkxMvGUfwzBKp5gygJEbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABQKhITE9WxY8cSXw+nggMAUIJixiwttXXtnFb4lYlvpVWrVmrSpIlmz55dovOUJkZuAACAqRBuAAC4RyUmJmr9+vWaM2eOLBaLLBaLMjIytH79ejVr1kxeXl6qVq2aXn31VV2/fv2W8+Tl5alfv36KiopSxYoVVa9ePc2ZM8cl28VuKQAA7lFz5szRgQMH1LBhQ02ZMkWSlJeXp3bt2ikxMVFLly7V/v37NWDAAHl7e2vSpEmFzhMSEqL8/HzVqFFDy5cvV5UqVbR582a9+OKLqlatmrp27Vqq20W4AQDgHhUYGChPT0/5+PgoLCxMkjR+/HiFh4frnXfekcViUf369XXy5Em98sormjBhQqHzSJK7u7smT55sfRwVFaXNmzdr+fLlpR5u2C0FAACs9u3bp7i4OFksFmtbixYtdOnSJR0/fvyW8y5YsECxsbEKCQmRn5+f/va3vykzM7OkS7ZDuAEAAFaGYdgEm4I2SXbtv7Z8+XK9/PLL6tu3r9LS0rR792698MILys3NLdF6C8NuKQAA7mGenp7Ky8uzPo6OjtaKFStsQs7mzZvl7++v++67r9B5JGnjxo1q3ry5Bg8ebG07fPhwKWyBPUZuAAC4h0VGRmrr1q3KyMjQ2bNnNXjwYB07dkzDhg3T/v379emnn2rixIkaOXKk3NzcCp0nPz9ftWvX1o4dO7R27VodOHBAf/rTn7R9+3aXbBPhBgCAe9jo0aPl7u6u6OhohYSE6Nq1a1qzZo22bdumBx98UIMGDVK/fv30xz/+sch5MjMzNWjQIHXq1EndunXTww8/rHPnztmM4pQmi1GwI+0eceHCBQUGBio7O1sBAQHW9uJcQXKV/zSH+3YPCrh9p//ZNGyTw30BAGXH1atXdfToUUVFRcnb29vV5ZRbt3odi/r7XRhGbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgFNERkZq9uzZri6Du4IDAFCSMqc0KrV1RUz4b7HnadWqlZo0aeKUULJ9+3b5+vre9XLuFuEGAAAUyTAM5eXlycPj9pEhJCSkFCq6PXZLAQBwj0pMTNT69es1Z84cWSwWWSwWpaSkyGKxaO3atYqNjZWXl5c2btyow4cPq0OHDgoNDZWfn59+85vf6PPPP7dZ3s27pSwWi959910988wz8vHxUZ06dbR69eoS3y7CDQAA96g5c+YoLi5OAwYMUFZWlrKyshQeHi5JGjt2rKZOnap9+/apcePGunTpktq1a6fPP/9cu3btUkJCgtq3b6/MzMxbrmPy5Mnq2rWr9uzZo3bt2qlnz5766aefSnS7CDcAANyjAgMD5enpKR8fH4WFhSksLEzu7u6SpClTpqh169aqVauWgoOD9eCDD2rgwIFq1KiR6tSpo9dff13333//bUdiEhMT1b17d9WuXVtvvvmmLl++rG3btpXodhFuAACAndjYWJvHly9f1tixYxUdHa1KlSrJz89P+/fvv+3ITePGja3/9/X1lb+/v06fPl0iNRfggGIAAGDn5rOexowZo7Vr12r69OmqXbu2KlasqC5duig3N/eWy6lQoYLNY4vFovz8fKfX+2uEGwAA7mGenp7Ky8u7bb+NGzcqMTFRzzzzjCTp0qVLysjIKOHq7gy7pQAAuIdFRkZq69atysjI0NmzZ4scValdu7ZWrlyp3bt365tvvlGPHj1KfATmTrk83MybN09RUVHy9vZWTEyMNm7c6NB8mzZtkoeHh5o0aVKyBQIAYGKjR4+Wu7u7oqOjFRISUuQxNLNmzVJQUJCaN2+u9u3bKyEhQU2bNi3lah3j0t1SqampGjFihObNm6cWLVpo4cKFatu2rfbu3auIiIgi58vOzlbv3r31xBNP6McffyzFigEAKJ47uWpwaapbt662bNli05aYmGjXLzIyUl9++aVN25AhQ2we37ybyjAMu+WcP3/+juosDpeO3MycOVP9+vVT//791aBBA82ePVvh4eGaP3/+LecbOHCgevToobi4uNuuIycnRxcuXLCZAACAebks3OTm5mrnzp2Kj4+3aY+Pj9fmzZuLnC85OVmHDx/WxIkTHVrP1KlTFRgYaJ0KLk4EAADMyWXh5uzZs8rLy1NoaKhNe2hoqE6dOlXoPAcPHtSrr76qDz74wKF7XEjSuHHjlJ2dbZ2OHTt217UDAICyy+WnglssFpvHhmHYtUlSXl6eevToocmTJ6tu3boOL9/Ly0teXl53XScAACgfXBZuqlSpInd3d7tRmtOnT9uN5kjSxYsXtWPHDu3atUtDhw6VJOXn58swDHl4eCgtLU3/93//Vyq1AwCAsstlu6U8PT0VExOj9PR0m/b09HQ1b97crn9AQID++9//avfu3dZp0KBBqlevnnbv3q2HH364tEoHAABlmEt3S40cOVK9evVSbGys4uLitGjRImVmZmrQoEGSbhwvc+LECS1dulRubm5q2LChzfxVq1aVt7e3XTsAALh3uTTcdOvWTefOndOUKVOUlZWlhg0bas2aNapZs6YkKSsr67Y35AIAAPg1i1HYFXZM7MKFCwoMDFR2drYCAgKs7TFjljq8jFX+0xzu2z0o4Pad/mfTsE0O9wUAlB1Xr17V0aNHrVfcx5251etY1N/vwrj89gsAAADO5PJTwQEAMLMWc1uU2rruZA9Aq1at1KRJE82ePdspNSQmJur8+fP65JNPnLK8O8HIDQAAMBXCDQAA96jExEStX79ec+bMkcVikcViUUZGhvbu3at27drJz89PoaGh6tWrl86ePWud7+OPP1ajRo1UsWJFBQcH68knn9Tly5c1adIkLVmyRJ9++ql1eevWrSv17SLcAABwj5ozZ47i4uI0YMAAZWVlKSsrSxUqVFDLli3VpEkT7dixQ//617/0448/qmvXrpJunMncvXt39e3bV/v27dO6devUqVMnGYah0aNHq2vXrmrTpo11eYVdu66kccwNAAD3qMDAQHl6esrHx0dhYWGSpAkTJqhp06Z68803rf2SkpIUHh6uAwcO6NKlS7p+/bo6depkvXRLo0aNrH0rVqyonJwc6/JcgXADAACsdu7cqa+++kp+fn52zx0+fFjx8fF64okn1KhRIyUkJCg+Pl5dunRRUFCQC6otHLulAACAVX5+vtq3b29zu6Pdu3fr4MGDeuyxx+Tu7q709HT985//VHR0tObOnat69erp6NGjri7dinADAMA9zNPTU3l5edbHTZs21XfffafIyEjVrl3bZvL19ZUkWSwWtWjRQpMnT9auXbvk6empVatWFbo8VyDcAABwD4uMjNTWrVuVkZGhs2fPasiQIfrpp5/UvXt3bdu2TUeOHFFaWpr69u2rvLw8bd26VW+++aZ27NihzMxMrVy5UmfOnFGDBg2sy9uzZ4++//57nT17VteuXSv1beKYGwAASlBZv7XO6NGj1adPH0VHR+uXX37R0aNHtWnTJr3yyitKSEhQTk6OatasqTZt2sjNzU0BAQHasGGDZs+erQsXLqhmzZqaMWOG2rZtK0kaMGCA1q1bp9jYWF26dElfffWVWrVqVarbRLgBAOAeVrduXW3ZssWufeXKlYX2b9Cggf71r38VubyQkBClpaU5rb47wW4pAABgKoQbAABgKoQbAABgKoQbAABgKoQbAACcxDAMV5dQrjnr9SPcAABwlypUqCBJunLliosrKd9yc3MlSe7u7ne1HE4FBwDgLrm7u6tSpUo6ffq0JMnHx0cWi8XFVZUv+fn5OnPmjHx8fOThcXfxhHADAIATFNwFuyDgoPjc3NwUERFx18GQcAMAgBNYLBZVq1ZNVatWdcktB8zA09NTbm53f8QM4QYAACdyd3e/62NGcHc4oBgAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJiKy8PNvHnzFBUVJW9vb8XExGjjxo1F9v3666/VokULBQcHq2LFiqpfv75mzZpVitUCAICyzsOVK09NTdWIESM0b948tWjRQgsXLlTbtm21d+9eRURE2PX39fXV0KFD1bhxY/n6+urrr7/WwIED5evrqxdffNEFWwAAAMoai2EYhqtW/vDDD6tp06aaP3++ta1Bgwbq2LGjpk6d6tAyOnXqJF9fX7333nsO9b9w4YICAwOVnZ2tgIAAa3vMmKUO173Kf5rDfbsHBdy+0/9sGrbJ4b4AANxLivr7XRiX7ZbKzc3Vzp07FR8fb9MeHx+vzZs3O7SMXbt2afPmzWrZsmWRfXJycnThwgWbCQAAmJfLws3Zs2eVl5en0NBQm/bQ0FCdOnXqlvPWqFFDXl5eio2N1ZAhQ9S/f/8i+06dOlWBgYHWKTw83Cn1AwCAssnlBxRbLBabx4Zh2LXdbOPGjdqxY4cWLFig2bNna9myZUX2HTdunLKzs63TsWPHnFI3AAAom1x2QHGVKlXk7u5uN0pz+vRpu9Gcm0VFRUmSGjVqpB9//FGTJk1S9+7dC+3r5eUlLy8v5xQNAADKPJeN3Hh6eiomJkbp6ek27enp6WrevLnDyzEMQzk5Oc4uDwAAlFMuPRV85MiR6tWrl2JjYxUXF6dFixYpMzNTgwYNknRjl9KJEye0dOmNM5n++te/KiIiQvXr15d047o306dP17Bhw1y2DQAAoGxxabjp1q2bzp07pylTpigrK0sNGzbUmjVrVLNmTUlSVlaWMjMzrf3z8/M1btw4HT16VB4eHqpVq5beeustDRw40FWbAAAAyhiXXufGFbjODQAA5U+5uM4NAABASSDcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAU/Eo7gwjR450uO/MmTOLu3gAAIC7Uuxws2vXLv3nP//R9evXVa9ePUnSgQMH5O7urqZNm1r7WSwW51UJAADgoGKHm/bt28vf319LlixRUFCQJOnnn3/WCy+8oEcffVSjRo1yepEAAACOKvYxNzNmzNDUqVOtwUaSgoKC9Prrr2vGjBlOLQ4AAKC4ih1uLly4oB9//NGu/fTp07p48aJTigIAALhTxQ43zzzzjF544QV9/PHHOn78uI4fP66PP/5Y/fr1U6dOnUqiRgAAAIcV+5ibBQsWaPTo0Xr++ed17dq1Gwvx8FC/fv00bdo0pxcIAABQHMUONz4+Ppo3b56mTZumw4cPyzAM1a5dW76+viVRHwAAQLHc8UX8srKylJWVpbp168rX11eGYTizLgAAgDtS7HBz7tw5PfHEE6pbt67atWunrKwsSVL//v05DRwAALhcscPNyy+/rAoVKigzM1M+Pj7W9m7duulf//qXU4sDAAAormIfc5OWlqa1a9eqRo0aNu116tTRDz/84LTCAAAA7kSxR24uX75sM2JT4OzZs/Ly8nJKUQAAAHeq2OHmscce09KlS62PLRaL8vPzNW3aND3++ONOLQ4AAKC4ir1batq0aWrVqpV27Nih3NxcjR07Vt99951++uknbdq0qSRqBAAAcFixR26io6O1Z88eNWvWTK1bt9bly5fVqVMn7dq1S7Vq1SqJGgEAABxWrJGba9euKT4+XgsXLtTkyZNLqiYAAIA7VqyRmwoVKujbb7+VxWIpqXoAAADuSrF3S/Xu3VuLFy8uiVoAAADuWrEPKM7NzdW7776r9PR0xcbG2t1TaubMmU4rDgAAoLgcCjd79uxRw4YN5ebmpm+//VZNmzaVJB04cMCmH7urAACAqzkUbh566CFlZWWpatWq+uGHH7R9+3YFBweXdG0AAADF5tAxN5UqVdLRo0clSRkZGcrPzy/RogAAAO6UQyM3nTt3VsuWLVWtWjVZLBbFxsbK3d290L5HjhxxaoEAAADF4VC4WbRokTp16qRDhw7ppZde0oABA+Tv71/StQEAABSbw2dLtWnTRpK0c+dODR8+nHADAADKpGKfCp6cnFwSdQAAADhFsS/iBwAAUJYRbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKm4PNzMmzdPUVFR8vb2VkxMjDZu3Fhk35UrV6p169YKCQlRQECA4uLitHbt2lKsFgAAlHUuDTepqakaMWKExo8fr127dunRRx9V27ZtlZmZWWj/DRs2qHXr1lqzZo127typxx9/XO3bt9euXbtKuXIAAFBWWQzDMFy18ocfflhNmzbV/PnzrW0NGjRQx44dNXXqVIeW8cADD6hbt26aMGGCQ/0vXLigwMBAZWdnKyAgwNoeM2apw3Wv8p/mcN/uQQG37/Q/m4ZtcrgvAAD3kqL+fhfGZSM3ubm52rlzp+Lj423a4+PjtXnzZoeWkZ+fr4sXL6py5cpF9snJydGFCxdsJgAAYF4uCzdnz55VXl6eQkNDbdpDQ0N16tQph5YxY8YMXb58WV27di2yz9SpUxUYGGidwsPD76puAABQtrn8gGKLxWLz2DAMu7bCLFu2TJMmTVJqaqqqVq1aZL9x48YpOzvbOh07duyuawYAAGWXh6tWXKVKFbm7u9uN0pw+fdpuNOdmqamp6tevnz766CM9+eSTt+zr5eUlLy+vu64XAACUDy4bufH09FRMTIzS09Nt2tPT09W8efMi51u2bJkSExP197//XU899VRJlwkAAMoZl43cSNLIkSPVq1cvxcbGKi4uTosWLVJmZqYGDRok6cYupRMnTmjp0htnMi1btky9e/fWnDlz9Mgjj1hHfSpWrKjAwECXbQcAACg7XBpuunXrpnPnzmnKlCnKyspSw4YNtWbNGtWsWVOSlJWVZXPNm4ULF+r69esaMmSIhgwZYm3v06ePUlJSSrt8AABQBrk03EjS4MGDNXjw4EKfuzmwrFu3ruQLAgAA5ZrLz5YCAABwJsINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFQ9XFwC4QsyYpQ73XeU/zeG+ERP+eyflAACciJEbAABgKozcAE7UYm4Lh/tuGrapBCsBgHsXIzcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUXB5u5s2bp6ioKHl7eysmJkYbN24ssm9WVpZ69OihevXqyc3NTSNGjCi9QgEAQLng0nCTmpqqESNGaPz48dq1a5ceffRRtW3bVpmZmYX2z8nJUUhIiMaPH68HH3ywlKsFAADlgYcrVz5z5kz169dP/fv3lyTNnj1ba9eu1fz58zV16lS7/pGRkZozZ44kKSkpyaF15OTkKCcnx/r4woULTqjcNWLGLHW47yr/aQ737R4U4HDfTcM2OdwXAABXcNnITW5urnbu3Kn4+Hib9vj4eG3evNlp65k6daoCAwOtU3h4uNOWDQAAyh6XhZuzZ88qLy9PoaGhNu2hoaE6deqU09Yzbtw4ZWdnW6djx445bdkAAKDsceluKUmyWCw2jw3DsGu7G15eXvLy8nLa8gAAQNnmspGbKlWqyN3d3W6U5vTp03ajOQAAAI5yWbjx9PRUTEyM0tPTbdrT09PVvHlzF1UFAADKO5fulho5cqR69eql2NhYxcXFadGiRcrMzNSgQYMk3The5sSJE1q69P8/S2j37t2SpEuXLunMmTPavXu3PD09FR0d7YpNAAAAZYxLw023bt107tw5TZkyRVlZWWrYsKHWrFmjmjVrSrpx0b6br3nz0EMPWf+/c+dO/f3vf1fNmjWVkZFRmqUDAIAyyuUHFA8ePFiDBw8u9LmUlBS7NsMwSrgiAABQnrn89gsAAADORLgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACm4uHqAgBHtJjbwuG+m4ZtKsFKAABlHSM3AADAVAg3AADAVNgtBaeKGbPU4b47p/UuwUoAAPcqwg1cJnNKI8c7BwWUXCEAAFNhtxQAADAVRm4A4A5xFh9QNjFyAwAATIVwAwAATIVwAwAATIVjbgCYHpcoAO4thBsA+BUuUQCUf+yWAgAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApsLZUgAApyvOWWcRE/5bgpXgXkS4AQA4pDjXC1rlX4KFALdBuAEAEysPN/csDzVK5adOcMwNAAAwGUZuAACmUrzdZ9McXzBXpC43CDcAUAZw/yvAeQg3AFDOcP8r4NY45gYAAJgK4QYAAJgKu6WAMqqkDorsXozdFJzOCpQMfr9LFiM3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVLgrOIBSkTmlkcN9Iyb8twQrAeBsZe332+UjN/PmzVNUVJS8vb0VExOjjRs33rL/+vXrFRMTI29vb91///1asGBBKVUKAADKA5eO3KSmpmrEiBGaN2+eWrRooYULF6pt27bau3evIiIi7PofPXpU7dq104ABA/T+++9r06ZNGjx4sEJCQtS5c2cXbAFwb4sZs9Thvqv8S7AQAE5Xnn+/XTpyM3PmTPXr10/9+/dXgwYNNHv2bIWHh2v+/PmF9l+wYIEiIiI0e/ZsNWjQQP3791ffvn01ffr0Uq4cAACUVS4bucnNzdXOnTv16quv2rTHx8dr8+bNhc6zZcsWxcfH27QlJCRo8eLFunbtmipUqGA3T05OjnJycqyPs7OzJUkXLlyw6ZeX84vDtV+skOdw3+u/XHe478013YwaHXO7GqXyUee9XKMjP8PiuJdfy3utRql81Hkv13inv98F8xmGcfvOhoucOHHCkGRs2rTJpv2NN94w6tatW+g8derUMd544w2btk2bNhmSjJMnTxY6z8SJEw1JTExMTExMTCaYjh07dtuM4fKzpSwWi81jwzDs2m7Xv7D2AuPGjdPIkSOtj/Pz8/XTTz8pODj4luspjgsXLig8PFzHjh1TQECAU5bpbOWhRql81EmNzlMe6qRG5ykPdVKj8zi7TsMwdPHiRVWvXv22fV0WbqpUqSJ3d3edOnXKpv306dMKDQ0tdJ6wsLBC+3t4eCg4OLjQeby8vOTl5WXTVqlSpTsv/BYCAgLK9BtNKh81SuWjTmp0nvJQJzU6T3mokxqdx5l1BgYGOtTPZQcUe3p6KiYmRunp6Tbt6enpat68eaHzxMXF2fVPS0tTbGxsocfbAACAe49Lz5YaOXKk3n33XSUlJWnfvn16+eWXlZmZqUGDBkm6sUupd+/e1v6DBg3SDz/8oJEjR2rfvn1KSkrS4sWLNXr0aFdtAgAAKGNcesxNt27ddO7cOU2ZMkVZWVlq2LCh1qxZo5o1a0qSsrKylJmZae0fFRWlNWvW6OWXX9Zf//pXVa9eXX/5y19cfo0bLy8vTZw40W73V1lSHmqUyked1Og85aFOanSe8lAnNTqPK+u0GIYj51QBAACUDy6//QIAAIAzEW4AAICpEG4AAICpEG4AAICp3PPhJjExUR07diz0ucjISFksFlksFlWsWFH169fXtGnTbO5rkZGRYe1z8/Tvf/9bkpSSkiKLxaIGDRrYrWP58uWyWCzy8/OTxWKxngb/a4MHD5bFYlFiYqK17dixY+rXr5+qV68uT09P1axZU8OHD9e5c+ds5m3VqpW1Hi8vL913331q3769Vq5cabeeorbjww8/lCStW7dOFotFQUFBunr1qs2827Zts/b/tdOnT2vgwIGKiIiQl5eXwsLClJCQoC1btlj7bN68We3atVNQUJC8vb3VqFEjzZgxQ3l59vcq+eqrr9SuXTsFBwfLx8dH0dHRGjVqlE6cOGHX927d7XvD2U6dOqXhw4erdu3a8vb2VmhoqH77299qwYIFunLlSqF1RUZGqmvXrvryyy9tlnXz+zYwMFCPPPKIPvvsM6fWnJiYKIvForfeesum/ZNPPrF5rxiGob/97W+Ki4tTQECA/Pz89MADD2j48OE6dOiQU2u623oLfg/Onz9fanXd7NfvzVt9BhVMkyZNKvUaN2/eLHd3d7Vp08am3ZHPzJLQqlUrjRgxwq791z/bgs9qi8Uid3d3BQUF6eGHH9aUKVOs9yUscKvPB2dw9POnYKpRo4bN87Nnzy6x2hx5LQv88ssvCgoKUuXKlfXLL/b3qiqpWu/5cHM7Baep79u3T6NHj9Yf/vAHLVq0yK7f559/rqysLJspJibG+ryvr69Onz5t80ddkpKSkhQRESFJCg8P14cffmjzBrh69aqWLVtm7SNJR44cUWxsrA4cOKBly5bp0KFDWrBggb744gvFxcXpp59+slnHgAEDlJWVpUOHDmnFihWKjo7Wc889pxdffNFuO5KTk+224+ZfMH9/f61atarI7fi1zp0765tvvtGSJUt04MABrV69Wq1atbLWuGrVKrVs2VI1atTQV199pf3792v48OF644039Nxzz9mEhYULF+rJJ59UWFiYVqxYob1792rBggXKzs7WjBkz7NZd0hx9bzjDkSNH9NBDDyktLU1vvvmmdu3apc8//1wvv/yyPvvsM33++ed2dX3//fdaunSpKlWqpCeffFJvvPGG3XIL3rdbt25Vs2bN1LlzZ3377bdOrd3b21tvv/22fv7550KfNwxDPXr00EsvvaR27dopLS1Ne/bs0V/+8hdVrFhRr7/+ulPrudt6y5rw8HCb39dRo0bpgQcesGlzxbXAkpKSNGzYMH399dc2l/QocLvPTFcJCAhQVlaWjh8/rs2bN+vFF1/U0qVL1aRJE508edLV5VkV/J4XTLt27XJ1SYVasWKFGjZsqOjo6EK/VJcUl99bqqzz9/dXWFiYJKl///6aP3++0tLSNHDgQJt+wcHB1n6F8fDwUI8ePZSUlKS4uDhJ0vHjx7Vu3Tq9/PLLmjt3rpo2baojR45o5cqV6tmzpyRp5cqVCg8P1/33329d1pAhQ+Tp6am0tDRVrFhRkhQREaGHHnpItWrV0vjx4zV//nxrfx8fH2tt4eHheuSRR1S/fn317dtXXbt21ZNPPmntW6lSpVtuhyT16dNHSUlJ6t69u6QbyfzDDz/USy+9pNdee83a7/z58/r666+1bt06tWzZUpJUs2ZNNWvWTJJ0+fJlDRgwQE8//bRNKOjfv79CQ0P19NNPa/ny5erWrZuOHz+ul156SS+99JJmzZpl7RsZGanHHnvMJd+iHX1vOMPgwYPl4eGhHTt2yNfX19reqFEjde7c2SYE/rquiIgIPfbYY6pWrZomTJigLl26qF69eta+Be/bsLAwvfHGG5o7d66++uorNWzY0Gm1P/nkkzp06JCmTp2qP//5z3bPp6am6sMPP9Snn36qp59+2tp+//3364knnijR0bDC3K7essbd3d3md9bPz08eHh63/T0uSZcvX9by5cu1fft2nTp1SikpKZowYYJNn9t9ZrqKxWKx1lWtWjU1aNBA7du31wMPPKCxY8fq/fffd3GFN/z697wsW7x4sZ5//nkZhqHFixdb/7aVNEZuHGQYhtatW6d9+/bd8a0e+vXrp9TUVOsuhJSUFLVp08bmXlovvPCCkpOTrY+TkpLUt29f6+OffvpJa9eu1eDBg63BpkBYWJh69uyp1NTU2/5B6NOnj4KCgu4oSffq1UsbN260fhtbsWKFIiMj1bRpU5t+fn5+8vPz0yeffKKcnBy75aSlpencuXOFfqts37696tatq2XLlkmSPvroI+Xm5mrs2LGF1lRS9wtzhDPeG7dy7tw5paWlaciQITbB5tdudxPY4cOHyzAMffrpp4U+f+3aNf3tb3+TJKdvg7u7u958803NnTtXx48ft3t+2bJlqlevnk2w+TVn3eDWUberF7eXmpqqevXqqV69enr++eeVnJxc6iHVmapWraqePXtq9erVhe4uR+EOHz6sLVu2qGvXruratas2b96sI0eOlMq6CTe38corr8jPz09eXl56/PHHZRiGXnrpJbt+zZs3t/4xL5hu/iVo0qSJatWqpY8//liGYSglJcUmuEg3gsPXX3+tjIwM/fDDD9q0aZOef/556/MHDx6UYRiFHr8jSQ0aNNDPP/+sM2fO3HK73NzcVLduXWVkZNi0d+/e3W47bn4zVq1aVW3btlVKSook+wBWwMPDQykpKVqyZIkqVaqkFi1a6A9/+IP27NkjSTpw4IC15sLUr1/f2ufgwYMKCAhQtWrVbrldpcnR98bdOnTokAzDsBlxkW7cfLbgZ/TKK6/cchmVK1dW1apV7X7eBe9bb29vjRo1ynqMjrM988wzatKkiSZOnGj33IEDB+y2bcSIEdZt+/WxBKXlVvXi9gq+rUtSmzZtdOnSJX3xxRc2fRz5zCxL6tevr4sXL9od1+gqBZ8/BdNf/vIXV5dkJykpSW3btrUec9OmTRslJSWVyroJN7cxZswY7d69W+vXr9fjjz+u8ePHF3pjz9TUVO3evdtmcnd3t+vXt29fJScna/369bp06ZLatWtn83yVKlX01FNPacmSJUpOTtZTTz2lKlWqOFxvwbcjR77tGoZh12/WrFl22xEeHl7odqSkpOjIkSPasmVLkUONnTt31smTJ7V69WolJCRo3bp1atq0qTUY/brmW9VXWK2u5uh7w1lu3v5t27Zp9+7deuCBBwodGbtZYa9hamqqdu3apdWrV6t27dp69913VblyZafWXeDtt9/WkiVLtHfvXrvnbq5r/Pjx2r17tyZMmKBLly6VSD23c6t6UbTvv/9e27Zt03PPPSfpxpecbt262f1Rc/Qzs6wozmdraSj4/CmYfn0fxrIgLy9PS5Yssfly/vzzz2vJkiWlEmI55uY2qlSpotq1a6t27dpasWKFateurUceecTmOBXpxrEstWvXvu3yevbsqbFjx2rSpEnq3bu3PDzsfwR9+/bV0KFDJUl//etfbZ6rXbu2LBaL9u7dW+iR9Pv371dQUNBtA1FeXp4OHjyo3/zmNzbtYWFhDm1Hu3btNHDgQPXr10/t27dXcHBwkX29vb3VunVrtW7dWhMmTFD//v01ceJE6xHy+/btKzQU7N+/X9HR0ZKkunXrKjs7W1lZWWVm9MbR98bdKviZ79+/36a94Dism3dPFubcuXM6c+aMoqKibNrDw8NVp04d1alTR35+furcubP27t2rqlWrOm8D/uexxx5TQkKC/vCHP9ic+VenTh27bQsJCVFISEiJ1OGoourFrS1evFjXr1/XfffdZ20zDEMVKlSwOUjb0c9MZwkICLA740m6cWxgQEDAbefft2+fAgICbvlZV5oKPn9cwZHXcu3atTpx4oS6detm0ycvL09paWlq27ZtidbIyE0xBAUFadiwYRo9evQd7z+uXLmynn76aa1fv77QXTnSjWHc3Nxc5ebmKiEhwea54OBgtW7dWvPmzbM7re7UqVP64IMP1K1bt9t+u1iyZIl+/vnnO77pqLu7u3r16qV169YVuR1FiY6O1uXLlxUfH6/KlSsXeqbT6tWrdfDgQetBy126dJGnp2eRB3i68rRcyTnvjaIU/MzfeecdXb58+Y6WMWfOHLm5ud3y1NWWLVuqYcOGhZ5V5SxvvfWWPvvsM23evNna1r17d33//fdFHg/kSoXVi6Jdv35dS5cu1YwZM2xGFb755hvVrFlTH3zwgctqq1+/vnbs2GHXvn37drvdojc7ffq0/v73v6tjx45yc+PPpiOv5eLFi/Xcc8/Zjc717NlTixcvLvEaGbmRlJ2drd27d9u0FTU0P2TIEL399ttasWKFunTpYm0/d+6cTp06ZdO3UqVK8vb2tltGSkqK5s2bV+Q3AHd3d+3bt8/6/5u98847at68uRISEvT6668rKipK3333ncaMGaP77rvP7o/TlStXdOrUKV2/fl0nTpzQypUrNWvWLP3+97/X448/btP3/Pnzdtvh7+9f6IGsr732msaMGVPkdpw7d07PPvus+vbtq8aNG8vf3187duzQn//8Z3Xo0EG+vr5auHCh9bT0oUOHKiAgQF988YXGjBmjLl26WI//CA8P16xZszR06FBduHBBvXv3VmRkpI4fP66lS5fKz8+vRE4Hd8Z7wxnmzZunFi1aKDY2VpMmTVLjxo3l5uam7du3a//+/Tan0F68eFGnTp3StWvXdPToUb3//vt69913NXXq1Nt+0xs1apSeffZZjR071uabt7M0atRIPXv21Ny5c61tzz33nFauXKnnnntO48aNU0JCgkJDQ/XDDz8oNTXVpbsqCqu3rCjOe7O0/OMf/9DPP/+sfv36KTAw0Oa5Ll26aPHixfrd734nqXifmc4wePBgvfPOOxoyZIhefPFFVaxYUenp6Vq8eLHee+89az/DMHTq1CkZhqHz589ry5YtevPNNxUYGGh3/aOSVhZ/xtLtX8szZ87os88+0+rVq+3OvOzTp4+eeuopnTlzRiEhISVXpHGP69OnjyHJburTp49Rs2ZNY9asWXbzDBgwwHjggQeMvLw84+jRo4XOL8lYtmyZYRiGkZycbAQGBhZZw6xZswxfX1+jQ4cORfbp0KGD0adPH+vjjIwMIzEx0QgLCzMqVKhghIeHG8OGDTPOnj1rM1/Lli2t9Xh6ehrVqlUzfve73xkrV660W0dR2zF16lTDMAzjq6++MiQZP//8c6E1rlq1yvj1W+rq1avGq6++ajRt2tQIDAw0fHx8jHr16hl//OMfjStXrlj7bdiwwWjTpo0RGBhoeHp6GtHR0cb06dON69ev260jPT3dSEhIMIKCggxvb2+jfv36xujRo42TJ08W+drdqbt9bzjbyZMnjaFDhxpRUVFGhQoVDD8/P6NZs2bGtGnTjMuXLxuGYRg1a9a0+XlHREQYXbt2Nb788kubZRW8b3ft2mXTnp+fb9SrV8/4/e9/75Sa+/TpY/e+zsjIMLy8vGzeK3l5ecaCBQuMhx9+2PD19TU8PT2N+++/3xgwYICxd+9ep9TirHpv93tQGm713pw4caLx4IMPuqSu3/3ud0a7du0KfW7nzp2GJOu/t/rMLCk7duwwEhISjKpVqxoBAQFGbGyszTqTk5OttVgsFiMwMNBo1qyZMWXKFCM7O9tmWYW9V5zpTj5/CtzueWe41Ws5ffp0o1KlSkZubq7dfNeuXTMqV65szJgxo0RrtRhGOT4/DwAA4CbsPAQAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAFQJkyaNElNmjRxdRkATIAbZwJwKcMwlJeX5+oyAJgIIzcAiqVVq1YaOnSohg4dqkqVKik4OFh//OMfVXCbuvfff1+xsbHy9/dXWFiYevToodOnT1vnX7dunSwWi9auXavY2Fh5eXnpvffe0+TJk/XNN9/IYrHIYrEoJSVFffv2td5FusD169cVFhampKQkh2p96aWXNHbsWFWuXFlhYWGaNGmSTZ+ZM2eqUaNG8vX1VXh4uAYPHqxLly5Zn09JSVGlSpX0j3/8Q/Xq1ZOPj4+6dOmiy5cva8mSJYqMjFRQUJCGDRtmE9Jyc3Otd1f39fXVww8/rHXr1t3BKw6g2Jx+K04AptayZUvDz8/PGD58uLF//37j/fffN3x8fIxFixYZhmEYixcvNtasWWMcPnzY2LJli/HII48Ybdu2tc5fcFftxo0bG2lpacahQ4eM48ePG6NGjTIeeOABIysry8jKyjKuXLlibNq0yXB3d7e56/unn35q+Pr6GhcvXnSo1oCAAGPSpEnGgQMHjCVLlhgWi8VIS0uz9pk1a5bx5ZdfGkeOHDG++OILu7uiJycnGxUqVDBat25t/Oc//zHWr19vBAcHG/Hx8UbXrl2N7777zvjss88MT09P48MPP7TO16NHD6N58+bGhg0bjEOHDhnTpk0zvLy8jAMHDtzV6w/g9gg3AIqlZcuWRoMGDYz8/Hxr2yuvvGI0aNCg0P7btm0zJFnDSEG4+eSTT2z6TZw40XjwwQft5o+Ojjbefvtt6+OOHTsaiYmJDtf629/+1qbtN7/5jfHKK68UOc/y5cuN4OBg6+Pk5GRDknHo0CFr28CBAw0fHx+bgJWQkGAMHDjQMAzDOHTokGGxWIwTJ07YLPuJJ54wxo0b51DtAO4cu6UAFNsjjzwii8VifRwXF6eDBw8qLy9Pu3btUocOHVSzZk35+/urVatWkqTMzEybZcTGxjq0rv79+ys5OVmSdPr0af2///f/1LdvX4drbdy4sc3jatWq2ewm++qrr9S6dWvdd9998vf3V+/evXXu3DldvnzZ2sfHx0e1atWyPg4NDVVkZKT8/Pxs2gqW+5///EeGYahu3bry8/OzTuvXr9fhw4cdrh3AneGAYgBOc/XqVcXHxys+Pl7vv/++QkJClJmZqYSEBOXm5tr09fX1dWiZvXv31quvvqotW7Zoy5YtioyM1KOPPupwTRUqVLB5bLFYlJ+fL0n64Ycf1K5dOw0aNEivvfaaKleurK+//lr9+vXTtWvXbrmMWy03Pz9f7u7u2rlzp9zd3W36/ToQASgZhBsAxfbvf//b7nGdOnW0f/9+nT17Vm+99ZbCw8MlSTt27HBomZ6enoWeNRUcHKyOHTsqOTlZW7Zs0QsvvHD3G/A/O3bs0PXr1zVjxgy5ud0YyF6+fPldL/ehhx5SXl6eTp8+XawgBsA52C0FoNiOHTumkSNH6vvvv9eyZcs0d+5cDR8+XBEREfL09NTcuXN15MgRrV69Wq+99ppDy4yMjNTRo0e1e/dunT17Vjk5Odbn+vfvryVLlmjfvn3q06eP07ajVq1aun79urXe9957TwsWLLjr5datW1c9e/ZU7969tXLlSh09elTbt2/X22+/rTVr1jihcgC3QrgBUGy9e/fWL7/8ombNmmnIkCEaNmyYXnzxRYWEhCglJUUfffSRoqOj9dZbb2n69OkOLbNz585q06aNHn/8cYWEhGjZsmXW55588klVq1ZNCQkJql69utO2o0mTJpo5c6befvttNWzYUB988IGmTp3qlGUnJyerd+/eGjVqlOrVq6enn35aW7dutY5oASg5FsP438UpAMABrVq1UpMmTTR79uxSW+eVK1dUvXp1JSUlqVOnTqW2XgDlE8fcACiz8vPzderUKc2YMUOBgYF6+umnXV0SgHKAcAOgzMrMzFRUVJRq1KihlJQUeXh42DwXHR1d5Lx79+5VREREaZQJoIxhtxSAcun69evKyMgo8vnIyEibMATg3kG4AQAApsLZUgAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFT+P8mk1uXi8+1iAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.barplot(x=\"party_name\", y=\"freq\", hue=\"type\", data=party_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_tokens_list, \"../data/2017-2022/train_data.pt\")\n",
    "# torch.save(test_tokens_list, \"../data/2017-2022/test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = torch.load(\"../data/2017-2022/train_data.pt\")\n",
    "test_dict = torch.load(\"../data/2017-2022/test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'masks', 'interventions_masks', 'labels'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterventionsDataset(Dataset):\n",
    "    def __init__(self, dict_data):\n",
    "        self.labels = dict_data[\"labels\"]\n",
    "        self.masks = dict_data[\"masks\"]\n",
    "        self.tokens = dict_data[\"tokens\"]\n",
    "        self.interventions_masks = dict_data[\"interventions_masks\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return {\n",
    "            \"tokens\": self.tokens[idx],\n",
    "            \"masks\": self.masks[idx],\n",
    "            \"interventions_masks\": self.interventions_masks[idx],\n",
    "        }\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InterventionsDataset(train_dict)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = InterventionsDataset(test_dict)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 500, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 7.80 GiB total capacity; 6.27 GiB already allocated; 93.56 MiB free; 6.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m camembert_model \u001b[39m=\u001b[39m CamembertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcamembert-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m camembert_model\u001b[39m.\u001b[39;49mcuda();\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:747\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    731\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:747\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    731\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 7.80 GiB total capacity; 6.27 GiB already allocated; 93.56 MiB free; 6.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "camembert_model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "camembert_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x[\"tokens\"][:, 1, :].to(device=device)\n",
    "attention_mask = x[\"masks\"][:, 1, :].to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.80 GiB total capacity; 6.32 GiB already allocated; 31.31 MiB free; 6.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m camembert_model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:898\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    891\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    892\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    893\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    897\u001b[0m )\n\u001b[0;32m--> 898\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    899\u001b[0m     embedding_output,\n\u001b[1;32m    900\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    901\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    902\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    903\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    904\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    905\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    906\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    907\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    908\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    911\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:534\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    525\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    526\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    527\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    533\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    535\u001b[0m         hidden_states,\n\u001b[1;32m    536\u001b[0m         attention_mask,\n\u001b[1;32m    537\u001b[0m         layer_head_mask,\n\u001b[1;32m    538\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    539\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    540\u001b[0m         past_key_value,\n\u001b[1;32m    541\u001b[0m         output_attentions,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:419\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    409\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    417\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    420\u001b[0m         hidden_states,\n\u001b[1;32m    421\u001b[0m         attention_mask,\n\u001b[1;32m    422\u001b[0m         head_mask,\n\u001b[1;32m    423\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    424\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    428\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:346\u001b[0m, in \u001b[0;36mCamembertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 346\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    347\u001b[0m         hidden_states,\n\u001b[1;32m    348\u001b[0m         attention_mask,\n\u001b[1;32m    349\u001b[0m         head_mask,\n\u001b[1;32m    350\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    351\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    352\u001b[0m         past_key_value,\n\u001b[1;32m    353\u001b[0m         output_attentions,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    356\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:248\u001b[0m, in \u001b[0;36mCamembertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    245\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    247\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 7.80 GiB total capacity; 6.32 GiB already allocated; 31.31 MiB free; 6.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for i in range(x.shape[0]):\n",
    "    input_ids = x[\"tokens\"][:, i, :]\n",
    "    attention_mask = x[\"masks\"][:, i, :]\n",
    "    camembert_model(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCamembertClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=11,\n",
    "        bert_model=camembert_model,\n",
    "        bert_dim=768,\n",
    "        input_dim=256,\n",
    "        embed_dim=256,\n",
    "        input_dim2=256,\n",
    "        num_heads=8,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(SimpleCamembertClassifier, self).__init__()\n",
    "\n",
    "        # Set the parameters\n",
    "        self.bert_dim = bert_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_dim = input_dim // num_heads\n",
    "\n",
    "        # Initialize the BERT model\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "        # Initialize the linear layers for the multi-head attention\n",
    "        self.query_linear = nn.Linear(bert_dim, input_dim)\n",
    "        self.key_linear = nn.Linear(bert_dim, input_dim)\n",
    "        self.value_linear = nn.Linear(bert_dim, input_dim)\n",
    "\n",
    "        # Initialize the multi-head attention layer\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Initialize the linear layers for the output projection\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim2)\n",
    "        self.linear2 = nn.Linear(input_dim, num_classes)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, tokens, masks, interventions_masks):\n",
    "        \"\"\"\n",
    "        tokens (batch_size, nb_int, nb_tokens)\n",
    "        masks (batch_size, nb_int, nb_tokens)\n",
    "        interventions_masks (batch_size, nb_int)\n",
    "        \"\"\"\n",
    "        pooled_output = self.bert_model(\n",
    "            input_ids=tokens, attention_mask=masks, return_dict=False\n",
    "        )  # (batch_size, nb_int, bert_dim)\n",
    "\n",
    "        # Add a zero padding for CLS in attention\n",
    "        pooled_output_with_cls = F.pad(\n",
    "            pooled_output, pad=(0, 0, 1, 0), mode=\"constant\", value=0.0\n",
    "        )  # (batch_size, nb_int + 1, bert_dim)\n",
    "\n",
    "        # Split the input tensors into the different attention heads\n",
    "        query_heads = self.query_linear(\n",
    "            pooled_output_with_cls\n",
    "        )  # (batch_size, nb_int + 1, input_dim)\n",
    "        key_heads = self.key_linear(pooled_output_with_cls)  # (batch_size, nb_int + 1, input_dim)\n",
    "        value_heads = self.value_linear(pooled_output_with_cls)  # (batch_size, input_dim)\n",
    "\n",
    "        # Apply the multi-head attention\n",
    "        attn_output = self.attention_layer(\n",
    "            query_heads,\n",
    "            key_heads,\n",
    "            value_heads,\n",
    "            key_padding_mask=interventions_masks,\n",
    "            need_weights=False,\n",
    "        )  # (batch_size, input_dim)\n",
    "\n",
    "        deputy_repr = attn_output[:, 0, :]  # (batch_size, input_dim)\n",
    "\n",
    "        dropout_output1 = self.dropout1(deputy_repr)\n",
    "        linear_output1 = nn.ReLU(self.linear1(dropout_output1))  # (batch_size, input_dim2)\n",
    "        dropout_output2 = self.dropout2(linear_output1)\n",
    "        linear_output2 = nn.ReLU(self.linear2(dropout_output2))  # (batch_size, num_classes)\n",
    "\n",
    "        final_layer = nn.Softmax(linear_output2)  # (batch_size, num_classes)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = InterventionsDataset(train_data), InterventionsDataset(val_data)\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val, batch_size=32)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            tokens = train_input[\"tokens\"].squeeze(1).to(device)\n",
    "            masks = train_input[\"masks\"].to(device)\n",
    "            interventions_masks = train_input[\"interventions_masks\"].to(device)\n",
    "\n",
    "            output = model(tokens, masks, interventions_masks)\n",
    "\n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input[\"attention_mask\"].to(device)\n",
    "                input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m SimpleCamembertClassifier()\n\u001b[1;32m      3\u001b[0m LR \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m\n\u001b[0;32m----> 5\u001b[0m train(model, train_dict, test_dict, LR, EPOCHS)\n",
      "Cell \u001b[0;32mIn[27], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m masks \u001b[39m=\u001b[39m train_input[\u001b[39m'\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m interventions_masks \u001b[39m=\u001b[39m train_input[\u001b[39m'\u001b[39m\u001b[39minterventions_masks\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m output \u001b[39m=\u001b[39m model(tokens, masks, interventions_masks)\n\u001b[1;32m     33\u001b[0m batch_loss \u001b[39m=\u001b[39m criterion(output, train_label\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     34\u001b[0m total_loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m, in \u001b[0;36mSimpleCamembertClassifier.forward\u001b[0;34m(self, tokens, masks, interventions_masks)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tokens, masks, interventions_masks):\n\u001b[1;32m     42\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m    tokens (batch_size, nb_int, nb_tokens)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    masks (batch_size, nb_int, nb_tokens)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m    interventions_masks (batch_size, nb_int)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_model(input_ids\u001b[39m=\u001b[39;49mtokens, attention_mask\u001b[39m=\u001b[39;49mmasks, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)  \u001b[39m# (batch_size, nb_int, bert_dim)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39m# Add a zero padding for CLS in attention\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     pooled_output_with_cls \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(pooled_output, pad\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, value\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m)  \u001b[39m# (batch_size, nb_int + 1, bert_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:852\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 852\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    853\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    855\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "model = SimpleCamembertClassifier()\n",
    "LR = 1e-4\n",
    "\n",
    "train(model, train_dict, test_dict, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input[\"attention_mask\"].to(device)\n",
    "            input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "    print(f\"Test Accuracy: {total_acc_test / len(test_data): .3f}\")\n",
    "\n",
    "\n",
    "evaluate(model, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mcamembert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           CamembertModel\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "CamembertModel(\n",
      "           (embeddings): CamembertEmbeddings(\n",
      "           (word_embeddings): Embedding(32005, 768, <...>\n",
      "           (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "           (activation): Tanh()\n",
      "           )\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\n",
      "\n",
      "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
      "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
      "etc.)\n",
      "\n",
      "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
      "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
      "and behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`CamembertConfig`]): Model configuration class with all the parameters of the\n",
      "        model. Initializing with a config file does not load the weights associated with the model, only the\n",
      "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\n",
      "\n",
      "The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
      "cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
      "all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
      "Kaiser and Illia Polosukhin.\n",
      "\n",
      "To behave as a decoder the model needs to be initialized with the `is_decoder` argument of the configuration set to\n",
      "`True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
      "`add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
      "\n",
      ".. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "?camembert_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-11-7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290de4d201867099a7cec8aa5bca78d01a6c85d7bcab7b52f8e97114ad853450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
