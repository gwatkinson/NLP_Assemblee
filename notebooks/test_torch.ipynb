{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bd16bcfba4303d79\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bd16bcfba4303d79\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=../lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, CamembertModel, CamembertTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deputies_df = pd.read_pickle(\"../data/2017-2022/deputies.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = deputies_df.groupe_sigle.unique()\n",
    "groups_to_int = dict(zip(groups, range(len(groups))))\n",
    "int_to_groups = dict(zip(range(len(groups)), groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: tokenizer(\n",
    "    x, padding=\"max_length\", max_length=256, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "decode_tokens = lambda x: tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"../data/2017-2022/interventions/*.json\")\n",
    "file_stats = [os.stat(file).st_size for file in files]\n",
    "no_empty_files = [files[i] for i in range(len(files)) if file_stats[i] > 63]\n",
    "slug_files = [file.split(\"/\")[-1].split(\".\")[0] for file in no_empty_files]\n",
    "file_dict = dict(zip(slug_files, no_empty_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(\"../tsvs/ND13_interventions_QAG.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(\"../tsvs/ND15_interventions_hemicycle_rich.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_pickle(\"../data/2017-2022/raw_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_without_pres = tdf[\n",
    "    (~tdf.fonction.isin(['président', 'présidente', \"président, doyen d'âge\"]))  # Enlève le président\n",
    "    # (~tdf.titre.str.contains(\"article\"))  # Enlève les interventions sur les articles\n",
    "]\n",
    "\n",
    "tdf_without_exclamations = tdf_without_pres[\n",
    "    (~tdf_without_pres.titre.isna()) &  # Enlève les interventions sans titre (transitions, présidents, ...)\n",
    "    (~tdf_without_pres.sexe.isna()) &  # Enlève les interventions sans sexe (groupe, interlocutions, non députés, ...)\n",
    "    (tdf_without_pres.nb_mots > 7)  # Enlève les interventions trop courtes\n",
    "]\n",
    "\n",
    "intervention_count = tdf_without_exclamations.parlementaire.value_counts().rename(\"intervention_count\").to_frame()\n",
    "tdf_without_exclamations = tdf_without_exclamations.merge(intervention_count, left_on=\"parlementaire\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intervention(row, max_len=256):\n",
    "    if row[\"nb_mots\"] > max_len:\n",
    "        paragraphs = BeautifulSoup(row[\"intervention\"]).find_all('p')\n",
    "        return [_.getText() for _ in paragraphs]\n",
    "    return BeautifulSoup(row[\"intervention\"]).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tdf_intervention = tdf_without_exclamations.apply(process_intervention, axis=1)\n",
    "tdf_intervention_processed = tdf_without_exclamations.assign(intervention=processed_tdf_intervention).explode(\"intervention\")\n",
    "tdf_intervention_processed = tdf_intervention_processed[tdf_intervention_processed.intervention.str.count(' ') > 3]\n",
    "tdf_intervention_processed[\"nb_mots_approx\"] = tdf_intervention_processed.intervention.str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_without_short = tdf_intervention_processed[tdf_intervention_processed[\"intervention_count\"] > 10].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_without_short.to_pickle(\"../data/2017-2022/clean_interventions_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_without_short = pd.read_pickle(\"../data/2017-2022/clean_interventions_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_seance = tdf_without_short.groupby([\"seance_id\", \"parlementaire\"]).agg(\n",
    "    count=pd.NamedAgg(column=\"id\", aggfunc=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deputies_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clean_deputies_df \u001b[39m=\u001b[39m deputies_df[[\u001b[39m\"\u001b[39m\u001b[39mnom\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdate_naissance\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnum_circo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprofession\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnb_mandats\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m clean_deputies_df[\u001b[39m\"\u001b[39m\u001b[39mprofession\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfillna(\u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deputies_df' is not defined"
     ]
    }
   ],
   "source": [
    "clean_deputies_df = deputies_df[[\"nom\", \"date_naissance\", \"num_circo\", \"profession\", \"nb_mandats\"]].copy()\n",
    "clean_deputies_df[\"profession\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_interventions = tdf_without_short[[\n",
    "    \"parlementaire\",\n",
    "    \"parlementaire_groupe_acronyme\",\n",
    "    \"seance_id\",\n",
    "    \"date\",\n",
    "    \"type\",\n",
    "    \"titre\",\n",
    "    \"titre_complet\",\n",
    "    \"intervention\"\n",
    "]].rename(columns={\n",
    "    \"parlementaire\": \"nom\",\n",
    "    \"parlementaire_groupe_acronyme\": \"groupe\",\n",
    "    \"date\": \"date_seance\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = simple_interventions.groupby([\"seance_id\", \"date_seance\", \"nom\", \"groupe\", \"type\", \"titre\", \"titre_complet\"]).agg(\n",
    "    intervention=pd.NamedAgg(column=\"intervention\", aggfunc=lambda group: \" \".join(group))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = grouped_df.merge(clean_deputies_df, left_on=\"nom\", right_on=\"nom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_pickle(\"../data/2017-2022/final_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_pickle(\"../data/2017-2022/final_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_groups_dict = {\n",
    "    'NI': 'Droite',\n",
    "    'UAI': 'Droite',\n",
    "    'LC': 'Droite',\n",
    "    'LR': 'Droite',\n",
    "    'LFI': 'Gauche',\n",
    "    'NG': 'Gauche',\n",
    "    'GDR': 'Gauche',\n",
    "    'LREM': 'Centre',\n",
    "    'MODEM': 'Centre'\n",
    "}\n",
    "\n",
    "sub_to_int = {\n",
    "    \"Gauche\": 0,\n",
    "    \"Centre\": 1,\n",
    "    \"Droite\": 2\n",
    "}\n",
    "\n",
    "labels_dict = {\n",
    "    group: sub_to_int[sub_groups_dict[group]] for group in sub_groups_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "class SeanceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        df,\n",
    "        labels_dict,\n",
    "        group_var=\"groupe\",\n",
    "        intervention_var=\"intervention\",\n",
    "        titre_var=\"titre_complet\",\n",
    "        profession_var=\"profession\",\n",
    "        max_len_padding=512,\n",
    "        max_len_padding_titre=64,\n",
    "        max_len_padding_profession=16,\n",
    "    ):\n",
    "        # Parameters\n",
    "        self.df = df\n",
    "        self.labels_dict = labels_dict\n",
    "        self.inverse_label_dict = {v: k for k, v in labels_dict.items()}\n",
    "        self.group_var = group_var\n",
    "        self.intervention_var = intervention_var\n",
    "        self.titre_var = titre_var\n",
    "        self.profession_var = profession_var\n",
    "        self.max_len_padding = max_len_padding\n",
    "        self.max_len_padding_titre = max_len_padding_titre\n",
    "        self.max_len_padding_profession = max_len_padding_profession\n",
    "        \n",
    "        # Inputs and labels\n",
    "        self.labels = [labels_dict[label] for label in df[group_var]]\n",
    "        \n",
    "        self.interventions = [tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_len_padding,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ) for text in df[intervention_var]]\n",
    "        \n",
    "        self.titres = [tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_len_padding_titre,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ) for text in df[titre_var]]\n",
    "        \n",
    "        self.professions = [tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_len_padding_profession,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ) for text in df[profession_var]]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_inputs(self, idx):\n",
    "        return {\n",
    "            \"intervention\": self.interventions[idx],\n",
    "            \"titre\": self.titres[idx],\n",
    "            \"profession\": self.professions[idx]\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch_inputs(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    tokenizer,\n",
    "    processed_df,\n",
    "    labels_dict,\n",
    "    group_var=\"groupe\",\n",
    "    intervention_var=\"intervention\",\n",
    "    titre_var=\"titre_complet\",\n",
    "    profession_var=\"profession\",\n",
    "    max_len_padding=512,\n",
    "    max_len_padding_titre=64,\n",
    "    max_len_padding_profession=16,\n",
    "    test_frac=0.25,\n",
    "    val_frac=0.2\n",
    "):\n",
    "    X, y = np.arange(len(processed_df)), processed_df[\"groupe\"]\n",
    "    test_frac = 0.25\n",
    "    val_frac = 0.2\n",
    "\n",
    "    idx_train, idx_test, y_train, y_test = train_test_split(X, y, test_size=test_frac, random_state=42, stratify=y)\n",
    "    idx_train, idx_val, y_train, y_val = train_test_split(idx_train, y_train, test_size=val_frac, random_state=42, stratify=y_train)\n",
    "    \n",
    "    train_dataset = SeanceDataset(\n",
    "        tokenizer,\n",
    "        processed_df.iloc[idx_train],\n",
    "        labels_dict,\n",
    "        group_var=group_var,\n",
    "        intervention_var=intervention_var,\n",
    "        titre_var=titre_var,\n",
    "        profession_var=profession_var,\n",
    "        max_len_padding=max_len_padding,\n",
    "        max_len_padding_titre=max_len_padding_titre,\n",
    "        max_len_padding_profession=max_len_padding_profession,\n",
    "    )\n",
    "\n",
    "    val_dataset = SeanceDataset(\n",
    "        tokenizer,\n",
    "        processed_df.iloc[idx_val],\n",
    "        labels_dict,\n",
    "        group_var=group_var,\n",
    "        intervention_var=intervention_var,\n",
    "        titre_var=titre_var,\n",
    "        profession_var=profession_var,\n",
    "        max_len_padding=max_len_padding,\n",
    "        max_len_padding_titre=max_len_padding_titre,\n",
    "        max_len_padding_profession=max_len_padding_profession,\n",
    "    )\n",
    "\n",
    "    test_dataset = SeanceDataset(\n",
    "        tokenizer,\n",
    "        processed_df.iloc[idx_test],\n",
    "        labels_dict,\n",
    "        group_var=group_var,\n",
    "        intervention_var=intervention_var,\n",
    "        titre_var=titre_var,\n",
    "        profession_var=profession_var,\n",
    "        max_len_padding=max_len_padding,\n",
    "        max_len_padding_titre=max_len_padding_titre,\n",
    "        max_len_padding_profession=max_len_padding_profession,\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def get_dataloader(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import CamembertModel\n",
    "\n",
    "class SeanceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model,\n",
    "        num_classes,\n",
    "        intervention_dim=256,\n",
    "        titre_dim=128,\n",
    "        profession_dim=64,\n",
    "        bert_dim=768,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(SeanceClassifier, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.num_classes = num_classes\n",
    "        self.intervention_dim = intervention_dim\n",
    "        self.titre_dim = titre_dim\n",
    "        self.profession_dim = profession_dim\n",
    "        self.inter_dim = intervention_dim + titre_dim + profession_dim\n",
    "        self.bert_dim = bert_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.bert = bert_model\n",
    "        \n",
    "        self.intervention_linear = nn.Linear(bert_dim, intervention_dim)\n",
    "        self.titre_linear = nn.Linear(bert_dim, titre_dim)\n",
    "        self.profession_linear = nn.Linear(bert_dim, profession_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.inter_dim, self.inter_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.inter_dim, self.inter_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.inter_dim, num_classes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, intervention, titre, profession):\n",
    "        # Embed interventions\n",
    "        _, intervention_pooled_output = self.bert(\n",
    "            input_ids=intervention['input_ids'].squeeze(1),\n",
    "            attention_mask=intervention['attention_mask'].squeeze(1),\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        # Embed titre\n",
    "        _, titre_pooled_output = self.bert(\n",
    "            input_ids=titre['input_ids'].squeeze(1),\n",
    "            attention_mask=titre['attention_mask'].squeeze(1),\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        # Embed profession\n",
    "        _, profession_pooled_output = self.bert(\n",
    "            input_ids=profession['input_ids'].squeeze(1),\n",
    "            attention_mask=profession['attention_mask'].squeeze(1),\n",
    "            return_dict=False\n",
    "        )\n",
    "        \n",
    "        intervention_embeded = self.intervention_linear(intervention_pooled_output)\n",
    "        titre_embeded = self.titre_linear(titre_pooled_output)\n",
    "        profession_embeded = self.profession_linear(profession_pooled_output)\n",
    "        \n",
    "        # Concatenate the embeddings\n",
    "        seance_repr = torch.cat([\n",
    "            intervention_embeded, titre_embeded, profession_embeded\n",
    "        ], dim=1)\n",
    "    \n",
    "        \n",
    "        # MLP classifier layer\n",
    "        final_output = self.mlp(seance_repr)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import optim, nn\n",
    "\n",
    "class SeanceLitClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        processed_df,\n",
    "        labels_dict,\n",
    "        batch_size=32,\n",
    "        num_workers=8,\n",
    "        type_bert=\"camembert\",\n",
    "        freeze_bert=True,\n",
    "        intervention_dim=256,\n",
    "        titre_dim=128,\n",
    "        profession_dim=64,\n",
    "        bert_dim=768,\n",
    "        dropout=0.1,\n",
    "        lr=1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.example_input_array = {\n",
    "            \"intervention\": {\"input_ids\": torch.randint(0, 100, (32, 1, 512)), \"attention_mask\": torch.randint(0, 2, (32, 1, 512))},\n",
    "            \"titre\": {\"input_ids\": torch.randint(0, 100, (32, 1, 64)), \"attention_mask\": torch.randint(0, 2, (32, 1, 64))},\n",
    "            \"profession\": {\"input_ids\": torch.randint(0, 100, (32, 1, 16)), \"attention_mask\": torch.randint(0, 2, (32, 1, 16))}\n",
    "        }\n",
    "        \n",
    "        # Parameters\n",
    "        self.num_classes = len(np.unique(list(labels_dict.values())))\n",
    "        self.processed_df = processed_df\n",
    "        self.labels_dict = labels_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.type_bert = type_bert\n",
    "        self.freeze_bert = freeze_bert\n",
    "        self.intervention_dim = intervention_dim\n",
    "        self.titre_dim = titre_dim\n",
    "        self.profession_dim = profession_dim\n",
    "        self.bert_dim = bert_dim\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Models\n",
    "        if type_bert == \"camembert\":\n",
    "            self.bert_model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "            self.tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "        else:\n",
    "            self.bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for p in self.bert_model.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        self.classifier = SeanceClassifier(\n",
    "            bert_model=self.bert_model,\n",
    "            num_classes=self.num_classes,\n",
    "            intervention_dim=intervention_dim,\n",
    "            titre_dim=titre_dim,\n",
    "            profession_dim=profession_dim,\n",
    "            bert_dim=bert_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        # Save the hyperparameters\n",
    "        # See https://pytorch-lightning.readthedocs.io/en/latest/common/checkpointing_basic.html#save-a-checkpoint\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, intervention, titre, profession):\n",
    "        return self.classifier(intervention, titre, profession)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x_hat = self.forward(x[\"intervention\"], x[\"titre\"], x[\"profession\"])\n",
    "        loss = nn.CrossEntropyLoss()(x_hat, y)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        # See more https://pytorch-lightning.readthedocs.io/en/latest/common/evaluation_basic.html\n",
    "        x, y = batch\n",
    "        x_hat = self.forward(x[\"intervention\"], x[\"titre\"], x[\"profession\"])\n",
    "        loss = nn.CrossEntropyLoss()(x_hat, y)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"test_loss\", loss)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x_hat = self.forward(x[\"intervention\"], x[\"titre\"], x[\"profession\"])\n",
    "        loss = nn.CrossEntropyLoss()(x_hat, y)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.train_data, self.val_data, self.test_data = get_dataset(\n",
    "            self.tokenizer,\n",
    "            self.processed_df,\n",
    "            self.labels_dict,\n",
    "            group_var=\"groupe\",\n",
    "            intervention_var=\"intervention\",\n",
    "            titre_var=\"titre_complet\",\n",
    "            profession_var=\"profession\",\n",
    "            max_len_padding=512,\n",
    "            max_len_padding_titre=64,\n",
    "            max_len_padding_profession=16,\n",
    "            test_frac=0.25,\n",
    "            val_frac=0.2\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, num_workers=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset, val_dataset, test_dataset  \u001b[39m=\u001b[39m get_dataset(\n\u001b[1;32m      2\u001b[0m     tokenizer,\n\u001b[1;32m      3\u001b[0m     processed_df,\n\u001b[1;32m      4\u001b[0m     labels_dict\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m train_dataloader, val_dataloader, test_dataloader \u001b[39m=\u001b[39m get_dataloader(\n\u001b[1;32m      8\u001b[0m     train_dataset, val_dataset, test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "Cell \u001b[0;32mIn[57], line 35\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(tokenizer, processed_df, labels_dict, group_var, intervention_var, titre_var, profession_var, max_len_padding, max_len_padding_titre, max_len_padding_profession, test_frac, val_frac)\u001b[0m\n\u001b[1;32m     20\u001b[0m idx_train, idx_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(idx_train, y_train, test_size\u001b[39m=\u001b[39mval_frac, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, stratify\u001b[39m=\u001b[39my_train)\n\u001b[1;32m     22\u001b[0m train_dataset \u001b[39m=\u001b[39m SeanceDataset(\n\u001b[1;32m     23\u001b[0m     tokenizer,\n\u001b[1;32m     24\u001b[0m     processed_df\u001b[39m.\u001b[39miloc[idx_train],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     max_len_padding_profession\u001b[39m=\u001b[39mmax_len_padding_profession,\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m val_dataset \u001b[39m=\u001b[39m SeanceDataset(\n\u001b[1;32m     36\u001b[0m     tokenizer,\n\u001b[1;32m     37\u001b[0m     processed_df\u001b[39m.\u001b[39;49miloc[idx_val],\n\u001b[1;32m     38\u001b[0m     labels_dict,\n\u001b[1;32m     39\u001b[0m     group_var\u001b[39m=\u001b[39;49mgroup_var,\n\u001b[1;32m     40\u001b[0m     intervention_var\u001b[39m=\u001b[39;49mintervention_var,\n\u001b[1;32m     41\u001b[0m     titre_var\u001b[39m=\u001b[39;49mtitre_var,\n\u001b[1;32m     42\u001b[0m     profession_var\u001b[39m=\u001b[39;49mprofession_var,\n\u001b[1;32m     43\u001b[0m     max_len_padding\u001b[39m=\u001b[39;49mmax_len_padding,\n\u001b[1;32m     44\u001b[0m     max_len_padding_titre\u001b[39m=\u001b[39;49mmax_len_padding_titre,\n\u001b[1;32m     45\u001b[0m     max_len_padding_profession\u001b[39m=\u001b[39;49mmax_len_padding_profession,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m test_dataset \u001b[39m=\u001b[39m SeanceDataset(\n\u001b[1;32m     49\u001b[0m     tokenizer,\n\u001b[1;32m     50\u001b[0m     processed_df\u001b[39m.\u001b[39miloc[idx_test],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     max_len_padding_profession\u001b[39m=\u001b[39mmax_len_padding_profession,\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m train_dataset, val_dataset, test_dataset\n",
      "Cell \u001b[0;32mIn[55], line 36\u001b[0m, in \u001b[0;36mSeanceDataset.__init__\u001b[0;34m(self, tokenizer, df, labels_dict, group_var, intervention_var, titre_var, profession_var, max_len_padding, max_len_padding_titre, max_len_padding_profession)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m# Inputs and labels\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m [labels_dict[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m df[group_var]]\n\u001b[0;32m---> 36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterventions \u001b[39m=\u001b[39m [tokenizer(\n\u001b[1;32m     37\u001b[0m     text,\n\u001b[1;32m     38\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m     max_length\u001b[39m=\u001b[39mmax_len_padding,\n\u001b[1;32m     40\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[intervention_var]]\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtitres \u001b[39m=\u001b[39m [tokenizer(\n\u001b[1;32m     45\u001b[0m     text,\n\u001b[1;32m     46\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m ) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[titre_var]]\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofessions \u001b[39m=\u001b[39m [tokenizer(\n\u001b[1;32m     53\u001b[0m     text,\n\u001b[1;32m     54\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m ) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[profession_var]]\n",
      "Cell \u001b[0;32mIn[55], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m# Inputs and labels\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m [labels_dict[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m df[group_var]]\n\u001b[0;32m---> 36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterventions \u001b[39m=\u001b[39m [tokenizer(\n\u001b[1;32m     37\u001b[0m     text,\n\u001b[1;32m     38\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     39\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_len_padding,\n\u001b[1;32m     40\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     41\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     42\u001b[0m ) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[intervention_var]]\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtitres \u001b[39m=\u001b[39m [tokenizer(\n\u001b[1;32m     45\u001b[0m     text,\n\u001b[1;32m     46\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m ) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[titre_var]]\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofessions \u001b[39m=\u001b[39m [tokenizer(\n\u001b[1;32m     53\u001b[0m     text,\n\u001b[1;32m     54\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m ) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[profession_var]]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2488\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2487\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2488\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2489\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2490\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2594\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2575\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2576\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2591\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2592\u001b[0m     )\n\u001b[1;32m   2593\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2595\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2596\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2597\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2598\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2599\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2600\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2601\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2602\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2603\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2604\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2605\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2606\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2607\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2608\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2609\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2610\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2611\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2612\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2613\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2667\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2659\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2660\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2664\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2665\u001b[0m )\n\u001b[0;32m-> 2667\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2668\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2669\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2670\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2671\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2672\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2673\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2674\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2675\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2676\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2677\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2678\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2679\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2680\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2681\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2682\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2683\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2684\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2685\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2686\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    617\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         tokenized_text\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m    546\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m         tokenized_text\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token))\n\u001b[1;32m    548\u001b[0m \u001b[39m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/tokenization_camembert.py:247\u001b[0m, in \u001b[0;36mCamembertTokenizer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp_model\u001b[39m.\u001b[39;49mencode(text, out_type\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/sentencepiece-0.1.96-py3.10-linux-x86_64.egg/sentencepiece/__init__.py:304\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Encode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, enable_sampling, nbest_size, alpha)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    302\u001b[0m   \u001b[39mreturn\u001b[39;00m [_encode(n) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m]\n\u001b[0;32m--> 304\u001b[0m \u001b[39mreturn\u001b[39;00m _encode(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/sentencepiece-0.1.96-py3.10-linux-x86_64.egg/sentencepiece/__init__.py:283\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Encode.<locals>._encode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    281\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSampleEncodeAsPieces(text, nbest_size, alpha)\n\u001b[1;32m    282\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncodeAsPieces(text)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m reverse:\n\u001b[1;32m    286\u001b[0m   result\u001b[39m.\u001b[39mreverse()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/sentencepiece-0.1.96-py3.10-linux-x86_64.egg/sentencepiece/__init__.py:99\u001b[0m, in \u001b[0;36mSentencePieceProcessor.EncodeAsPieces\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mEncodeAsPieces\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39;49mSentencePieceProcessor_EncodeAsPieces(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset  = get_dataset(\n",
    "    tokenizer,\n",
    "    processed_df,\n",
    "    labels_dict\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size=128, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "lt_classifier = SeanceLitClassifier(\n",
    "    processed_df=processed_df,\n",
    "    labels_dict=labels_dict,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    type_bert=\"camembert\",\n",
    "    freeze_bert=False,\n",
    "    intervention_dim=128,\n",
    "    titre_dim=64,\n",
    "    profession_dim=32,\n",
    "    bert_dim=768,\n",
    "    dropout=0.1,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ligthning help\n",
    "\n",
    "```python\n",
    "# To reload from a checkpoint\n",
    "model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# To resume training\n",
    "model = LitModel()\n",
    "trainer = Trainer()\n",
    "trainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import callbacks\n",
    "\n",
    "lt_classifier = SeanceLitClassifier(\n",
    "    processed_df=processed_df,\n",
    "    labels_dict=labels_dict,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    type_bert=\"camembert\",\n",
    "    freeze_bert=True,\n",
    "    intervention_dim=128,\n",
    "    titre_dim=50,\n",
    "    profession_dim=32,\n",
    "    bert_dim=768,\n",
    "    dropout=0.1,\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    profiler=\"simple\",\n",
    "    max_epochs=100,\n",
    "    default_root_dir=\"../\",\n",
    "    # fast_dev_run=True,\n",
    "    # overfit_batches=1,\n",
    "    # limit_train_batches=10,\n",
    "    # limit_val_batches=5,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", check_finite=True, patience=5),\n",
    "        callbacks.ModelSummary(),\n",
    "        callbacks.Timer(duration=\"00:03:00:00\", interval=\"epoch\"),  # Max three hours\n",
    "        # callbacks.RichModelSummary(),\n",
    "        # callbacks.RichProgressBar(),\n",
    "        # callbacks.LearningRateFinder(min_lr=1e-06),\n",
    "        # callbacks.BatchSizeFinder(init_val=16),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params | In sizes        | Out sizes                  \n",
      "------------------------------------------------------------------------------------------------\n",
      "0 | bert_model | CamembertModel   | 110 M  | ?               | [[32, 512, 768], [32, 768]]\n",
      "1 | classifier | SeanceClassifier | 110 M  | ['?', '?', '?'] | [32, 3]                    \n",
      "------------------------------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "443.583   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0562c376704faaba604e55613765f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwatk/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e67bac5af74a44958f912cec1eda66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 7.80 GiB total capacity; 6.45 GiB already allocated; 392.19 MiB free; 6.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lt_classifier)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1163\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1162\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:214\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    213\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    218\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:200\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> 200\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    201\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:247\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    239\u001b[0m         closure()\n\u001b[1;32m    241\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    249\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:357\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    356\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    358\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    360\u001b[0m     batch_idx,\n\u001b[1;32m    361\u001b[0m     optimizer,\n\u001b[1;32m    362\u001b[0m     opt_idx,\n\u001b[1;32m    363\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    364\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    365\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[1;32m    366\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1305\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1305\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1307\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1661\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1580\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1581\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1588\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1589\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \n\u001b[1;32m   1660\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1661\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# TODO(lite): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    235\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    236\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:121\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/optim/adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 183\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    186\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:107\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     96\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:147\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:133\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 133\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:406\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39m    A ``ClosureResult`` containing the training step output.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    409\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_end\u001b[39m\u001b[39m\"\u001b[39m, training_step_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1443\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1443\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1445\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    377\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[59], line 75\u001b[0m, in \u001b[0;36mSeanceLitClassifier.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     72\u001b[0m     \u001b[39m# training_step defines the train loop.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39m# it is independent of forward\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     x, y \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 75\u001b[0m     x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x[\u001b[39m\"\u001b[39;49m\u001b[39mintervention\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtitre\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mprofession\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     76\u001b[0m     loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(x_hat, y)\n\u001b[1;32m     77\u001b[0m     \u001b[39m# Logging to TensorBoard by default\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 69\u001b[0m, in \u001b[0;36mSeanceLitClassifier.forward\u001b[0;34m(self, intervention, titre, profession)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, intervention, titre, profession):\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(intervention, titre, profession)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[58], line 46\u001b[0m, in \u001b[0;36mSeanceClassifier.forward\u001b[0;34m(self, intervention, titre, profession)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, intervention, titre, profession):\n\u001b[1;32m     45\u001b[0m     \u001b[39m# Embed interventions\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     _, intervention_pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m     47\u001b[0m         input_ids\u001b[39m=\u001b[39;49mintervention[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     48\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mintervention[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     49\u001b[0m         return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[39m# Embed titre\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     _, titre_pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[1;32m     54\u001b[0m         input_ids\u001b[39m=\u001b[39mtitre[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),\n\u001b[1;32m     55\u001b[0m         attention_mask\u001b[39m=\u001b[39mtitre[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),\n\u001b[1;32m     56\u001b[0m         return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:898\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    891\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    892\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    893\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    897\u001b[0m )\n\u001b[0;32m--> 898\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    899\u001b[0m     embedding_output,\n\u001b[1;32m    900\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    901\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    902\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    903\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    904\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    905\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    906\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    907\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    908\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    911\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:534\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    525\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    526\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    527\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    533\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    535\u001b[0m         hidden_states,\n\u001b[1;32m    536\u001b[0m         attention_mask,\n\u001b[1;32m    537\u001b[0m         layer_head_mask,\n\u001b[1;32m    538\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    539\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    540\u001b[0m         past_key_value,\n\u001b[1;32m    541\u001b[0m         output_attentions,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:419\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    409\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    417\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    420\u001b[0m         hidden_states,\n\u001b[1;32m    421\u001b[0m         attention_mask,\n\u001b[1;32m    422\u001b[0m         head_mask,\n\u001b[1;32m    423\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    424\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    428\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:346\u001b[0m, in \u001b[0;36mCamembertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 346\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    347\u001b[0m         hidden_states,\n\u001b[1;32m    348\u001b[0m         attention_mask,\n\u001b[1;32m    349\u001b[0m         head_mask,\n\u001b[1;32m    350\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    351\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    352\u001b[0m         past_key_value,\n\u001b[1;32m    353\u001b[0m         output_attentions,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    356\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:276\u001b[0m, in \u001b[0;36mCamembertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    272\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    278\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 7.80 GiB total capacity; 6.45 GiB already allocated; 392.19 MiB free; 6.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.fit(lt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 82011), started 0:05:05 ago. (Use '!kill 82011' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-361de3dbe4ec8bab\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-361de3dbe4ec8bab\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ../lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seance_id</th>\n",
       "      <th>date</th>\n",
       "      <th>moment</th>\n",
       "      <th>type</th>\n",
       "      <th>titre</th>\n",
       "      <th>titre_complet</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>intervention</th>\n",
       "      <th>nb_mots</th>\n",
       "      <th>nom</th>\n",
       "      <th>parlementaire</th>\n",
       "      <th>sexe</th>\n",
       "      <th>parlementaire_groupe_acronyme</th>\n",
       "      <th>fonction</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>15:00</td>\n",
       "      <td>loi</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>110</td>\n",
       "      <td>&lt;p&gt;Monsieur le président, sans vouloir compliq...</td>\n",
       "      <td>230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jean-Luc Mélenchon</td>\n",
       "      <td>H</td>\n",
       "      <td>LFI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2016-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>15:00</td>\n",
       "      <td>loi</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>150</td>\n",
       "      <td>&lt;p&gt;Monsieur le président, nous avons en effet ...</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Christian Jacob</td>\n",
       "      <td>H</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2016-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>15:00</td>\n",
       "      <td>loi</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>190</td>\n",
       "      <td>&lt;p&gt;Mes chers collègues, nous vivons une situat...</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Olivier Faure</td>\n",
       "      <td>H</td>\n",
       "      <td>NG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2016-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>15:00</td>\n",
       "      <td>loi</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>230</td>\n",
       "      <td>&lt;p&gt;Monsieur le président, chers collègues, pou...</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Franck Riester</td>\n",
       "      <td>H</td>\n",
       "      <td>LC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2016-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>15:00</td>\n",
       "      <td>loi</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>Élection des six vice-présidents et des trois ...</td>\n",
       "      <td>250</td>\n",
       "      <td>&lt;p&gt;à tous les membres des groupes de l'opposit...</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Franck Riester</td>\n",
       "      <td>H</td>\n",
       "      <td>LC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2016-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211984</th>\n",
       "      <td>288222</td>\n",
       "      <td>1698</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>21:30</td>\n",
       "      <td>loi</td>\n",
       "      <td>discussion générale</td>\n",
       "      <td>défense du droit de propriété &gt; discussion gén...</td>\n",
       "      <td>4100</td>\n",
       "      <td>&lt;p&gt;Évidemment qu'il faut en avoir un. On ne se...</td>\n",
       "      <td>214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Julien Aubert</td>\n",
       "      <td>H</td>\n",
       "      <td>LR</td>\n",
       "      <td>rapporteur de la commission des affaires écono...</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2017-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211986</th>\n",
       "      <td>288225</td>\n",
       "      <td>1698</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>21:30</td>\n",
       "      <td>loi</td>\n",
       "      <td>discussion générale</td>\n",
       "      <td>défense du droit de propriété &gt; discussion gén...</td>\n",
       "      <td>4120</td>\n",
       "      <td>&lt;p&gt;Excellent !&lt;/p&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sophie Auconie</td>\n",
       "      <td>F</td>\n",
       "      <td>UAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2017-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211987</th>\n",
       "      <td>288224</td>\n",
       "      <td>1698</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>21:30</td>\n",
       "      <td>loi</td>\n",
       "      <td>discussion générale</td>\n",
       "      <td>défense du droit de propriété &gt; discussion gén...</td>\n",
       "      <td>4121</td>\n",
       "      <td>&lt;p&gt;Excellent !&lt;/p&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Constance Le Grip</td>\n",
       "      <td>F</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2017-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211989</th>\n",
       "      <td>288227</td>\n",
       "      <td>1698</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>21:30</td>\n",
       "      <td>loi</td>\n",
       "      <td>discussion générale</td>\n",
       "      <td>défense du droit de propriété &gt; discussion gén...</td>\n",
       "      <td>4140</td>\n",
       "      <td>&lt;p&gt;Je n'aurai pas le temps de la défendre, mon...</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damien Adam</td>\n",
       "      <td>H</td>\n",
       "      <td>LREM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2017-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211990</th>\n",
       "      <td>288228</td>\n",
       "      <td>1698</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>21:30</td>\n",
       "      <td>loi</td>\n",
       "      <td>discussion générale</td>\n",
       "      <td>défense du droit de propriété &gt; discussion gén...</td>\n",
       "      <td>4150</td>\n",
       "      <td>&lt;p&gt;Si l'on n'a pas le temps de la mettre aux v...</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roland Lescure</td>\n",
       "      <td>H</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président de la commission des affaires économ...</td>\n",
       "      <td>http://www.assemblee-nationale.fr/15/cri/2017-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95185 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  seance_id        date moment type  \\\n",
       "57          58          2  2017-06-28  15:00  loi   \n",
       "60          61          2  2017-06-28  15:00  loi   \n",
       "63          64          2  2017-06-28  15:00  loi   \n",
       "66          67          2  2017-06-28  15:00  loi   \n",
       "68          69          2  2017-06-28  15:00  loi   \n",
       "...        ...        ...         ...    ...  ...   \n",
       "211984  288222       1698  2018-06-21  21:30  loi   \n",
       "211986  288225       1698  2018-06-21  21:30  loi   \n",
       "211987  288224       1698  2018-06-21  21:30  loi   \n",
       "211989  288227       1698  2018-06-21  21:30  loi   \n",
       "211990  288228       1698  2018-06-21  21:30  loi   \n",
       "\n",
       "                                                    titre  \\\n",
       "57      Élection des six vice-présidents et des trois ...   \n",
       "60      Élection des six vice-présidents et des trois ...   \n",
       "63      Élection des six vice-présidents et des trois ...   \n",
       "66      Élection des six vice-présidents et des trois ...   \n",
       "68      Élection des six vice-présidents et des trois ...   \n",
       "...                                                   ...   \n",
       "211984                                discussion générale   \n",
       "211986                                discussion générale   \n",
       "211987                                discussion générale   \n",
       "211989                                discussion générale   \n",
       "211990                                discussion générale   \n",
       "\n",
       "                                            titre_complet  timestamp  \\\n",
       "57      Élection des six vice-présidents et des trois ...        110   \n",
       "60      Élection des six vice-présidents et des trois ...        150   \n",
       "63      Élection des six vice-présidents et des trois ...        190   \n",
       "66      Élection des six vice-présidents et des trois ...        230   \n",
       "68      Élection des six vice-présidents et des trois ...        250   \n",
       "...                                                   ...        ...   \n",
       "211984  défense du droit de propriété > discussion gén...       4100   \n",
       "211986  défense du droit de propriété > discussion gén...       4120   \n",
       "211987  défense du droit de propriété > discussion gén...       4121   \n",
       "211989  défense du droit de propriété > discussion gén...       4140   \n",
       "211990  défense du droit de propriété > discussion gén...       4150   \n",
       "\n",
       "                                             intervention  nb_mots  nom  \\\n",
       "57      <p>Monsieur le président, sans vouloir compliq...      230  NaN   \n",
       "60      <p>Monsieur le président, nous avons en effet ...      138  NaN   \n",
       "63      <p>Mes chers collègues, nous vivons une situat...      182  NaN   \n",
       "66      <p>Monsieur le président, chers collègues, pou...      147  NaN   \n",
       "68      <p>à tous les membres des groupes de l'opposit...       32  NaN   \n",
       "...                                                   ...      ...  ...   \n",
       "211984  <p>Évidemment qu'il faut en avoir un. On ne se...      214  NaN   \n",
       "211986                                 <p>Excellent !</p>        3  NaN   \n",
       "211987                                 <p>Excellent !</p>        3  NaN   \n",
       "211989  <p>Je n'aurai pas le temps de la défendre, mon...       15  NaN   \n",
       "211990  <p>Si l'on n'a pas le temps de la mettre aux v...       22  NaN   \n",
       "\n",
       "             parlementaire sexe parlementaire_groupe_acronyme  \\\n",
       "57      Jean-Luc Mélenchon    H                           LFI   \n",
       "60         Christian Jacob    H                            LR   \n",
       "63           Olivier Faure    H                            NG   \n",
       "66          Franck Riester    H                            LC   \n",
       "68          Franck Riester    H                            LC   \n",
       "...                    ...  ...                           ...   \n",
       "211984       Julien Aubert    H                            LR   \n",
       "211986      Sophie Auconie    F                           UAI   \n",
       "211987   Constance Le Grip    F                            LR   \n",
       "211989         Damien Adam    H                          LREM   \n",
       "211990      Roland Lescure    H                          LREM   \n",
       "\n",
       "                                                 fonction  \\\n",
       "57                                                    NaN   \n",
       "60                                                    NaN   \n",
       "63                                                    NaN   \n",
       "66                                                    NaN   \n",
       "68                                                    NaN   \n",
       "...                                                   ...   \n",
       "211984  rapporteur de la commission des affaires écono...   \n",
       "211986                                                NaN   \n",
       "211987                                                NaN   \n",
       "211989                                                NaN   \n",
       "211990  président de la commission des affaires économ...   \n",
       "\n",
       "                                                   source  \n",
       "57      http://www.assemblee-nationale.fr/15/cri/2016-...  \n",
       "60      http://www.assemblee-nationale.fr/15/cri/2016-...  \n",
       "63      http://www.assemblee-nationale.fr/15/cri/2016-...  \n",
       "66      http://www.assemblee-nationale.fr/15/cri/2016-...  \n",
       "68      http://www.assemblee-nationale.fr/15/cri/2016-...  \n",
       "...                                                   ...  \n",
       "211984  http://www.assemblee-nationale.fr/15/cri/2017-...  \n",
       "211986  http://www.assemblee-nationale.fr/15/cri/2017-...  \n",
       "211987  http://www.assemblee-nationale.fr/15/cri/2017-...  \n",
       "211989  http://www.assemblee-nationale.fr/15/cri/2017-...  \n",
       "211990  http://www.assemblee-nationale.fr/15/cri/2017-...  \n",
       "\n",
       "[95185 rows x 16 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf_without_pres[tdf_without_pres.parlementaire.isin((tdf_without_pres.parlementaire.value_counts() > 10).index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "président de la commission des finances, de l'économie générale et du contrôle budgétaire                                      354\n",
       "président de la commission des affaires économiques                                                                            137\n",
       "présidente de la commission des affaires sociales                                                                              129\n",
       "président de la commission des affaires culturelles et de l'éducation                                                          117\n",
       "présidente de la commission du développement durable et de l'aménagement du territoire                                          68\n",
       "présidente de la commission des affaires étrangères                                                                             55\n",
       "président de la commission des finances                                                                                         29\n",
       "présidente de la commission des lois constitutionnelles, de la législation et de l'administration générale de la République     21\n",
       "président, doyen d'âge                                                                                                          18\n",
       "vice-présidente de la commission des affaires économiques                                                                       14\n",
       "présidente de la commission des affaires européennes                                                                            14\n",
       "président de la commission de la défense nationale et des forces armées                                                         10\n",
       "vice-président de la commission mixte paritaire                                                                                  8\n",
       "Vice-présidente de la commission des finances                                                                                    8\n",
       "vice-présidente de la commission des finances, de l'économie générale et du contrôle budgétaire                                  7\n",
       "vice-présidente de la commission des finances                                                                                    7\n",
       "vice-président de la commission des finances, de l'économie générale et du contrôle budgétaire                                   6\n",
       "président du Bundestag                                                                                                           6\n",
       "présidente de la commission spéciale chargée d'examiner le projet de loi pour un état au service d'une société de confiance      6\n",
       "Premier président de la Cour des comptes                                                                                         5\n",
       "Name: fonction, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf_without_pres[tdf_without_pres.fonction.fillna(\"\").str.contains(\"président\")].fonction.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intervention</th>\n",
       "      <th>nom</th>\n",
       "      <th>parlementaire</th>\n",
       "      <th>parlementaire_groupe_acronyme</th>\n",
       "      <th>fonction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>&lt;p&gt;Je vais réunir les présidents de groupe.&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>&lt;p&gt;La séance est suspendue.&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>&lt;p&gt;La séance est reprise.&lt;/p&gt;&lt;p&gt;Mes chers coll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>&lt;p&gt;La séance est suspendue.&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>&lt;p&gt;La séance est reprise.&lt;/p&gt;&lt;p&gt;Avant de repre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>&lt;p&gt;Monsieur le président, la majorité a décidé...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Christian Jacob</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>&lt;p&gt;Monsieur le président, tant que les droits ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Christian Jacob</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>&lt;p&gt;La parole est à M. Richard Ferrand, préside...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>&lt;p&gt;M. Jacob m'apparaît un peu audacieux quand ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richard Ferrand</td>\n",
       "      <td>LREM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>&lt;p&gt;Avec des questeurs tous désignés par La Rép...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Olivier Marleix</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>&lt;p&gt;Si toute décision prise par un groupe est r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richard Ferrand</td>\n",
       "      <td>LREM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>&lt;p&gt;Il fallait y penser avant !&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bernard Deflesselles</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>&lt;p&gt;Il est nécessaire d'avancer, quels que soie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richard Ferrand</td>\n",
       "      <td>LREM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>&lt;p&gt;nous estimons qu'il faut constituer sans dé...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richard Ferrand</td>\n",
       "      <td>LREM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>&lt;p&gt;C'est déjà fait !&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patrice Verchère</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>&lt;p&gt;…et pour briguer de nouveaux postes à respo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richard Ferrand</td>\n",
       "      <td>LREM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>&lt;p&gt;La parole est à M. Olivier Faure.&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>François de Rugy</td>\n",
       "      <td>LREM</td>\n",
       "      <td>président</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>&lt;p&gt;Monsieur le président, mes chers collègues,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Olivier Faure</td>\n",
       "      <td>NG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>&lt;p&gt;C'est vrai !&lt;/p&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damien Abad</td>\n",
       "      <td>LR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>&lt;p&gt;…et je le regrette. Je le regrette amèremen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Olivier Faure</td>\n",
       "      <td>NG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          intervention  nom  \\\n",
       "109    <p>Je vais réunir les présidents de groupe.</p>  NaN   \n",
       "110                    <p>La séance est suspendue.</p>  NaN   \n",
       "112  <p>La séance est reprise.</p><p>Mes chers coll...  NaN   \n",
       "113                    <p>La séance est suspendue.</p>  NaN   \n",
       "115  <p>La séance est reprise.</p><p>Avant de repre...  NaN   \n",
       "116  <p>Monsieur le président, la majorité a décidé...  NaN   \n",
       "118  <p>Monsieur le président, tant que les droits ...  NaN   \n",
       "120  <p>La parole est à M. Richard Ferrand, préside...  NaN   \n",
       "121  <p>M. Jacob m'apparaît un peu audacieux quand ...  NaN   \n",
       "122  <p>Avec des questeurs tous désignés par La Rép...  NaN   \n",
       "123  <p>Si toute décision prise par un groupe est r...  NaN   \n",
       "124                 <p>Il fallait y penser avant !</p>  NaN   \n",
       "125  <p>Il est nécessaire d'avancer, quels que soie...  NaN   \n",
       "127  <p>nous estimons qu'il faut constituer sans dé...  NaN   \n",
       "128                           <p>C'est déjà fait !</p>  NaN   \n",
       "129  <p>…et pour briguer de nouveaux postes à respo...  NaN   \n",
       "131           <p>La parole est à M. Olivier Faure.</p>  NaN   \n",
       "132  <p>Monsieur le président, mes chers collègues,...  NaN   \n",
       "133                                <p>C'est vrai !</p>  NaN   \n",
       "134  <p>…et je le regrette. Je le regrette amèremen...  NaN   \n",
       "\n",
       "            parlementaire parlementaire_groupe_acronyme   fonction  \n",
       "109      François de Rugy                          LREM  président  \n",
       "110      François de Rugy                          LREM  président  \n",
       "112      François de Rugy                          LREM  président  \n",
       "113      François de Rugy                          LREM  président  \n",
       "115      François de Rugy                          LREM  président  \n",
       "116       Christian Jacob                            LR        NaN  \n",
       "118       Christian Jacob                            LR        NaN  \n",
       "120      François de Rugy                          LREM  président  \n",
       "121       Richard Ferrand                          LREM        NaN  \n",
       "122       Olivier Marleix                            LR        NaN  \n",
       "123       Richard Ferrand                          LREM        NaN  \n",
       "124  Bernard Deflesselles                            LR        NaN  \n",
       "125       Richard Ferrand                          LREM        NaN  \n",
       "127       Richard Ferrand                          LREM        NaN  \n",
       "128      Patrice Verchère                            LR        NaN  \n",
       "129       Richard Ferrand                          LREM        NaN  \n",
       "131      François de Rugy                          LREM  président  \n",
       "132         Olivier Faure                            NG        NaN  \n",
       "133           Damien Abad                            LR        NaN  \n",
       "134         Olivier Faure                            NG        NaN  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf[~tdf['parlementaire_groupe_acronyme'].isna()][60:80][[\"intervention\", \"nom\", \"parlementaire\", \"parlementaire_groupe_acronyme\", \"fonction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for dep in slug_files:\n",
    "    tmp = deputies_df[deputies_df.slug == dep]\n",
    "\n",
    "    with open(file_dict[dep], \"r\", encoding=\"utf-8\") as f:\n",
    "        ints = json.load(f)\n",
    "\n",
    "    ints_list = [\n",
    "        {\n",
    "            \"intervention\": bs4.BeautifulSoup(intervention[\"intervention\"]).text,\n",
    "            \"nb_mots\": intervention[\"nb_mots\"],\n",
    "            \"seance_id\": intervention[\"seance_id\"],\n",
    "            \"section_id\": intervention[\"section_id\"],\n",
    "            \"type\": intervention[\"type\"],\n",
    "            \"date\": intervention[\"date\"],\n",
    "        } for i, intervention in enumerate(ints[\"interventions\"]) if i < 500\n",
    "    ]\n",
    "    \n",
    "    video = \"(disponible uniquement en vidéo)\"\n",
    "    ints_list = [x for x in ints_list if (x[\"intervention\"] != video) and (x[\"nb_mots\"] > 3)]\n",
    "\n",
    "    tokens = tokenize([x[\"intervention\"] for x in ints_list])\n",
    "\n",
    "    value = {\n",
    "        \"nom_circo\": tmp[\"nom_circo\"].iloc[0],\n",
    "        \"ancien_depute\": tmp[\"ancien_depute\"].iloc[0],\n",
    "        \"groupe_sigle\": tmp[\"groupe_sigle\"].iloc[0],\n",
    "        \"slug\": tmp[\"slug\"].iloc[0],\n",
    "        \"nom\": tmp[\"nom\"].iloc[0],\n",
    "        \"nb_mandats\": tmp[\"nb_mandats\"].iloc[0],\n",
    "        \"nb_interventions\": ints[\"last_result\"],\n",
    "        \"nb_ints\": len(ints_list),\n",
    "        \"interventions\": ints_list,\n",
    "        \"tokens\": tokens,\n",
    "    }\n",
    "\n",
    "    data.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837585333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/2017-2022/tokenized.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "os.stat(\"../data/2017-2022/tokenized.pkl\").st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/2017-2022/tokenized.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [x[\"nb_ints\"] for x in data]\n",
    "all_input_ids = [x[\"tokens\"][\"input_ids\"] for x in data]\n",
    "all_masks = [x[\"tokens\"][\"attention_mask\"] for x in data]\n",
    "labels = [groups_to_int[x[\"groupe_sigle\"]] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_ids[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = nn.utils.rnn.pad_sequence(all_input_ids, batch_first=True, padding_value=1)\n",
    "padded_masks = nn.utils.rnn.pad_sequence(all_masks, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(padded_inputs, \"../data/2017-2022/2017-2022_padded_inputs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = {\n",
    "    \"tokens\": padded_inputs,\n",
    "    \"masks\": padded_masks,\n",
    "    \"interventions_masks\": torch.all(padded_masks== 0, dim=2),\n",
    "    \"labels\": np.array(labels)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tokens_list, \"../data/2017-2022/tokens_list.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = torch.load(\"../data/2017-2022/tokens_list.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(disponible uniquement en vidéo)<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens(tokens_list[\"tokens\"][0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_frac = 0.25\n",
    "X, y = np.arange(len(tokens_list[\"labels\"])), tokens_list[\"labels\"]\n",
    "train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    stratify=y,\n",
    "    test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_list = {\n",
    "    \"tokens\": tokens_list[\"tokens\"][train_idx],\n",
    "    \"masks\": tokens_list[\"masks\"][train_idx],\n",
    "    \"interventions_masks\": tokens_list[\"interventions_masks\"][train_idx],\n",
    "    \"labels\": tokens_list[\"labels\"][train_idx]\n",
    "}\n",
    "test_tokens_list = {\n",
    "    \"tokens\": tokens_list[\"tokens\"][test_idx],\n",
    "    \"masks\": tokens_list[\"masks\"][test_idx],\n",
    "    \"interventions_masks\": tokens_list[\"interventions_masks\"][test_idx],\n",
    "    \"labels\": tokens_list[\"labels\"][test_idx]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, count = np.unique(test_tokens_list[\"labels\"], return_counts=True)\n",
    "test_party_count = pd.DataFrame({\n",
    "    \"party\": unique,\n",
    "    \"count\": count\n",
    "})\n",
    "test_party_count[\"freq\"] = test_party_count[\"count\"]/test_party_count[\"count\"].sum()\n",
    "test_party_count[\"party_name\"] = test_party_count[\"party\"].apply(lambda x: int_to_groups[x])\n",
    "test_party_count[\"type\"] = \"test\"\n",
    "\n",
    "\n",
    "unique, count = np.unique(train_tokens_list[\"labels\"], return_counts=True)\n",
    "train_party_count = pd.DataFrame({\n",
    "    \"party\": unique,\n",
    "    \"count\": count\n",
    "})\n",
    "train_party_count[\"freq\"] = train_party_count[\"count\"]/train_party_count[\"count\"].sum()\n",
    "train_party_count[\"party_name\"] = train_party_count[\"party\"].apply(lambda x: int_to_groups[x])\n",
    "train_party_count[\"type\"] = \"train\"\n",
    "\n",
    "\n",
    "unique, count = np.unique(tokens_list[\"labels\"], return_counts=True)\n",
    "party_count = pd.DataFrame({\n",
    "    \"party\": unique,\n",
    "    \"count\": count\n",
    "})\n",
    "party_count[\"freq\"] = party_count[\"count\"]/party_count[\"count\"].sum()\n",
    "party_count[\"party_name\"] = party_count[\"party\"].apply(lambda x: int_to_groups[x])\n",
    "party_count[\"type\"] = \"total\"\n",
    "\n",
    "party_count = pd.concat([party_count, train_party_count, test_party_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='party_name', ylabel='freq'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA65klEQVR4nO3deVyVdf7//+cBBGQVEUEThNxJzYSx0CntU4LamKamqankko5Lmls5zri1WOM+Nm6TgFZjWGrZfJ0RWlxSx200p9RcCRfMpcQtQeH6/eFwfh0P6EEPHLh83G+366bnfd7Xdb2uw+HwPO9rsxiGYQgAAMAk3FxdAAAAgDMRbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKl4uLqA0pafn6+TJ0/K399fFovF1eUAAAAHGIahixcvqnr16nJzu/XYzD0Xbk6ePKnw8HBXlwEAAO7AsWPHVKNGjVv2uefCjb+/v6QbL05AQICLqwEAAI64cOGCwsPDrX/Hb+WeCzcFu6ICAgIINwAAlDOOHFLCAcUAAMBUCDcAAMBUCDcAAMBU7rljbgAAKEl5eXm6du2aq8solzw9PW97mrcjCDcAADiBYRg6deqUzp8/7+pSyi03NzdFRUXJ09PzrpZDuAEAwAkKgk3VqlXl4+PDhWKLqeAiu1lZWYqIiLir149wAwDAXcrLy7MGm+DgYFeXU26FhITo5MmTun79uipUqHDHy+GAYgAA7lLBMTY+Pj4urqR8K9gdlZeXd1fLIdwAAOAk7Iq6O856/Qg3AADAVAg3AADAVAg3AADAVAg3AACUM61atdKIESNcXUaZRbgBAACmQrgBAKAcSUxM1Pr16zVnzhxZLBZZLBZ5eHho+vTpNv2+/fZbubm56fDhw5JunIk0f/58tW3bVhUrVlRUVJQ++ugjm3lOnDihbt26KSgoSMHBwerQoYMyMjJKa9Ochov4/U/MmKUO913lP83hvhET/nsn5QAAUKg5c+bowIEDatiwoaZMmSJJWrx4sZKTkzV69Ghrv6SkJD366KOqVauWte1Pf/qT3nrrLc2ZM0fvvfeeunfvroYNG6pBgwa6cuWKHn/8cT366KPasGGDPDw89Prrr6tNmzbas2fPXd8SoTQRbkpYi7ktHO67adimEqwEAGAGgYGB8vT0lI+Pj8LCwiRJffv21cSJE7Vt2zY1a9ZM165d0/vvv69p02y/jD/77LPq37+/JOm1115Tenq65s6dq3nz5unDDz+Um5ub3n33Xev1ZpKTk1WpUiWtW7dO8fHxpbuhd4HdUgAAlHPVqlXTU089paSkJEnSP/7xD129elXPPvusTb+4uDi7x/v27ZMk7dy5U4cOHZK/v7/8/Pzk5+enypUr6+rVq9ZdW+UFIzcAAJhA//791atXL82aNUvJycnq1q2bQ7eDKBilyc/PV0xMjD744AO7PiEhIU6vtyQRbgAAKGc8PT3t7r/Url07+fr6av78+frnP/+pDRs22M3373//W71797Z5/NBDD0mSmjZtqtTUVFWtWlUBAQEluwEljN1SAACUM5GRkdq6dasyMjJ09uxZ5efny93dXYmJiRo3bpxq165ttwtKkj766CMlJSXpwIED1mN0hg4dKknq2bOnqlSpog4dOmjjxo06evSo1q9fr+HDh+v48eOlvYl3hXADAEA5M3r0aLm7uys6OlohISHKzMyUJPXr10+5ubnq27dvofNNnjxZH374oRo3bqwlS5bogw8+UHR0tKQbdzTfsGGDIiIi1KlTJzVo0EB9+/bVL7/8Uu5GctgtBQBAOVO3bl1t2bLFrj0rK0seHh42u55+rXr16kpLSytyuWFhYVqyZInT6nQVwg0AAOVcTk6Ojh07pj/96U/q2rWrQkNDXV2SS7FbCgCAcm7ZsmWqV6+esrOz9ec//9nV5bgcIzcAAJRziYmJSkxMvGUfwzBKp5gygJEbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABQKhITE9WxY8cSXw+nggMAUIJixiwttXXtnFb4lYlvpVWrVmrSpIlmz55dovOUJkZuAACAqRBuAAC4RyUmJmr9+vWaM2eOLBaLLBaLMjIytH79ejVr1kxeXl6qVq2aXn31VV2/fv2W8+Tl5alfv36KiopSxYoVVa9ePc2ZM8cl28VuKQAA7lFz5szRgQMH1LBhQ02ZMkWSlJeXp3bt2ikxMVFLly7V/v37NWDAAHl7e2vSpEmFzhMSEqL8/HzVqFFDy5cvV5UqVbR582a9+OKLqlatmrp27Vqq20W4AQDgHhUYGChPT0/5+PgoLCxMkjR+/HiFh4frnXfekcViUf369XXy5Em98sormjBhQqHzSJK7u7smT55sfRwVFaXNmzdr+fLlpR5u2C0FAACs9u3bp7i4OFksFmtbixYtdOnSJR0/fvyW8y5YsECxsbEKCQmRn5+f/va3vykzM7OkS7ZDuAEAAFaGYdgEm4I2SXbtv7Z8+XK9/PLL6tu3r9LS0rR792698MILys3NLdF6C8NuKQAA7mGenp7Ky8uzPo6OjtaKFStsQs7mzZvl7++v++67r9B5JGnjxo1q3ry5Bg8ebG07fPhwKWyBPUZuAAC4h0VGRmrr1q3KyMjQ2bNnNXjwYB07dkzDhg3T/v379emnn2rixIkaOXKk3NzcCp0nPz9ftWvX1o4dO7R27VodOHBAf/rTn7R9+3aXbBPhBgCAe9jo0aPl7u6u6OhohYSE6Nq1a1qzZo22bdumBx98UIMGDVK/fv30xz/+sch5MjMzNWjQIHXq1EndunXTww8/rHPnztmM4pQmi1GwI+0eceHCBQUGBio7O1sBAQHW9uJcQXKV/zSH+3YPCrh9p//ZNGyTw30BAGXH1atXdfToUUVFRcnb29vV5ZRbt3odi/r7XRhGbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgFNERkZq9uzZri6Du4IDAFCSMqc0KrV1RUz4b7HnadWqlZo0aeKUULJ9+3b5+vre9XLuFuEGAAAUyTAM5eXlycPj9pEhJCSkFCq6PXZLAQBwj0pMTNT69es1Z84cWSwWWSwWpaSkyGKxaO3atYqNjZWXl5c2btyow4cPq0OHDgoNDZWfn59+85vf6PPPP7dZ3s27pSwWi959910988wz8vHxUZ06dbR69eoS3y7CDQAA96g5c+YoLi5OAwYMUFZWlrKyshQeHi5JGjt2rKZOnap9+/apcePGunTpktq1a6fPP/9cu3btUkJCgtq3b6/MzMxbrmPy5Mnq2rWr9uzZo3bt2qlnz5766aefSnS7CDcAANyjAgMD5enpKR8fH4WFhSksLEzu7u6SpClTpqh169aqVauWgoOD9eCDD2rgwIFq1KiR6tSpo9dff13333//bUdiEhMT1b17d9WuXVtvvvmmLl++rG3btpXodhFuAACAndjYWJvHly9f1tixYxUdHa1KlSrJz89P+/fvv+3ITePGja3/9/X1lb+/v06fPl0iNRfggGIAAGDn5rOexowZo7Vr12r69OmqXbu2KlasqC5duig3N/eWy6lQoYLNY4vFovz8fKfX+2uEGwAA7mGenp7Ky8u7bb+NGzcqMTFRzzzzjCTp0qVLysjIKOHq7gy7pQAAuIdFRkZq69atysjI0NmzZ4scValdu7ZWrlyp3bt365tvvlGPHj1KfATmTrk83MybN09RUVHy9vZWTEyMNm7c6NB8mzZtkoeHh5o0aVKyBQIAYGKjR4+Wu7u7oqOjFRISUuQxNLNmzVJQUJCaN2+u9u3bKyEhQU2bNi3lah3j0t1SqampGjFihObNm6cWLVpo4cKFatu2rfbu3auIiIgi58vOzlbv3r31xBNP6McffyzFigEAKJ47uWpwaapbt662bNli05aYmGjXLzIyUl9++aVN25AhQ2we37ybyjAMu+WcP3/+juosDpeO3MycOVP9+vVT//791aBBA82ePVvh4eGaP3/+LecbOHCgevToobi4uNuuIycnRxcuXLCZAACAebks3OTm5mrnzp2Kj4+3aY+Pj9fmzZuLnC85OVmHDx/WxIkTHVrP1KlTFRgYaJ0KLk4EAADMyWXh5uzZs8rLy1NoaKhNe2hoqE6dOlXoPAcPHtSrr76qDz74wKF7XEjSuHHjlJ2dbZ2OHTt217UDAICyy+WnglssFpvHhmHYtUlSXl6eevToocmTJ6tu3boOL9/Ly0teXl53XScAACgfXBZuqlSpInd3d7tRmtOnT9uN5kjSxYsXtWPHDu3atUtDhw6VJOXn58swDHl4eCgtLU3/93//Vyq1AwCAsstlu6U8PT0VExOj9PR0m/b09HQ1b97crn9AQID++9//avfu3dZp0KBBqlevnnbv3q2HH364tEoHAABlmEt3S40cOVK9evVSbGys4uLitGjRImVmZmrQoEGSbhwvc+LECS1dulRubm5q2LChzfxVq1aVt7e3XTsAALh3uTTcdOvWTefOndOUKVOUlZWlhg0bas2aNapZs6YkKSsr67Y35AIAAPg1i1HYFXZM7MKFCwoMDFR2drYCAgKs7TFjljq8jFX+0xzu2z0o4Pad/mfTsE0O9wUAlB1Xr17V0aNHrVfcx5251etY1N/vwrj89gsAAADO5PJTwQEAMLMWc1uU2rruZA9Aq1at1KRJE82ePdspNSQmJur8+fP65JNPnLK8O8HIDQAAMBXCDQAA96jExEStX79ec+bMkcVikcViUUZGhvbu3at27drJz89PoaGh6tWrl86ePWud7+OPP1ajRo1UsWJFBQcH68knn9Tly5c1adIkLVmyRJ9++ql1eevWrSv17SLcAABwj5ozZ47i4uI0YMAAZWVlKSsrSxUqVFDLli3VpEkT7dixQ//617/0448/qmvXrpJunMncvXt39e3bV/v27dO6devUqVMnGYah0aNHq2vXrmrTpo11eYVdu66kccwNAAD3qMDAQHl6esrHx0dhYWGSpAkTJqhp06Z68803rf2SkpIUHh6uAwcO6NKlS7p+/bo6depkvXRLo0aNrH0rVqyonJwc6/JcgXADAACsdu7cqa+++kp+fn52zx0+fFjx8fF64okn1KhRIyUkJCg+Pl5dunRRUFCQC6otHLulAACAVX5+vtq3b29zu6Pdu3fr4MGDeuyxx+Tu7q709HT985//VHR0tObOnat69erp6NGjri7dinADAMA9zNPTU3l5edbHTZs21XfffafIyEjVrl3bZvL19ZUkWSwWtWjRQpMnT9auXbvk6empVatWFbo8VyDcAABwD4uMjNTWrVuVkZGhs2fPasiQIfrpp5/UvXt3bdu2TUeOHFFaWpr69u2rvLw8bd26VW+++aZ27NihzMxMrVy5UmfOnFGDBg2sy9uzZ4++//57nT17VteuXSv1beKYGwAASlBZv7XO6NGj1adPH0VHR+uXX37R0aNHtWnTJr3yyitKSEhQTk6OatasqTZt2sjNzU0BAQHasGGDZs+erQsXLqhmzZqaMWOG2rZtK0kaMGCA1q1bp9jYWF26dElfffWVWrVqVarbRLgBAOAeVrduXW3ZssWufeXKlYX2b9Cggf71r38VubyQkBClpaU5rb47wW4pAABgKoQbAABgKoQbAABgKoQbAABgKoQbAACcxDAMV5dQrjnr9SPcAABwlypUqCBJunLliosrKd9yc3MlSe7u7ne1HE4FBwDgLrm7u6tSpUo6ffq0JMnHx0cWi8XFVZUv+fn5OnPmjHx8fOThcXfxhHADAIATFNwFuyDgoPjc3NwUERFx18GQcAMAgBNYLBZVq1ZNVatWdcktB8zA09NTbm53f8QM4QYAACdyd3e/62NGcHc4oBgAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJgK4QYAAJiKy8PNvHnzFBUVJW9vb8XExGjjxo1F9v3666/VokULBQcHq2LFiqpfv75mzZpVitUCAICyzsOVK09NTdWIESM0b948tWjRQgsXLlTbtm21d+9eRURE2PX39fXV0KFD1bhxY/n6+urrr7/WwIED5evrqxdffNEFWwAAAMoai2EYhqtW/vDDD6tp06aaP3++ta1Bgwbq2LGjpk6d6tAyOnXqJF9fX7333nsO9b9w4YICAwOVnZ2tgIAAa3vMmKUO173Kf5rDfbsHBdy+0/9sGrbJ4b4AANxLivr7XRiX7ZbKzc3Vzp07FR8fb9MeHx+vzZs3O7SMXbt2afPmzWrZsmWRfXJycnThwgWbCQAAmJfLws3Zs2eVl5en0NBQm/bQ0FCdOnXqlvPWqFFDXl5eio2N1ZAhQ9S/f/8i+06dOlWBgYHWKTw83Cn1AwCAssnlBxRbLBabx4Zh2LXdbOPGjdqxY4cWLFig2bNna9myZUX2HTdunLKzs63TsWPHnFI3AAAom1x2QHGVKlXk7u5uN0pz+vRpu9Gcm0VFRUmSGjVqpB9//FGTJk1S9+7dC+3r5eUlLy8v5xQNAADKPJeN3Hh6eiomJkbp6ek27enp6WrevLnDyzEMQzk5Oc4uDwAAlFMuPRV85MiR6tWrl2JjYxUXF6dFixYpMzNTgwYNknRjl9KJEye0dOmNM5n++te/KiIiQvXr15d047o306dP17Bhw1y2DQAAoGxxabjp1q2bzp07pylTpigrK0sNGzbUmjVrVLNmTUlSVlaWMjMzrf3z8/M1btw4HT16VB4eHqpVq5beeustDRw40FWbAAAAyhiXXufGFbjODQAA5U+5uM4NAABASSDcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAU/Eo7gwjR450uO/MmTOLu3gAAIC7Uuxws2vXLv3nP//R9evXVa9ePUnSgQMH5O7urqZNm1r7WSwW51UJAADgoGKHm/bt28vf319LlixRUFCQJOnnn3/WCy+8oEcffVSjRo1yepEAAACOKvYxNzNmzNDUqVOtwUaSgoKC9Prrr2vGjBlOLQ4AAKC4ih1uLly4oB9//NGu/fTp07p48aJTigIAALhTxQ43zzzzjF544QV9/PHHOn78uI4fP66PP/5Y/fr1U6dOnUqiRgAAAIcV+5ibBQsWaPTo0Xr++ed17dq1Gwvx8FC/fv00bdo0pxcIAABQHMUONz4+Ppo3b56mTZumw4cPyzAM1a5dW76+viVRHwAAQLHc8UX8srKylJWVpbp168rX11eGYTizLgAAgDtS7HBz7tw5PfHEE6pbt67atWunrKwsSVL//v05DRwAALhcscPNyy+/rAoVKigzM1M+Pj7W9m7duulf//qXU4sDAAAormIfc5OWlqa1a9eqRo0aNu116tTRDz/84LTCAAAA7kSxR24uX75sM2JT4OzZs/Ly8nJKUQAAAHeq2OHmscce09KlS62PLRaL8vPzNW3aND3++ONOLQ4AAKC4ir1batq0aWrVqpV27Nih3NxcjR07Vt99951++uknbdq0qSRqBAAAcFixR26io6O1Z88eNWvWTK1bt9bly5fVqVMn7dq1S7Vq1SqJGgEAABxWrJGba9euKT4+XgsXLtTkyZNLqiYAAIA7VqyRmwoVKujbb7+VxWIpqXoAAADuSrF3S/Xu3VuLFy8uiVoAAADuWrEPKM7NzdW7776r9PR0xcbG2t1TaubMmU4rDgAAoLgcCjd79uxRw4YN5ebmpm+//VZNmzaVJB04cMCmH7urAACAqzkUbh566CFlZWWpatWq+uGHH7R9+3YFBweXdG0AAADF5tAxN5UqVdLRo0clSRkZGcrPzy/RogAAAO6UQyM3nTt3VsuWLVWtWjVZLBbFxsbK3d290L5HjhxxaoEAAADF4VC4WbRokTp16qRDhw7ppZde0oABA+Tv71/StQEAABSbw2dLtWnTRpK0c+dODR8+nHADAADKpGKfCp6cnFwSdQAAADhFsS/iBwAAUJYRbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKm4PNzMmzdPUVFR8vb2VkxMjDZu3Fhk35UrV6p169YKCQlRQECA4uLitHbt2lKsFgAAlHUuDTepqakaMWKExo8fr127dunRRx9V27ZtlZmZWWj/DRs2qHXr1lqzZo127typxx9/XO3bt9euXbtKuXIAAFBWWQzDMFy18ocfflhNmzbV/PnzrW0NGjRQx44dNXXqVIeW8cADD6hbt26aMGGCQ/0vXLigwMBAZWdnKyAgwNoeM2apw3Wv8p/mcN/uQQG37/Q/m4ZtcrgvAAD3kqL+fhfGZSM3ubm52rlzp+Lj423a4+PjtXnzZoeWkZ+fr4sXL6py5cpF9snJydGFCxdsJgAAYF4uCzdnz55VXl6eQkNDbdpDQ0N16tQph5YxY8YMXb58WV27di2yz9SpUxUYGGidwsPD76puAABQtrn8gGKLxWLz2DAMu7bCLFu2TJMmTVJqaqqqVq1aZL9x48YpOzvbOh07duyuawYAAGWXh6tWXKVKFbm7u9uN0pw+fdpuNOdmqamp6tevnz766CM9+eSTt+zr5eUlLy+vu64XAACUDy4bufH09FRMTIzS09Nt2tPT09W8efMi51u2bJkSExP197//XU899VRJlwkAAMoZl43cSNLIkSPVq1cvxcbGKi4uTosWLVJmZqYGDRok6cYupRMnTmjp0htnMi1btky9e/fWnDlz9Mgjj1hHfSpWrKjAwECXbQcAACg7XBpuunXrpnPnzmnKlCnKyspSw4YNtWbNGtWsWVOSlJWVZXPNm4ULF+r69esaMmSIhgwZYm3v06ePUlJSSrt8AABQBrk03EjS4MGDNXjw4EKfuzmwrFu3ruQLAgAA5ZrLz5YCAABwJsINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFQ9XFwC4QsyYpQ73XeU/zeG+ERP+eyflAACciJEbAABgKozcAE7UYm4Lh/tuGrapBCsBgHsXIzcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUCDcAAMBUXB5u5s2bp6ioKHl7eysmJkYbN24ssm9WVpZ69OihevXqyc3NTSNGjCi9QgEAQLng0nCTmpqqESNGaPz48dq1a5ceffRRtW3bVpmZmYX2z8nJUUhIiMaPH68HH3ywlKsFAADlgYcrVz5z5kz169dP/fv3lyTNnj1ba9eu1fz58zV16lS7/pGRkZozZ44kKSkpyaF15OTkKCcnx/r4woULTqjcNWLGLHW47yr/aQ737R4U4HDfTcM2OdwXAABXcNnITW5urnbu3Kn4+Hib9vj4eG3evNlp65k6daoCAwOtU3h4uNOWDQAAyh6XhZuzZ88qLy9PoaGhNu2hoaE6deqU09Yzbtw4ZWdnW6djx445bdkAAKDsceluKUmyWCw2jw3DsGu7G15eXvLy8nLa8gAAQNnmspGbKlWqyN3d3W6U5vTp03ajOQAAAI5yWbjx9PRUTEyM0tPTbdrT09PVvHlzF1UFAADKO5fulho5cqR69eql2NhYxcXFadGiRcrMzNSgQYMk3The5sSJE1q69P8/S2j37t2SpEuXLunMmTPavXu3PD09FR0d7YpNAAAAZYxLw023bt107tw5TZkyRVlZWWrYsKHWrFmjmjVrSrpx0b6br3nz0EMPWf+/c+dO/f3vf1fNmjWVkZFRmqUDAIAyyuUHFA8ePFiDBw8u9LmUlBS7NsMwSrgiAABQnrn89gsAAADORLgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACm4uHqAgBHtJjbwuG+m4ZtKsFKAABlHSM3AADAVAg3AADAVNgtBaeKGbPU4b47p/UuwUoAAPcqwg1cJnNKI8c7BwWUXCEAAFNhtxQAADAVRm4A4A5xFh9QNjFyAwAATIVwAwAATIVwAwAATIVjbgCYHpcoAO4thBsA+BUuUQCUf+yWAgAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApsLZUgAApyvOWWcRE/5bgpXgXkS4AQA4pDjXC1rlX4KFALdBuAEAEysPN/csDzVK5adOcMwNAAAwGUZuAACmUrzdZ9McXzBXpC43CDcAUAZw/yvAeQg3AFDOcP8r4NY45gYAAJgK4QYAAJgKu6WAMqqkDorsXozdFJzOCpQMfr9LFiM3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVLgrOIBSkTmlkcN9Iyb8twQrAeBsZe332+UjN/PmzVNUVJS8vb0VExOjjRs33rL/+vXrFRMTI29vb91///1asGBBKVUKAADKA5eO3KSmpmrEiBGaN2+eWrRooYULF6pt27bau3evIiIi7PofPXpU7dq104ABA/T+++9r06ZNGjx4sEJCQtS5c2cXbAFwb4sZs9Thvqv8S7AQAE5Xnn+/XTpyM3PmTPXr10/9+/dXgwYNNHv2bIWHh2v+/PmF9l+wYIEiIiI0e/ZsNWjQQP3791ffvn01ffr0Uq4cAACUVS4bucnNzdXOnTv16quv2rTHx8dr8+bNhc6zZcsWxcfH27QlJCRo8eLFunbtmipUqGA3T05OjnJycqyPs7OzJUkXLlyw6ZeX84vDtV+skOdw3+u/XHe478013YwaHXO7GqXyUee9XKMjP8PiuJdfy3utRql81Hkv13inv98F8xmGcfvOhoucOHHCkGRs2rTJpv2NN94w6tatW+g8derUMd544w2btk2bNhmSjJMnTxY6z8SJEw1JTExMTExMTCaYjh07dtuM4fKzpSwWi81jwzDs2m7Xv7D2AuPGjdPIkSOtj/Pz8/XTTz8pODj4luspjgsXLig8PFzHjh1TQECAU5bpbOWhRql81EmNzlMe6qRG5ykPdVKj8zi7TsMwdPHiRVWvXv22fV0WbqpUqSJ3d3edOnXKpv306dMKDQ0tdJ6wsLBC+3t4eCg4OLjQeby8vOTl5WXTVqlSpTsv/BYCAgLK9BtNKh81SuWjTmp0nvJQJzU6T3mokxqdx5l1BgYGOtTPZQcUe3p6KiYmRunp6Tbt6enpat68eaHzxMXF2fVPS0tTbGxsocfbAACAe49Lz5YaOXKk3n33XSUlJWnfvn16+eWXlZmZqUGDBkm6sUupd+/e1v6DBg3SDz/8oJEjR2rfvn1KSkrS4sWLNXr0aFdtAgAAKGNcesxNt27ddO7cOU2ZMkVZWVlq2LCh1qxZo5o1a0qSsrKylJmZae0fFRWlNWvW6OWXX9Zf//pXVa9eXX/5y19cfo0bLy8vTZw40W73V1lSHmqUyked1Og85aFOanSe8lAnNTqPK+u0GIYj51QBAACUDy6//QIAAIAzEW4AAICpEG4AAICpEG4AAICp3PPhJjExUR07diz0ucjISFksFlksFlWsWFH169fXtGnTbO5rkZGRYe1z8/Tvf/9bkpSSkiKLxaIGDRrYrWP58uWyWCzy8/OTxWKxngb/a4MHD5bFYlFiYqK17dixY+rXr5+qV68uT09P1axZU8OHD9e5c+ds5m3VqpW1Hi8vL913331q3769Vq5cabeeorbjww8/lCStW7dOFotFQUFBunr1qs2827Zts/b/tdOnT2vgwIGKiIiQl5eXwsLClJCQoC1btlj7bN68We3atVNQUJC8vb3VqFEjzZgxQ3l59vcq+eqrr9SuXTsFBwfLx8dH0dHRGjVqlE6cOGHX927d7XvD2U6dOqXhw4erdu3a8vb2VmhoqH77299qwYIFunLlSqF1RUZGqmvXrvryyy9tlnXz+zYwMFCPPPKIPvvsM6fWnJiYKIvForfeesum/ZNPPrF5rxiGob/97W+Ki4tTQECA/Pz89MADD2j48OE6dOiQU2u623oLfg/Onz9fanXd7NfvzVt9BhVMkyZNKvUaN2/eLHd3d7Vp08am3ZHPzJLQqlUrjRgxwq791z/bgs9qi8Uid3d3BQUF6eGHH9aUKVOs9yUscKvPB2dw9POnYKpRo4bN87Nnzy6x2hx5LQv88ssvCgoKUuXKlfXLL/b3qiqpWu/5cHM7Baep79u3T6NHj9Yf/vAHLVq0yK7f559/rqysLJspJibG+ryvr69Onz5t80ddkpKSkhQRESFJCg8P14cffmjzBrh69aqWLVtm7SNJR44cUWxsrA4cOKBly5bp0KFDWrBggb744gvFxcXpp59+slnHgAEDlJWVpUOHDmnFihWKjo7Wc889pxdffNFuO5KTk+224+ZfMH9/f61atarI7fi1zp0765tvvtGSJUt04MABrV69Wq1atbLWuGrVKrVs2VI1atTQV199pf3792v48OF644039Nxzz9mEhYULF+rJJ59UWFiYVqxYob1792rBggXKzs7WjBkz7NZd0hx9bzjDkSNH9NBDDyktLU1vvvmmdu3apc8//1wvv/yyPvvsM33++ed2dX3//fdaunSpKlWqpCeffFJvvPGG3XIL3rdbt25Vs2bN1LlzZ3377bdOrd3b21tvv/22fv7550KfNwxDPXr00EsvvaR27dopLS1Ne/bs0V/+8hdVrFhRr7/+ulPrudt6y5rw8HCb39dRo0bpgQcesGlzxbXAkpKSNGzYMH399dc2l/QocLvPTFcJCAhQVlaWjh8/rs2bN+vFF1/U0qVL1aRJE508edLV5VkV/J4XTLt27XJ1SYVasWKFGjZsqOjo6EK/VJcUl99bqqzz9/dXWFiYJKl///6aP3++0tLSNHDgQJt+wcHB1n6F8fDwUI8ePZSUlKS4uDhJ0vHjx7Vu3Tq9/PLLmjt3rpo2baojR45o5cqV6tmzpyRp5cqVCg8P1/33329d1pAhQ+Tp6am0tDRVrFhRkhQREaGHHnpItWrV0vjx4zV//nxrfx8fH2tt4eHheuSRR1S/fn317dtXXbt21ZNPPmntW6lSpVtuhyT16dNHSUlJ6t69u6QbyfzDDz/USy+9pNdee83a7/z58/r666+1bt06tWzZUpJUs2ZNNWvWTJJ0+fJlDRgwQE8//bRNKOjfv79CQ0P19NNPa/ny5erWrZuOHz+ul156SS+99JJmzZpl7RsZGanHHnvMJd+iHX1vOMPgwYPl4eGhHTt2yNfX19reqFEjde7c2SYE/rquiIgIPfbYY6pWrZomTJigLl26qF69eta+Be/bsLAwvfHGG5o7d66++uorNWzY0Gm1P/nkkzp06JCmTp2qP//5z3bPp6am6sMPP9Snn36qp59+2tp+//3364knnijR0bDC3K7essbd3d3md9bPz08eHh63/T0uSZcvX9by5cu1fft2nTp1SikpKZowYYJNn9t9ZrqKxWKx1lWtWjU1aNBA7du31wMPPKCxY8fq/fffd3GFN/z697wsW7x4sZ5//nkZhqHFixdb/7aVNEZuHGQYhtatW6d9+/bd8a0e+vXrp9TUVOsuhJSUFLVp08bmXlovvPCCkpOTrY+TkpLUt29f6+OffvpJa9eu1eDBg63BpkBYWJh69uyp1NTU2/5B6NOnj4KCgu4oSffq1UsbN260fhtbsWKFIiMj1bRpU5t+fn5+8vPz0yeffKKcnBy75aSlpencuXOFfqts37696tatq2XLlkmSPvroI+Xm5mrs2LGF1lRS9wtzhDPeG7dy7tw5paWlaciQITbB5tdudxPY4cOHyzAMffrpp4U+f+3aNf3tb3+TJKdvg7u7u958803NnTtXx48ft3t+2bJlqlevnk2w+TVn3eDWUberF7eXmpqqevXqqV69enr++eeVnJxc6iHVmapWraqePXtq9erVhe4uR+EOHz6sLVu2qGvXruratas2b96sI0eOlMq6CTe38corr8jPz09eXl56/PHHZRiGXnrpJbt+zZs3t/4xL5hu/iVo0qSJatWqpY8//liGYSglJcUmuEg3gsPXX3+tjIwM/fDDD9q0aZOef/556/MHDx6UYRiFHr8jSQ0aNNDPP/+sM2fO3HK73NzcVLduXWVkZNi0d+/e3W47bn4zVq1aVW3btlVKSook+wBWwMPDQykpKVqyZIkqVaqkFi1a6A9/+IP27NkjSTpw4IC15sLUr1/f2ufgwYMKCAhQtWrVbrldpcnR98bdOnTokAzDsBlxkW7cfLbgZ/TKK6/cchmVK1dW1apV7X7eBe9bb29vjRo1ynqMjrM988wzatKkiSZOnGj33IEDB+y2bcSIEdZt+/WxBKXlVvXi9gq+rUtSmzZtdOnSJX3xxRc2fRz5zCxL6tevr4sXL9od1+gqBZ8/BdNf/vIXV5dkJykpSW3btrUec9OmTRslJSWVyroJN7cxZswY7d69W+vXr9fjjz+u8ePHF3pjz9TUVO3evdtmcnd3t+vXt29fJScna/369bp06ZLatWtn83yVKlX01FNPacmSJUpOTtZTTz2lKlWqOFxvwbcjR77tGoZh12/WrFl22xEeHl7odqSkpOjIkSPasmVLkUONnTt31smTJ7V69WolJCRo3bp1atq0qTUY/brmW9VXWK2u5uh7w1lu3v5t27Zp9+7deuCBBwodGbtZYa9hamqqdu3apdWrV6t27dp69913VblyZafWXeDtt9/WkiVLtHfvXrvnbq5r/Pjx2r17tyZMmKBLly6VSD23c6t6UbTvv/9e27Zt03PPPSfpxpecbt262f1Rc/Qzs6wozmdraSj4/CmYfn0fxrIgLy9PS5Yssfly/vzzz2vJkiWlEmI55uY2qlSpotq1a6t27dpasWKFateurUceecTmOBXpxrEstWvXvu3yevbsqbFjx2rSpEnq3bu3PDzsfwR9+/bV0KFDJUl//etfbZ6rXbu2LBaL9u7dW+iR9Pv371dQUNBtA1FeXp4OHjyo3/zmNzbtYWFhDm1Hu3btNHDgQPXr10/t27dXcHBwkX29vb3VunVrtW7dWhMmTFD//v01ceJE6xHy+/btKzQU7N+/X9HR0ZKkunXrKjs7W1lZWWVm9MbR98bdKviZ79+/36a94Dism3dPFubcuXM6c+aMoqKibNrDw8NVp04d1alTR35+furcubP27t2rqlWrOm8D/uexxx5TQkKC/vCHP9ic+VenTh27bQsJCVFISEiJ1OGoourFrS1evFjXr1/XfffdZ20zDEMVKlSwOUjb0c9MZwkICLA740m6cWxgQEDAbefft2+fAgICbvlZV5oKPn9cwZHXcu3atTpx4oS6detm0ycvL09paWlq27ZtidbIyE0xBAUFadiwYRo9evQd7z+uXLmynn76aa1fv77QXTnSjWHc3Nxc5ebmKiEhwea54OBgtW7dWvPmzbM7re7UqVP64IMP1K1bt9t+u1iyZIl+/vnnO77pqLu7u3r16qV169YVuR1FiY6O1uXLlxUfH6/KlSsXeqbT6tWrdfDgQetBy126dJGnp2eRB3i68rRcyTnvjaIU/MzfeecdXb58+Y6WMWfOHLm5ud3y1NWWLVuqYcOGhZ5V5SxvvfWWPvvsM23evNna1r17d33//fdFHg/kSoXVi6Jdv35dS5cu1YwZM2xGFb755hvVrFlTH3zwgctqq1+/vnbs2GHXvn37drvdojc7ffq0/v73v6tjx45yc+PPpiOv5eLFi/Xcc8/Zjc717NlTixcvLvEaGbmRlJ2drd27d9u0FTU0P2TIEL399ttasWKFunTpYm0/d+6cTp06ZdO3UqVK8vb2tltGSkqK5s2bV+Q3AHd3d+3bt8/6/5u98847at68uRISEvT6668rKipK3333ncaMGaP77rvP7o/TlStXdOrUKV2/fl0nTpzQypUrNWvWLP3+97/X448/btP3/Pnzdtvh7+9f6IGsr732msaMGVPkdpw7d07PPvus+vbtq8aNG8vf3187duzQn//8Z3Xo0EG+vr5auHCh9bT0oUOHKiAgQF988YXGjBmjLl26WI//CA8P16xZszR06FBduHBBvXv3VmRkpI4fP66lS5fKz8+vRE4Hd8Z7wxnmzZunFi1aKDY2VpMmTVLjxo3l5uam7du3a//+/Tan0F68eFGnTp3StWvXdPToUb3//vt69913NXXq1Nt+0xs1apSeffZZjR071uabt7M0atRIPXv21Ny5c61tzz33nFauXKnnnntO48aNU0JCgkJDQ/XDDz8oNTXVpbsqCqu3rCjOe7O0/OMf/9DPP/+sfv36KTAw0Oa5Ll26aPHixfrd734nqXifmc4wePBgvfPOOxoyZIhefPFFVaxYUenp6Vq8eLHee+89az/DMHTq1CkZhqHz589ry5YtevPNNxUYGGh3/aOSVhZ/xtLtX8szZ87os88+0+rVq+3OvOzTp4+eeuopnTlzRiEhISVXpHGP69OnjyHJburTp49Rs2ZNY9asWXbzDBgwwHjggQeMvLw84+jRo4XOL8lYtmyZYRiGkZycbAQGBhZZw6xZswxfX1+jQ4cORfbp0KGD0adPH+vjjIwMIzEx0QgLCzMqVKhghIeHG8OGDTPOnj1rM1/Lli2t9Xh6ehrVqlUzfve73xkrV660W0dR2zF16lTDMAzjq6++MiQZP//8c6E1rlq1yvj1W+rq1avGq6++ajRt2tQIDAw0fHx8jHr16hl//OMfjStXrlj7bdiwwWjTpo0RGBhoeHp6GtHR0cb06dON69ev260jPT3dSEhIMIKCggxvb2+jfv36xujRo42TJ08W+drdqbt9bzjbyZMnjaFDhxpRUVFGhQoVDD8/P6NZs2bGtGnTjMuXLxuGYRg1a9a0+XlHREQYXbt2Nb788kubZRW8b3ft2mXTnp+fb9SrV8/4/e9/75Sa+/TpY/e+zsjIMLy8vGzeK3l5ecaCBQuMhx9+2PD19TU8PT2N+++/3xgwYICxd+9ep9TirHpv93tQGm713pw4caLx4IMPuqSu3/3ud0a7du0KfW7nzp2GJOu/t/rMLCk7duwwEhISjKpVqxoBAQFGbGyszTqTk5OttVgsFiMwMNBo1qyZMWXKFCM7O9tmWYW9V5zpTj5/CtzueWe41Ws5ffp0o1KlSkZubq7dfNeuXTMqV65szJgxo0RrtRhGOT4/DwAA4CbsPAQAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAFQJkyaNElNmjRxdRkATIAbZwJwKcMwlJeX5+oyAJgIIzcAiqVVq1YaOnSohg4dqkqVKik4OFh//OMfVXCbuvfff1+xsbHy9/dXWFiYevToodOnT1vnX7dunSwWi9auXavY2Fh5eXnpvffe0+TJk/XNN9/IYrHIYrEoJSVFffv2td5FusD169cVFhampKQkh2p96aWXNHbsWFWuXFlhYWGaNGmSTZ+ZM2eqUaNG8vX1VXh4uAYPHqxLly5Zn09JSVGlSpX0j3/8Q/Xq1ZOPj4+6dOmiy5cva8mSJYqMjFRQUJCGDRtmE9Jyc3Otd1f39fXVww8/rHXr1t3BKw6g2Jx+K04AptayZUvDz8/PGD58uLF//37j/fffN3x8fIxFixYZhmEYixcvNtasWWMcPnzY2LJli/HII48Ybdu2tc5fcFftxo0bG2lpacahQ4eM48ePG6NGjTIeeOABIysry8jKyjKuXLlibNq0yXB3d7e56/unn35q+Pr6GhcvXnSo1oCAAGPSpEnGgQMHjCVLlhgWi8VIS0uz9pk1a5bx5ZdfGkeOHDG++OILu7uiJycnGxUqVDBat25t/Oc//zHWr19vBAcHG/Hx8UbXrl2N7777zvjss88MT09P48MPP7TO16NHD6N58+bGhg0bjEOHDhnTpk0zvLy8jAMHDtzV6w/g9gg3AIqlZcuWRoMGDYz8/Hxr2yuvvGI0aNCg0P7btm0zJFnDSEG4+eSTT2z6TZw40XjwwQft5o+Ojjbefvtt6+OOHTsaiYmJDtf629/+1qbtN7/5jfHKK68UOc/y5cuN4OBg6+Pk5GRDknHo0CFr28CBAw0fHx+bgJWQkGAMHDjQMAzDOHTokGGxWIwTJ07YLPuJJ54wxo0b51DtAO4cu6UAFNsjjzwii8VifRwXF6eDBw8qLy9Pu3btUocOHVSzZk35+/urVatWkqTMzEybZcTGxjq0rv79+ys5OVmSdPr0af2///f/1LdvX4drbdy4sc3jatWq2ewm++qrr9S6dWvdd9998vf3V+/evXXu3DldvnzZ2sfHx0e1atWyPg4NDVVkZKT8/Pxs2gqW+5///EeGYahu3bry8/OzTuvXr9fhw4cdrh3AneGAYgBOc/XqVcXHxys+Pl7vv/++QkJClJmZqYSEBOXm5tr09fX1dWiZvXv31quvvqotW7Zoy5YtioyM1KOPPupwTRUqVLB5bLFYlJ+fL0n64Ycf1K5dOw0aNEivvfaaKleurK+//lr9+vXTtWvXbrmMWy03Pz9f7u7u2rlzp9zd3W36/ToQASgZhBsAxfbvf//b7nGdOnW0f/9+nT17Vm+99ZbCw8MlSTt27HBomZ6enoWeNRUcHKyOHTsqOTlZW7Zs0QsvvHD3G/A/O3bs0PXr1zVjxgy5ud0YyF6+fPldL/ehhx5SXl6eTp8+XawgBsA52C0FoNiOHTumkSNH6vvvv9eyZcs0d+5cDR8+XBEREfL09NTcuXN15MgRrV69Wq+99ppDy4yMjNTRo0e1e/dunT17Vjk5Odbn+vfvryVLlmjfvn3q06eP07ajVq1aun79urXe9957TwsWLLjr5datW1c9e/ZU7969tXLlSh09elTbt2/X22+/rTVr1jihcgC3QrgBUGy9e/fWL7/8ombNmmnIkCEaNmyYXnzxRYWEhCglJUUfffSRoqOj9dZbb2n69OkOLbNz585q06aNHn/8cYWEhGjZsmXW55588klVq1ZNCQkJql69utO2o0mTJpo5c6befvttNWzYUB988IGmTp3qlGUnJyerd+/eGjVqlOrVq6enn35aW7dutY5oASg5FsP438UpAMABrVq1UpMmTTR79uxSW+eVK1dUvXp1JSUlqVOnTqW2XgDlE8fcACiz8vPzderUKc2YMUOBgYF6+umnXV0SgHKAcAOgzMrMzFRUVJRq1KihlJQUeXh42DwXHR1d5Lx79+5VREREaZQJoIxhtxSAcun69evKyMgo8vnIyEibMATg3kG4AQAApsLZUgAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFQINwAAwFT+P8mk1uXi8+1iAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=\"party_name\", y=\"freq\", hue=\"type\", data=party_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_tokens_list, \"../data/2017-2022/train_data.pt\")\n",
    "# torch.save(test_tokens_list, \"../data/2017-2022/test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = torch.load(\"../data/2017-2022/train_data.pt\")\n",
    "test_dict = torch.load(\"../data/2017-2022/test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'masks', 'interventions_masks', 'labels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterventionsDataset(Dataset):\n",
    "    def __init__(self, dict_data, max_interventions=100):\n",
    "        self.labels = dict_data[\"labels\"]\n",
    "        self.masks = dict_data[\"masks\"][:, :max_interventions, :]\n",
    "        self.tokens = dict_data[\"tokens\"][:, :max_interventions, :]\n",
    "        self.interventions_masks = dict_data[\"interventions_masks\"][:, :max_interventions]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return {\n",
    "            \"tokens\": self.tokens[idx],\n",
    "            \"masks\": self.masks[idx],\n",
    "            \"interventions_masks\": self.interventions_masks[idx],\n",
    "        }\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InterventionsDataset(train_dict)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = InterventionsDataset(test_dict)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x[\"tokens\"].to(device=device)\n",
    "attention_mask = x[\"masks\"].to(device=device)\n",
    "interventions_masks = x[\"interventions_masks\"].to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnInterventions(nn.Module):\n",
    "    def __init__(self, bert_model, bert_dim=768):\n",
    "        super(BertOnInterventions, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.bert_dim = bert_dim\n",
    "        \n",
    "    def forward(self, tokens, masks):\n",
    "        \"\"\"\n",
    "        tokens (batch_size, nb_int, nb_tokens)\n",
    "        masks (batch_size, nb_int, nb_tokens)\n",
    "        \"\"\"\n",
    "        nb_int = tokens.shape[1]\n",
    "        res = []\n",
    "        for i in range(nb_int):\n",
    "            input_ids = tokens[:, i, :]\n",
    "            attention_mask = masks[:, i, :]\n",
    "            tmp = self.bert_model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask\n",
    "            )[\"pooler_output\"]\n",
    "            tmp = tmp.unsqueeze(dim=1)\n",
    "            res.append(tmp)\n",
    "        pooled_output = torch.cat(res, dim=1)  # (batch_size, nb_int, bert_dim)\n",
    "        return pooled_output\n",
    "\n",
    "class InterventionsAttention(nn.Module):\n",
    "    def __init__(self, bert_dim, num_heads, embed_dim):\n",
    "        super(InterventionsAttention, self).__init__()\n",
    "        # Set the parameters\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Initialize the linear layers for the multi-head attention\n",
    "        self.query_linear = nn.Linear(bert_dim, embed_dim)\n",
    "        self.key_linear = nn.Linear(bert_dim, embed_dim)\n",
    "        self.value_linear = nn.Linear(bert_dim, embed_dim)\n",
    "\n",
    "        # Initialize the multi-head attention layer\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, pooled_output, interventions_masks):\n",
    "        # Add a zero padding for CLS in attention\n",
    "        pooled_output_with_cls = F.pad(\n",
    "            pooled_output, pad=(0, 0, 1, 0), mode=\"constant\", value=0.0\n",
    "        )  # (batch_size, nb_int + 1, bert_dim)\n",
    "        \n",
    "        # Pad the mask as well\n",
    "        interventions_masks_with_cls = F.pad(interventions_masks, pad=(1, 0), mode=\"constant\", value=1)\n",
    "\n",
    "        # Split the input tensors into the different attention heads\n",
    "        query_heads = self.query_linear(\n",
    "            pooled_output_with_cls\n",
    "        )  # (batch_size, nb_int + 1, embed_dim)\n",
    "        key_heads = self.key_linear(pooled_output_with_cls)  # (batch_size, nb_int + 1, embed_dim)\n",
    "        value_heads = self.value_linear(pooled_output_with_cls)  # (batch_size, embed_dim)\n",
    "\n",
    "        # Apply the multi-head attention\n",
    "        attn_output, _ = self.attention_layer(\n",
    "            query_heads,\n",
    "            key_heads,\n",
    "            value_heads,\n",
    "            key_padding_mask=interventions_masks_with_cls\n",
    "        )  # (batch_size, embed_dim)\n",
    "\n",
    "        deputy_repr = attn_output[:, 0, :]  # (batch_size, embed_dim)\n",
    "\n",
    "        return deputy_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "camembert_model = CamembertModel.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnInterventions(\n",
       "  (bert_model): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): CamembertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = BertOnInterventions(camembert_model, bert_dim=768)\n",
    "encoder.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t = encoder(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0638,  0.1658, -0.0528,  ...,  0.0301, -0.0713, -0.0503],\n",
       "         [ 0.0364,  0.1532, -0.0667,  ...,  0.0101, -0.0971, -0.0164],\n",
       "         [ 0.0246,  0.1430, -0.0794,  ..., -0.0073, -0.0749, -0.0401],\n",
       "         ...,\n",
       "         [-0.0191,  0.1226, -0.0541,  ...,  0.0478, -0.1060,  0.0096],\n",
       "         [-0.0130,  0.1417, -0.0159,  ...,  0.0538, -0.1065,  0.0467],\n",
       "         [ 0.0381,  0.1499, -0.0719,  ...,  0.0093, -0.1196, -0.0387]],\n",
       "\n",
       "        [[ 0.0539,  0.1599, -0.0666,  ...,  0.0035, -0.1023, -0.0491],\n",
       "         [ 0.0318,  0.1507, -0.0585,  ...,  0.0422, -0.1150, -0.0169],\n",
       "         [ 0.0139,  0.1357, -0.0343,  ...,  0.0277, -0.0859, -0.0104],\n",
       "         ...,\n",
       "         [-0.1143,  0.0934, -0.0202,  ...,  0.0302, -0.0867,  0.0606],\n",
       "         [-0.0252,  0.1245, -0.0255,  ...,  0.0228, -0.1275,  0.0418],\n",
       "         [-0.1093,  0.0342,  0.0122,  ...,  0.0848, -0.1800,  0.0435]],\n",
       "\n",
       "        [[ 0.0439,  0.1139, -0.0494,  ...,  0.0311, -0.0799, -0.0731],\n",
       "         [ 0.0310,  0.1306, -0.0680,  ...,  0.0350, -0.0953, -0.0377],\n",
       "         [ 0.0169,  0.1221, -0.0805,  ...,  0.0481, -0.0808, -0.0009],\n",
       "         ...,\n",
       "         [ 0.0291,  0.1493, -0.0071,  ..., -0.0091, -0.1014, -0.0383],\n",
       "         [ 0.0378,  0.0907, -0.0905,  ...,  0.0006, -0.0901, -0.0420],\n",
       "         [-0.1216,  0.0699,  0.0098,  ...,  0.0668, -0.1203,  0.0591]],\n",
       "\n",
       "        [[-0.0686,  0.1076, -0.0098,  ...,  0.0660, -0.1401,  0.0742],\n",
       "         [-0.1006,  0.0863,  0.0158,  ...,  0.0829, -0.1592,  0.0368],\n",
       "         [ 0.0151,  0.0838, -0.0240,  ...,  0.0334, -0.1418, -0.0422],\n",
       "         ...,\n",
       "         [-0.0329,  0.1256,  0.0012,  ...,  0.0672, -0.1404,  0.0543],\n",
       "         [-0.1016,  0.0678, -0.0016,  ...,  0.0511, -0.1313,  0.0501],\n",
       "         [ 0.0019,  0.1473, -0.0397,  ...,  0.0307, -0.0478, -0.0161]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCamembertClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model,\n",
    "        num_classes,\n",
    "        bert_dim=768,\n",
    "        input_dim=256,\n",
    "        embed_dim=256,\n",
    "        input_dim2=256,\n",
    "        num_heads=8,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(SimpleCamembertClassifier, self).__init__()\n",
    "\n",
    "        # Set the parameters\n",
    "        self.bert_dim = bert_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_dim = input_dim // num_heads\n",
    "\n",
    "        # Initialize the BERT model\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "        # Initialize the linear layers for the multi-head attention\n",
    "        self.query_linear = nn.Linear(bert_dim, input_dim)\n",
    "        self.key_linear = nn.Linear(bert_dim, input_dim)\n",
    "        self.value_linear = nn.Linear(bert_dim, input_dim)\n",
    "\n",
    "        # Initialize the multi-head attention layer\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Initialize the linear layers for the output projection\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim2)\n",
    "        self.linear2 = nn.Linear(input_dim, num_classes)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # self.mlp = nn.Sequential(\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.Linear(input_dim, input_dim2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.Linear(input_dim2, num_classes)\n",
    "        # )\n",
    "\n",
    "    def forward(self, tokens, masks, interventions_masks):\n",
    "        \"\"\"\n",
    "        tokens (batch_size, nb_int, nb_tokens)\n",
    "        masks (batch_size, nb_int, nb_tokens)\n",
    "        interventions_masks (batch_size, nb_int)\n",
    "        \"\"\"\n",
    "        nb_int = tokens.shape[1]\n",
    "\n",
    "        res = []\n",
    "        for i in range(nb_int):\n",
    "            input_ids = tokens[:, i, :]\n",
    "            attention_mask = masks[:, i, :]\n",
    "            tmp = self.bert_model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask\n",
    "            )[\"pooler_output\"]\n",
    "            tmp = tmp.unsqueeze(dim=1)\n",
    "            res.append(tmp)\n",
    "        pooled_output = torch.cat(res, dim=1)  # (batch_size, nb_int, bert_dim)\n",
    "        \n",
    "        # pooled_output = self.bert_model(\n",
    "        #     input_ids=tokens, attention_mask=masks, return_dict=False\n",
    "        # )  \n",
    "\n",
    "        # Add a zero padding for CLS in attention\n",
    "        pooled_output_with_cls = F.pad(\n",
    "            pooled_output, pad=(0, 0, 1, 0), mode=\"constant\", value=0.0\n",
    "        )  # (batch_size, nb_int + 1, bert_dim)\n",
    "\n",
    "        interventions_masks_with_cls = F.pad(interventions_masks, pad=(1, 0), mode=\"constant\", value=1)\n",
    "\n",
    "        # Split the input tensors into the different attention heads\n",
    "        query_heads = self.query_linear(\n",
    "            pooled_output_with_cls\n",
    "        )  # (batch_size, nb_int + 1, input_dim)\n",
    "        key_heads = self.key_linear(pooled_output_with_cls)  # (batch_size, nb_int + 1, input_dim)\n",
    "        value_heads = self.value_linear(pooled_output_with_cls)  # (batch_size, input_dim)\n",
    "\n",
    "        # Apply the multi-head attention\n",
    "        attn_output, _ = self.attention_layer(\n",
    "            query_heads,\n",
    "            key_heads,\n",
    "            value_heads,\n",
    "            key_padding_mask=interventions_masks_with_cls\n",
    "        )  # (batch_size, input_dim)\n",
    "\n",
    "        deputy_repr = attn_output[:, 0, :]  # (batch_size, input_dim)\n",
    "\n",
    "        dropout_output1 = self.dropout1(deputy_repr)\n",
    "        linear_output1 = self.relu(self.linear1(dropout_output1))  # (batch_size, input_dim2)\n",
    "        dropout_output2 = self.dropout2(linear_output1)\n",
    "        final_layer = self.relu(self.linear2(dropout_output2))  # (batch_size, num_classes)\n",
    "        \n",
    "        # final_layer = self.mlp(deputy_repr)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(np.unique(train_dict[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "camembert_model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "for param in camembert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleCamembertClassifier(\n",
    "    bert_model=camembert_model,\n",
    "    num_classes=nb_classes,\n",
    "    bert_dim=768,\n",
    "    input_dim=64,\n",
    "    embed_dim=64,\n",
    "    input_dim2=64,\n",
    "    num_heads=1,\n",
    "    dropout=0.2,\n",
    ")\n",
    "classifier.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = x[\"tokens\"].to(device)\n",
    "masks = x[\"masks\"].to(device)\n",
    "interventions_masks = x[\"interventions_masks\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_dataset(\n",
    "    tokenizer,\n",
    "    processed_df,\n",
    "    labels_dict,\n",
    "    group_var=\"groupe\",\n",
    "    intervention_var=\"intervention\",\n",
    "    titre_var=\"titre_complet\",\n",
    "    profession_var=\"profession\",\n",
    "    max_len_padding=512,\n",
    "    max_len_padding_titre=64,\n",
    "    max_len_padding_profession=16,\n",
    "    test_frac=0.25,\n",
    "    val_frac=0.2\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "bert_model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "for p in bert_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "model = SeanceClassifier(\n",
    "    bert_model=bert_model,\n",
    "    num_classes=3,\n",
    "    intervention_dim=256,\n",
    "    titre_dim=128,\n",
    "    profession_dim=64,\n",
    "    bert_dim=768,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    learning_rate,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    "):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "    total_acc_trains = []\n",
    "    total_loss_trains = []\n",
    "    total_acc_vals = []\n",
    "    total_loss_vals = []\n",
    "    \n",
    "    pbar = tqdm(range(epochs), leave=False)\n",
    "    for epoch_num in pbar:\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        inner_pbar = tqdm(train_dataloader, leave=False, total=len(train_dataloader), desc=\"Training\")\n",
    "        for train_input, train_label in train_dataloader:\n",
    "            train_label = train_label.to(device)\n",
    "            \n",
    "            x = {_: {\n",
    "                \"input_ids\": train_input[_][\"input_ids\"].to(device),\n",
    "                \"attention_mask\": train_input[_][\"attention_mask\"].to(device),\n",
    "            } for _ in [\"intervention\", \"titre\", \"profession\"]}\n",
    "\n",
    "            output = model(**x)\n",
    "\n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            inner_pbar.update()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inner_pbar.reset(total=len(val_dataloader))\n",
    "            inner_pbar.set_description(\"Validation\")\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                x = {_: {\n",
    "                    \"input_ids\": val_input[_][\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": val_input[_][\"attention_mask\"].to(device),\n",
    "                } for _ in [\"intervention\", \"titre\", \"profession\"]}\n",
    "\n",
    "                output = model(**x)\n",
    "\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "                inner_pbar.update()\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_dataloader): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_dataloader): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_dataloader): .3f}\"\n",
    "        )\n",
    "        \n",
    "        total_acc_trains.append(total_acc_train)\n",
    "        total_loss_trains.append(total_loss_train)\n",
    "        total_acc_vals.append(total_acc_val)\n",
    "        total_loss_vals.append(total_loss_val)\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": total_loss_trains,\n",
    "        \"test_loss\": total_loss_vals,\n",
    "        \"train_acc\": total_acc_trains,\n",
    "        \"test_acc\": total_acc_vals,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d86b9554164b10be16e0822cd89541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb27ad65da5415d916a8dd53a5bbb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 7.80 GiB total capacity; 6.73 GiB already allocated; 124.31 MiB free; 6.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     \u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m10\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     train_dataloader,\n\u001b[1;32m      6\u001b[0m     val_dataloader\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[68], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, learning_rate, epochs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     34\u001b[0m train_label \u001b[39m=\u001b[39m train_label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m {_: {\n\u001b[1;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: train_input[_][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: train_input[_][\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     39\u001b[0m } \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mintervention\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtitre\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprofession\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m---> 41\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     43\u001b[0m batch_loss \u001b[39m=\u001b[39m criterion(output, train_label\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     44\u001b[0m total_loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[58], line 46\u001b[0m, in \u001b[0;36mSeanceClassifier.forward\u001b[0;34m(self, intervention, titre, profession)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, intervention, titre, profession):\n\u001b[1;32m     45\u001b[0m     \u001b[39m# Embed interventions\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     _, intervention_pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m     47\u001b[0m         input_ids\u001b[39m=\u001b[39;49mintervention[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     48\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mintervention[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     49\u001b[0m         return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[39m# Embed titre\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     _, titre_pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[1;32m     54\u001b[0m         input_ids\u001b[39m=\u001b[39mtitre[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),\n\u001b[1;32m     55\u001b[0m         attention_mask\u001b[39m=\u001b[39mtitre[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m),\n\u001b[1;32m     56\u001b[0m         return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:898\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    889\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    891\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    892\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    893\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    897\u001b[0m )\n\u001b[0;32m--> 898\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    899\u001b[0m     embedding_output,\n\u001b[1;32m    900\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    901\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    902\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    903\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    904\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    905\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    906\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    907\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    908\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    911\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:534\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    525\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    526\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    527\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    533\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    535\u001b[0m         hidden_states,\n\u001b[1;32m    536\u001b[0m         attention_mask,\n\u001b[1;32m    537\u001b[0m         layer_head_mask,\n\u001b[1;32m    538\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    539\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    540\u001b[0m         past_key_value,\n\u001b[1;32m    541\u001b[0m         output_attentions,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:419\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    409\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    417\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    420\u001b[0m         hidden_states,\n\u001b[1;32m    421\u001b[0m         attention_mask,\n\u001b[1;32m    422\u001b[0m         head_mask,\n\u001b[1;32m    423\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    424\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    428\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:346\u001b[0m, in \u001b[0;36mCamembertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 346\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    347\u001b[0m         hidden_states,\n\u001b[1;32m    348\u001b[0m         attention_mask,\n\u001b[1;32m    349\u001b[0m         head_mask,\n\u001b[1;32m    350\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    351\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    352\u001b[0m         past_key_value,\n\u001b[1;32m    353\u001b[0m         output_attentions,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    356\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-11-7/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:248\u001b[0m, in \u001b[0;36mCamembertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    245\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    247\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    251\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 7.80 GiB total capacity; 6.73 GiB already allocated; 124.31 MiB free; 6.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    1e-4,\n",
    "    10,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SimpleCamembertClassifier(\n",
    "    bert_model=camembert_model,\n",
    "    num_classes=nb_classes,\n",
    "    bert_dim=768,\n",
    "    input_dim=64,\n",
    "    embed_dim=64,\n",
    "    input_dim2=64,\n",
    "    num_heads=1,\n",
    "    dropout=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889f470fb1d34df9b1384da6b722dc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e40ad7ea3f49a9be4515331c057e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  55.731             | Train Accuracy:  1.750             | Val Loss:  18.784             | Val Accuracy:  0.250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f2210e4c3d470385025d26e9f27422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  55.732             | Train Accuracy:  1.000             | Val Loss:  18.776             | Val Accuracy:  0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a3588625694a45b9c6b0af27ac9728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  55.688             | Train Accuracy:  0.750             | Val Loss:  18.765             | Val Accuracy:  0.750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c073cb11ba47b388651cde92c33c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  55.649             | Train Accuracy:  1.000             | Val Loss:  18.758             | Val Accuracy:  0.750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14481a9abd6d4e3eb20104cb4246f24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  55.613             | Train Accuracy:  1.500             | Val Loss:  18.757             | Val Accuracy:  0.250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34c847329ad484ab62121c4f8c06e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  55.605             | Train Accuracy:  1.500             | Val Loss:  18.732             | Val Accuracy:  0.750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9a28cfb4da4ad989d6d7f92daf0d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  55.569             | Train Accuracy:  2.250             | Val Loss:  18.735             | Val Accuracy:  0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4115edfdb74a46a6b1feda7f4a3f011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  55.535             | Train Accuracy:  1.500             | Val Loss:  18.722             | Val Accuracy:  0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051f44c9fa6248449e6e89197fc803c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  55.515             | Train Accuracy:  1.750             | Val Loss:  18.718             | Val Accuracy:  0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d7e9cb3f7346c6af12ba22ee1f7117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  55.484             | Train Accuracy:  1.250             | Val Loss:  18.704             | Val Accuracy:  0.500\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LR = 1e-6\n",
    "\n",
    "results = train(classifier, train_dict, test_dict, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_path = Path(\"../models/2017-2022\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(classifier.state_dict(), model_path / \"camembert_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path / \"camembert_classifier_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)\n",
    "res_df[\"train_loss\"] = res_df[\"train_loss\"] / len(train_dict[\"labels\"])\n",
    "res_df[\"train_acc\"] = res_df[\"train_acc\"] / len(train_dict[\"labels\"])\n",
    "\n",
    "res_df[\"test_loss\"] = res_df[\"test_loss\"] / len(test_dict[\"labels\"])\n",
    "res_df[\"test_acc\"] = res_df[\"test_acc\"] / len(test_dict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNAAAAGsCAYAAAAc6lr4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD9DElEQVR4nOzdeVxU9frA8c/MsO+yIyqLyOaaYKZmmqKmtmfanlqW1zY12663MuvqvWb+rG7appZaZpvdrpmKmru5pWWCgILiAiIoIPsy8/vjMIMkoODAmYHn/Xqd12zfOeeZQeHMM9/v82gMBoMBIYQQQgghhBBCCCFErbRqByCEEEIIIYQQQgghhCWTBJoQQgghhBBCCCGEEPWQBJoQQgghhBBCCCGEEPWQBJoQQgghhBBCCCGEEPWQBJoQQgghhBBCCCGEEPWQBJoQQgghhBBCCCGEEPWQBJoQQgghhBBCCCGEEPWwUTuA5qTX6zlz5gyurq5oNBq1wxFCCCGEFTAYDFy8eJG2bdui1cp3j5ZKzvOEEEII0VANOc9rVQm0M2fO0L59e7XDEEIIIYQVOnnyJO3atVM7DFEHOc8TQgghRGNdzXleq0qgubq6Asob4+bmpnI0QgghhLAG+fn5tG/f3nQeISyTnOcJIYQQoqEacp7XqhJoxun8bm5ucmIlhBBCiAaRZYGWTc7zhBBCCNFYV3OeJ4U8hBBCCCGEEEIIIYSoR6MSaAsWLCAkJAQHBwdiYmLYtm1bveNLS0uZPn06QUFB2Nvb07FjRxYvXmx6/Pvvvyc2NhYPDw+cnZ3p0aMHy5Ytq7GP4OBgNBrNZdtTTz3VmJcghBBCCCGEEEIIIcRVafASzpUrVzJ58mQWLFhAv379+Oijjxg+fDgJCQl06NCh1ueMHj2as2fPsmjRIsLCwsjKyqKiosL0uKenJ9OnTycyMhI7OztWr17NuHHj8PX1ZdiwYQDs3buXyspK03P+/PNPhgwZwr333tvQlyCEEEIIIYQQQgghxFXTGAwGQ0Oe0Lt3b3r27MnChQtN90VFRXHnnXcye/bsy8avXbuW++67j9TUVDw9Pa/6OD179mTkyJG8+eabtT4+efJkVq9eTUpKSp1rVUtLSyktLTXdNhaHy8vLk9oYQgghzKayspLy8nK1wxCNZGtri06nq/Px/Px83N3d5fzBwsnPSQghRFOTcz7rY87zvAbNQCsrK2P//v28/PLLNe4fOnQoO3furPU5P/74I7GxscyZM4dly5bh7OzM7bffzptvvomjo+Nl4w0GA5s2bSIpKYl///vfdcaxfPlypk6dWm+ht9mzZ/PGG2804BUKIYQQV89gMJCZmUlubq7aoYhr5OHhgb+/vzQKEEIIIcRl5JzPupnrPK9BCbTs7GwqKyvx8/Orcb+fnx+ZmZm1Pic1NZXt27fj4ODAqlWryM7OZtKkSZw/f75GHbS8vDwCAwMpLS1Fp9OxYMEChgwZUus+f/jhB3Jzcxk7dmy98b7yyitMnTrVdNs4A00IIYQwB+OJlK+vL05OTpJ8sUIGg4GioiKysrIACAgIUDkiIYQQQlgaOeezTuY+z2twDTS4vL2nwWCo8x+QXq9Ho9HwxRdf4O7uDsC8efMYNWoUH3zwgWkWmqurKwcPHqSgoICNGzcydepUQkNDGThw4GX7XLRoEcOHD6dt27b1xmlvb4+9vX0jXqEQQghRv8rKStOJlJeXl9rhiGtgPBfJysrC19e33mn+QgghhGhd5JzPupnzPK9BCTRvb290Ot1ls82ysrIum5VmFBAQQGBgoCl5BkrNNIPBwKlTp+jUqRMAWq2WsLAwAHr06EFiYiKzZ8++LIF24sQJNmzYwPfff9+Q0IUQQgizMta/cHJyUjkSYQ7Gn2N5ebkk0IQQQghhIud81s9c53nahgy2s7MjJiaG+Pj4GvfHx8fTt2/fWp/Tr18/zpw5Q0FBgem+5ORktFot7dq1q/NYBoOhRgMAoyVLluDr68vIkSMbEroQQgjRJGQKf8sgP0chhBBC1EfOFayXuX52DUqgAUydOpVPP/2UxYsXk5iYyJQpU0hPT2fixImAUnfskUceMY1/4IEH8PLyYty4cSQkJLB161ZeeOEFxo8fb5pKN3v2bOLj40lNTeXIkSPMmzePpUuX8tBDD9U4tl6vZ8mSJTz66KPY2DRq9akQQgghhBBCCCGEEA3S4CzUmDFjyMnJYebMmWRkZNClSxfWrFlDUFAQABkZGaSnp5vGu7i4EB8fzzPPPENsbCxeXl6MHj2at956yzSmsLCQSZMmcerUKRwdHYmMjGT58uWMGTOmxrE3bNhAeno648ePb+zrFUIIIYQQQgghhBCiQTQGg8GgdhDNJT8/H3d3d/Ly8nBzc1M7HCGEEFaspKSEtLQ0QkJCcHBwUDsc1QQHBzN58mQmT558zfvavHkzN998MxcuXMDDw+Oa99cQ9f085fzBOsjPSQghRFOQcz6FOc/5mpu5zvNkHaQQQgjRygwcOJAePXowf/78a97X3r17cXZ2vvaghBBCCCGEWck5n3lJAs2cyoqgrBBcfNSORAghhGg0g8FAZWXlVdUb9fGRv3lCCCFES5aVX4JOq8HLxV7tUISZyTlfwzS4iYCox7GNMDcMPugNq6fCn9/BxbNqRyWEEKIZGAwGisoqVNkaUo1h7NixbNmyhXfffReNRoNGo+Gzzz5Do9Gwbt06YmNjsbe3Z9u2bRw7dow77rgDPz8/XFxc6NWrFxs2bKixv+Dg4Brfamo0Gj799FPuuusunJyc6NSpEz/++GOj39fvvvuOzp07Y29vT3BwMO+8806NxxcsWECnTp1wcHDAz8+PUaNGmR779ttv6dq1K46Ojnh5eREXF0dhYWGjYxFCCCFam9yiMobN38rt/9lBWYVe7XAsgpzzKa7lnK+yspLHHnuMkJAQHB0diYiI4N13371s3OLFi03ngQEBATz99NOmx3Jzc3niiSfw8/PDwcGBLl26sHr16qt+fxpDZqCZ07mkqssjyrZvkXLbqxME94PQgdD5LtXCE0II0XSKyyuJfm2dKsdOmDkMJ7ur+5P+7rvvkpycTJcuXZg5cyYAhw8fBuDFF19k7ty5hIaG4uHhwalTpxgxYgRvvfUWDg4OfP7559x2220kJSXRoUOHOo/xxhtvMGfOHN5++23ef/99HnzwQU6cOIGnp2eDXtf+/fsZPXo0M2bMYMyYMezcuZNJkybh5eXF2LFj2bdvH88++yzLli2jb9++nD9/nm3btgFKU6P777+fOXPmcNddd3Hx4kW2bdvWoBNPIYSVS1oLW9+u+3EbBxj3U/XtZXdDSV7d4/s9C9F3KNd//wr2fFL3WFd/uO8L5brBAJ/G1R/r4FeVzwoAuz+CP76u+bhGAz0ehNhx9e9HqCvvNKx9CW6cAoExakdjFr8kZXGhqJwLReXsPX6efmHeaoekOjnnq9bYcz69Xk+7du34+uuv8fb2ZufOnTzxxBMEBAQwevRoABYuXMjUqVP517/+xfDhw8nLy2PHjh2m5w8fPpyLFy+yfPlyOnbsSEJCAjqd7qrem8aSBJo53TQNYsZB+k44vgNObIfMPyEnRdnOJlQn0MqKIPFHCOoHHu3VjVsIIUSr4e7ujp2dHU5OTvj7+wNw5MgRAGbOnMmQIUNMY728vOjevbvp9ltvvcWqVav48ccfa3wD+Fdjx47l/vvvB2DWrFm8//777Nmzh1tuuaVBsc6bN4/Bgwfz6quvAhAeHk5CQgJvv/02Y8eOJT09HWdnZ2699VZcXV0JCgriuuuuA5QEWkVFBXfffbepU3jXrl0bdHwhhBUqLwHbqgLRRdlwel/dY22dat7OOAhFOXWPL8iqvn4xs/59e/zlA2d9YwGKL1RfzztZ+/jzqZJAs3TnjsCZ3+Hnl+HxeLWjMYsNCdX/7uMTzkoCzYpY8jmfra0tb7zxhul2SEgIO3fu5OuvvzYl0N566y2ef/55nnvuOdO4Xr16AbBhwwb27NlDYmIi4eHhAISGhl75TblGkkAzN2cviLpN2UD5Y5j+KxzfDp4h1eNO7YVVTyrXPTpA0I0QfKMyU80jSPmWSQghhNVwtNWRMHOYasc2h9jY2Bq3CwsLeeONN1i9ejVnzpyhoqKC4uJi0tPT691Pt27dTNednZ1xdXUlKyurnmfULjExkTvuuKPGff369WP+/PlUVlYyZMgQgoKCCA0N5ZZbbuGWW24xLSPo3r07gwcPpmvXrgwbNoyhQ4cyatQo2rRp0+A4hBBWorQA3omEdrEweimEDID7v6p7vOYvvzvv/hgqy+se7xtdfT3qNvCJqHuszV869dUXB0BAj+rrPR5SvmQ30lfAyoeU5F7ReXBq2Gxe0Yw8QyEvHfJPK58DHa37b05pRSVbks+Zbm9IPMvrt0WjaeWfVeWcr9q1nPN9+OGHfPrpp5w4cYLi4mLKysro0aMHAFlZWZw5c4bBgwfX+tyDBw/Srl07U/KsuUgCrak5toGI4cp2KUOlMq33zEHITYfcL+H3L5XH3NopybQ7PgCd/IiEEMIaaDSaq55Sb6n+2lnphRdeYN26dcydO5ewsDAcHR0ZNWoUZWVl9e7H1ta2xm2NRoNe3/C6KQaD4bKT9EuXYLq6uvLbb7+xefNm1q9fz2uvvcaMGTPYu3cvHh4exMfHs3PnTtavX8/777/P9OnT2b17NyEhIX89lBCiJTi2CcouwoXjYO8KDm4NW+kRdoVllpfy6qhsV0OjufyzQH18I5XtUm7tIP8UZKdAh95Xvy/RvDxDlERrVgKkxEO30WpHdE12p56noLQCbxd7LpaUc+pCMUlnLxLp76Z2aKqSc75qjT3n+/rrr5kyZQrvvPMOffr0wdXVlbfffpvdu3cD4OjoWO/zr/R4U5EmAmrpOAgmbIKXT8BD3ynr5NtdD1ob5Y/jmQPVyTN9JfzwFOxbovzRlPotQgghroGdnR2VlZVXHLdt2zbGjh3LXXfdRdeuXfH39+f48eNNH2CV6Ohotm/fXuO+nTt3Eh4ebqpxYWNjQ1xcHHPmzOGPP/7g+PHjbNq0CVBO4vr168cbb7zBgQMHsLOzY9WqVc0WvxCimSWtUS4jR7a81Rz3LoGn97eYulotkl4P/326ui628d+jFYtPUBriDYn2o38nZelm/GFpkmdNLPWcb9u2bfTt25dJkyZx3XXXERYWxrFjx0yPu7q6EhwczMaNG2t9frdu3Th16hTJyclNFmNtrDtt2hLYuyrfdhm/8SorhJO7lfoNRpmH4OByZQNw8YOgvsrU7uD+yvTxlnaSIIQQoskEBweze/dujh8/jouLS53fFIaFhfH9999z2223odFoePXVVxs1k6yxnn/+eXr16sWbb77JmDFj2LVrF//5z39YsGABAKtXryY1NZWbbrqJNm3asGbNGvR6PREREezevZuNGzcydOhQfH192b17N+fOnSMqKqrZ4hdCNKPKCkiuKurdkNle1qL99WpHIK4k7yQcWFZ9O2UDVJSBjZ16MV0Dg8HAhkRjAs2XrPxSNiRmsSHxLM8M7qRydOJqWeo5X1hYGEuXLmXdunWEhISwbNky9u7dW2OVwIwZM5g4cSK+vr6mhgE7duzgmWeeYcCAAdx0003cc889zJs3j7CwMI4cOYJGo2lwzd2GkBlolsbOWZmdFjmi+j4nTxjwklInTWcPBWfh8CpYMw0W9Ia3w2oWNRVCCCHqMW3aNHQ6HdHR0fj4+NRZ3+L//u//aNOmDX379uW2225j2LBh9OzZs9ni7NmzJ19//TVfffUVXbp04bXXXmPmzJmMHTsWAA8PD77//nsGDRpEVFQUH374IStWrKBz5864ubmxdetWRowYQXh4OP/4xz945513GD68BX6wboAFCxYQEhKCg4MDMTExpq6lddmyZQsxMTE4ODgQGhrKhx9+WOPxTz75hP79+9OmTRvatGlDXFwce/bsqTFmxowZaDSaGpuxmLGRwWBgxowZtG3bFkdHRwYOHGjqFCbEVTm5G4rPK+VT2t+gdjSiNcpOUS69I5QJD2UX4Xj9v2Mt2eEz+WTkleBoq6NvR28GRfkC8PupPM7ml1zh2cJSWOo538SJE7n77rsZM2YMvXv3Jicnh0mTJtUY8+ijjzJ//nwWLFhA586dufXWW0lJSTE9/t1339GrVy/uv/9+oqOjefHFF69qtt210BhaUT/3/Px83N3dycvLw83NStdtl5coXXmMXT5P7gVHD5iaqMxCMxjgo/5KI4KgfkpTAr8uoG3adq5CCNHalJSUkJaWZkpGCOtW38+zRZw/ACtXruThhx9mwYIF9OvXj48++ohPP/2UhISEWlvUp6Wl0aVLFyZMmMCTTz7Jjh07mDRpEitWrOCee+4B4MEHH6Rfv3707dsXBwcH5syZw/fff8/hw4cJDAwElATat99+y4YNG0z71ul0+Pj4mG7/+9//5p///CefffYZ4eHhvPXWW2zdupWkpCRcXV2v6vW1lJ+TaKR102HXf6DbfXD3R2pHY345x2D9P5SGAg9+o3Y0oja/LoS1LysNJhw94bfPodcEGDlX7cgaZf6GZOZvSGFotB8fP6IUnL/zgx0cPJnLrLu68kDvy/9utFRyzmf9zHWeJ0s4rY2tQ1W3zhuBl5Rpwbnp1Us4LxxXlnxmHoIjq5X7HNyhQ18lmRbUD/y7SXMCIYQQopWZN28ejz32GI8//jgA8+fPZ926dSxcuJDZs2dfNv7DDz+kQ4cOzJ8/H4CoqCj27dvH3LlzTQm0L774osZzPvnkE7799ls2btzII488YrrfxsbmsllnRgaDgfnz5zN9+nTuvvtuAD7//HP8/Pz48ssvefLJJ6/5tYsWzmCorjfVEpdvAujslNeotVWWq8q5vOUxzkDz6gQdblASaCd2qhvTNahevulnum9ItB8HT+ayIfFsq0qgCWEkSzitnY0deIdV33ZvB49tgLgZEDYE7FygJA+Sf1a+tfrkZsj8vXp87sn623ULIYQQZjJx4kRcXFxq3SZOnKh2eC1aWVkZ+/fvZ+jQoTXuHzp0KDt31v4Bb9euXZeNHzZsGPv27aO8vPZzh6KiIsrLy/H09Kxxf0pKCm3btiUkJIT77ruP1NRU02NpaWlkZmbWOJa9vT0DBgyoMzaA0tJS8vPza2yilcpNh/NpSpIpbLDa0TQNt0CwcQR9OeSeUDsaUZsc4xLOThByEzzyIzy5Rd2YGulMbjF/ns5Ho4FBkb6m+43JtO1HsyksrVArPGEFWuo5n3x10dLobKF9L2W7cYryDVXm71VLPnfA2cPg3716/Jej4cIJ5VuS4H5KnbW211ltsUshhBCWa+bMmUybNq3Wx2TJXdPKzs6msrISPz+/Gvf7+fmRmZlZ63MyMzNrHV9RUUF2djYBAQGXPefll18mMDCQuLg40329e/dm6dKlhIeHc/bsWd566y369u3L4cOH8fLyMh2/tmOdOFF3omD27Nm88cYb9b9w0Tq0CYJpKco5r/3VLfm1OloteIXB2UOQnQxeHdWOSPxV9lHl0qsT2DpC6AB147kGG6tmn8V0aIOXi73p/k6+LnTwdCL9fBHbUrK5pUvtM4uFaKnnfJJAa+l0Nkq768AY6PesMsXduNyzrBAuZkJ5IRzbqGwAtk5Kp5+gG6HrKPAMqXv/QgghxFXy9fXF19f3ygNFk9H8pWu3wWC47L4rja/tfoA5c+awYsUKNm/eXKO+yKWNG7p27UqfPn3o2LEjn3/+OVOnTm10bK+88kqN5+fn59O+ffs6x4sWzsWnuqt9S+XdqSqBltJyl6paq9KLcPGMcv3S1UEAhTng7NX8MV2D+ESlQV1cdM0vNjQaDXFRfizekcaGxLOSQBN1aqnnfLKEs7W59ETUzhleOAYTd8At/1YKXjp5QXkRpG6GX96CC2nV4zMPQf6ZZg9ZCCGEENfG29sbnU532WyzrKysy2Z+Gfn7+9c63sbGBi+vmh8G586dy6xZs1i/fj3dunWrNxZnZ2e6du1q6qRlrI3WkNhAWebp5uZWYxOtUHmJUhO4NfDupFzmpNQ/TjS/siLofLeydNOxjXJfZTl8OgTe7gh5p9SNrwEulpSz61g2AHFRl/8OjotWkiKbjmRRqW81/QiFACSBJrRa8O8CN0yEMcth2lGY9CuMmKv8EWjfu3rsmhdgXhR8eCNsfBNO7gF907aJFUIIIcS1s7OzIyYmhvj4+Br3x8fH07dv31qf06dPn8vGr1+/ntjYWGxtbU33vf3227z55pusXbuW2NjYK8ZSWlpKYmKiaQloSEgI/v7+NY5VVlbGli1b6oxNCJND38CcUOXctKXzqkqgGZcKCsvh6gf3LoFH/1d9n84WNFrAAEk/qxZaQ21Lyaa80kCItzMdfZwve7xXsCfujracLyzjQPoFFSIUQj2SQBM1abXgGwXXT1D+CNhV/dKsrFCWf6JRZqJtmwuLhsDbYfDdBDj0LRTnqhm5EEIIIeoxdepUPv30UxYvXkxiYiJTpkwhPT3dVMz3lVdeqdE5c+LEiZw4cYKpU6eSmJjI4sWLWbRoUY2aJnPmzOEf//gHixcvJjg4mMzMTDIzMykoKDCNmTZtGlu2bCEtLY3du3czatQo8vPzefTRRwFlSdDkyZOZNWsWq1at4s8//2Ts2LE4OTnxwAMPNNO7I6xW0hoouwg29lcea+2MSwOzk9WNQ1y9yBHKpbFLrBWIT6juvlnbMnpbnZabI3xqjBWitZAaaOLq6GzgsXVQmA1HN0DyOqVmWvF5OPS1so1aDF2UtvaU5IG9W80lo0IIIYRQzZgxY8jJyWHmzJlkZGTQpUsX1qxZQ1BQEAAZGRmkp6ebxoeEhLBmzRqmTJnCBx98QNu2bXnvvfe45557TGMWLFhAWVkZo0aNqnGs119/nRkzZgBw6tQp7r//frKzs/Hx8eGGG27g119/NR0X4MUXX6S4uJhJkyZx4cIFevfuzfr163F1baEF4YV5lBXBsV+U662hJphXJ4gdD97hoNcrX3wLy3Byj9I4wKsT2FbXgCRiBMS/BmnblM9HDu7qxXgVKir1bDpSVf+sluWbRnHRfvxw8AzxiWd5ZURUc4UnhOo0BmM12FYgPz8fd3d38vLypE6GOVRWwMndkLIOjm6Esaur1/x/OQYy/4ROQyB8mFIPwO7yKcBCCGGtSkpKSEtLIyQkpEbBdGGd6vt5yvmDdZCfUyt0ZA18dT+4d4DJf8gXt0I9C29UGjzcvxIibqn52PuxSt26UUugy93qxHeVfk3N4b6Pf6WNky17p8dho6s9SZtfUk7Mm/GUVxrY+PwAOvq4NHOkzUvO+ayfuc7z5GsL0Xg6GwjuB0Nmwt92XFIws0L5Fib/FOxfAivug3+HwLK7YfdHcD6t/v0KIYRo0Y4fP45Go+HgwYNqhyKEsGbGZXERwyV5JtSj10NOVV06Y6OHSxlnR1rBMs4NVUsyb470rTN5BuDmYMsNoUozmY2JsoxTtB6SQBPmp7OBqQnwwDfQ63HlW8HKUmXJ588vwns94I+v1Y5SCCFarYEDBzJ58mSz7W/s2LHceeedZtufEEJckb4Sktcq11vD8k2jc8mwd5FSTkVYhvzTUFEMWlvwCLr88ciRymXKeqUzp4UyGAzEVyXDhtSzfNPIuMRzQ0JWk8Ylro2c85mXJNBE07B1hPChMPIdZUr9pN3KTLWgG0FrA0GXdNX6aRp8/Qgc+AIK5BewEEIIIYS4gtP7ofAc2LtD8I1qR9N8jm2En6bCgWVqRyKMclKUS88QZSLBX7XrBU5eUF4C5440b2wNcOxcASdyirDTabkp3OeK4+OilQTavhPnOV9Y1tThCWERJIEmmp5GA76R0O85GPcTvHQc3Nspj+kr4c/vIOG/8N9JMLcTfHwzbP4XnP5NmRIthBDWpKyw/q2yonpsRVn9Y8uLq8caDLWPaaCxY8eyZcsW3n33XTQaDRqNhuPHj5OQkMCIESNwcXHBz8+Phx9+mOzsbNPzvv32W7p27YqjoyNeXl7ExcVRWFjIjBkz+Pzzz/nvf/9r2t/mzZsbHNeWLVu4/vrrsbe3JyAggJdffpmKiur3qq7jA2zevJnrr78eZ2dnPDw86NevHydOnGhwDEIIK5L+q3LZKQ50turG0pyMSwSzU9SNQ1TLrlq+6VXL8k0ArQ4e+g5eTAX/rs0XVwOtr1q+2TfMC2f7K/caDPRwJDrADb0BU+OBVkfO+a7qnO+ll14iPDwcJycnQkNDefXVVykvrzkb88cffyQ2NhYHBwe8vb25++7qeoGlpaW8+OKLtG/fHnt7ezp16sSiRYsa/H6Yg3ThFM3P/tKOWhp48BtlGnrKOsj4Hc78pmybZ4OzL9zxgTKbTQghrMGstvU/fu9n0Pku5fqmmbDz/brHtr0OntisXC/Kgbc7Xj5mRl6Dwnv33XdJTk6mS5cuzJw5E4DKykoGDBjAhAkTmDdvHsXFxbz00kuMHj2aTZs2kZGRwf3338+cOXO46667uHjxItu2bcNgMDBt2jQSExPJz89nyZIlAHh6ejYoptOnTzNixAjGjh3L0qVLOXLkCBMmTMDBwYEZM2bUe/yKigruvPNOJkyYwIoVKygrK2PPnj1opB6SEC1bv2ch6taaH1BbA2OS5nyq8kW0VqduPAKyk5XL2uqfGbW9rnliuQbG+mf1dd/8q7hoPxIy8tmQcJZRMe2aKjTLJed8V3XO5+rqymeffUbbtm05dOgQEyZMwNXVlRdffBGAn376ibvvvpvp06ezbNkyysrK+Omnn0zPf+SRR9i1axfvvfce3bt3Jy0trUbCrzlJAk2oS6uFdrHKNmg6XMxU6gOkrFfakhdmgUf76vG7P4aKEqWzp3e4FIwVQogGcnd3x87ODicnJ/z9/QF47bXX6NmzJ7NmzTKNW7x4Me3btyc5OZmCggIqKiq4++67CQpS6rt07Vr9LbqjoyOlpaWm/TXUggULaN++Pf/5z3/QaDRERkZy5swZXnrpJV577TUyMjLqPP758+fJy8vj1ltvpWNH5WQzKiqqUXEIIayMZ6jaETQ/9/Zg46CcD+eeaJ3vgaUxLuGsL4FmVJIPhsrq5msW4tzFUg6czAVgcJTvVT9vSJQf721MYWvKOUrKK3GwlYSuJbGUc75//OMfpuvBwcE8//zzrFy50pRA++c//8l9993HG2+8YRrXvXt3AJKTk/n666+Jj48nLi4OgNBQ9X7vSQJNWBZXf+j5iLJVlMGpPeATqTxmMMCu9yE3HeJfVYp0hg+DTsOU2he20lJYCGEB/n6m/sd19tXXB70GA1+pe6zmkkoLTl5X3ncj7d+/n19++QUXl8vb0B87doyhQ4cyePBgunbtyrBhwxg6dCijRo2iTRvzfABITEykT58+NWaN9evXj4KCAk6dOkX37t3rPL6npydjx45l2LBhDBkyhLi4OEaPHk1AQIBZYhNCWKDCbHD0VL6IbW20WvDsCFmHlaWDkkBT38VM5bKuJZxGv8yCbfNg4Etw0wtNH1cD/HIkC4MBuga6E+DueNXP6xLohr+bA5n5JexKzeHmiKtPvrUIcs53Vb799lvmz5/P0aNHTQk6Nzc30+MHDx5kwoQJtT734MGD6HQ6BgwY0Ojjm1Mr/KsjrIaNnZIYM36g0lfCDU9Bx0Ggs1O+ddvzMXxxD8wJgS/vg1P71I1ZCCHsnOvfLi0wbGNX/1jbS05iNZrax5iBXq/ntttu4+DBgzW2lJQUbrrpJnQ6HfHx8fz8889ER0fz/vvvExERQVpamlmObzAYLltyaTAYANBoNFc8/pIlS9i1axd9+/Zl5cqVhIeH8+uvv5olNiGEBfr6EZgXBalb1I5EHd5hyqVx6aBQ16Rf4flkaNuj/nFubUFfDkk/N0tYDWHsvtmQ5Zug/I2Oi1aSZvFVS0BbFTnnu6Jff/2V++67j+HDh7N69WoOHDjA9OnTKSurbjzh6Fh30ra+x9QgCTRhPXQ2cMNEeHgVvJgG930JPR8F17ZQXgTJP4P+kjoYxzYpBWZbW20MIYS4Ajs7OyorK023e/bsyeHDhwkODiYsLKzG5uysnLBpNBr69evHG2+8wYEDB7Czs2PVqlW17q+hoqOj2blzpylpBrBz505cXV0JDAy84vEBrrvuOl555RV27txJly5d+PLLLxsdjxDCghXmQPouKMiENsFqR6MO73Dl0rh0UKhLowFXP7Cxr39c+C3K5en9kJ/R9HFdpeKySralnANgSHTDEmhQnXTbmHgWvd5whdGiual9zrdjxw6CgoKYPn06sbGxdOrU6bJGT926dWPjxo21Pr9r167o9Xq2bLGML0wkgSask70LRI6E29+DqQkwcTvEzVDaRBvFvwaLh8HcMPj2Mfjjayg6r1rIQghhKYKDg9m9ezfHjx8nOzubp556ivPnz3P//fezZ88eUlNTWb9+PePHj6eyspLdu3cza9Ys9u3bR3p6Ot9//z3nzp0z1RoLDg7mjz/+ICkpiezs7Ms6K13JpEmTOHnyJM888wxHjhzhv//9L6+//jpTp05Fq9XWe/y0tDReeeUVdu3axYkTJ1i/fj3JyclSB02IliplPRj04NcF2gSpHY06vCPAowPYu115rGhaev3Vj3X1h8BY5Xry2qaJpxF2HM2mpFxPoIcjUQGuV37CX/Tp6IWznY6z+aX8eaZhRe5F01P7nC8sLIz09HS++uorjh07xnvvvVfjC1CA119/nRUrVvD666+TmJjIoUOHmDNnjul4jz76KOPHj+eHH34gLS2NzZs38/XXXzfNG3YFkkAT1k+jUVpC3ziluhNRZYVSO83BA4ovwJ/fwvcTlG4mnw6BLXPgwnE1oxZCCNVMmzYNnU5HdHQ0Pj4+lJWVsWPHDiorKxk2bBhdunThueeew93dHa1Wi5ubG1u3bmXEiBGEh4fzj3/8g3feeYfhw4cDMGHCBCIiIoiNjcXHx4cdO3Y0KJ7AwEDWrFnDnj176N69OxMnTuSxxx4zFZ2t7/hOTk4cOXKEe+65h/DwcJ544gmefvppnnzySbO/b0IIC5C0RrmMGKFuHGrqdi9MPgRD31Q7ErH1bZgbAdvnX934COXvpunfsQXYYFq+6duoDtb2NjpuCvdR9tUal3FaOLXP+e644w6mTJnC008/TY8ePdi5cyevvvpqjTEDBw7km2++4ccff6RHjx4MGjSI3bt3mx5fuHAho0aNYtKkSURGRjJhwgQKCwvN/2ZdBY3h0vUSLVx+fj7u7u7k5eXVKFonWrDKCji1F1LWQUo8nP2z+rFH/guhA5Xr+5YoCTX/ruDfDbw6SltwIUS9SkpKSEtLIyQkBAcHaWJi7er7ecr5g3WQn1MrUF4Cc0KhvBAm/AKBPdWOSLR23z6mfFEf9wbcOPnK47MSYcENSnH5F1OVVTUq0usNXD9rI9kFpSx77Hr6d/Jp1H6+23+K57/5nagAN35+rr+Zo1SfnPNZP3Od5zVqBtqCBQtMB46JiWHbtm31ji8tLWX69OkEBQVhb29Px44dWbx4senx77//ntjYWDw8PHB2dqZHjx4sW7bssv2cPn2ahx56CC8vL5ycnOjRowf79+9vzEsQrYXOBoL6KMs7/7YDphyGW/8POt+tJMqMDn0LO+bDd4/BB71gViB8Mhj+Nxn2LoKsIyq9ACGEEEIIAcDxbUryzDUA2l6ndjTq0uuVzvQlsmROVcY6dN5X6MBp5BOp1O6rLIXUX5osrKv1+6lcsgtKcbW3oXeIV6P3MyjSF60GEjPyOXWhyIwRCmFZGpxAW7lyJZMnT2b69OkcOHCA/v37M3z4cNLT0+t8zujRo9m4cSOLFi0iKSmJFStWEBkZaXrc09OT6dOns2vXLv744w/GjRvHuHHjWLdunWnMhQsX6NevH7a2tvz8888kJCTwzjvv4OHh0dCXIFoz93YQOx7uXQJOntX393xYub9dL7B1gopiOL0P9i+Bn6bCweXVYzP+UFpQp2yAizJNWQgh/mrWrFm4uLjUuhmXAAghRIMd+Um5jBhe3aW9tVp+N8zvapEdHVsNgwGyjyrXva4ygabRQMRIQKPMRlOZsXPmgAgf7GwaX92pjbMdscHKZytZxtm6tLZzPpsrD6lp3rx5PPbYYzz++OMAzJ8/n3Xr1rFw4UJmz5592fi1a9eyZcsWUlNT8fRU/lMFBwfXGDNw4MAat5977jk+//xztm/fzrBhwwD497//Tfv27VmyZIlp3F/3I0Sjdb9P2QD0lXA+FTL/gMxDytb+huqxxzbCxjeqbzv7Vi39rNoCul/9t1BCCNECTZw4kdGjR9f6mKW1IxdCWJGM35XLiJHqxmEJjB1Is6UTp2ryzygzIrU24Bly9c/r+4yy3NPFt8lCu1rG+meN6b75V0Oi/NiTdp4NiVmM7deA90NYtdZ2ztegBFpZWRn79+/n5ZdfrnH/0KFD2blzZ63P+fHHH4mNjWXOnDksW7YMZ2dnbr/9dt58881a31CDwcCmTZtISkri3//+d439DBs2jHvvvZctW7YQGBjIpEmTmDBhQp3xlpaWUlpaarqdn5/fkJcrWiutTkmAeXeCLvdc/rhPJHQZpSTWclKgMEtJqh2rar3rEwVP/apcryiDA0uV5aK+0arXORBCiObg6elp+tJMCCHMZsImyDionFO1dsYva3MkgaYa43vfJhh0tlf/PLeAJgmnoU7kFJJ8tgCdVsPA8GtP5sVF+/HPNYn8mppDfkk5bg4NeE+E1Wpt53wNSqBlZ2dTWVmJn1/NDLWfnx+ZmZm1Pic1NZXt27fj4ODAqlWryM7OZtKkSZw/f75GHbS8vDwCAwMpLS1Fp9OxYMEChgwZUmM/CxcuZOrUqfz9739nz549PPvss9jb2/PII4/UeuzZs2fzxhtv1PqYEI0WMby6g05ZkTL9+tLZan6XnNRlJ8NPz1fd0CjNCUyz1boply5+sgxBCCvWinrxtGjycxTCCmg0UvvMyLhk0LiEUDQ/4+y/q12++VcVZZB3Uvl8oIINiVkAXB/sibvTtSe7Qryd6ejjzLFzhWxJOsdt3dte8z4tjZwrWC9z/ewavIQTuKy9rcFgqLPlrV6vR6PR8MUXX+Du7g4oy0BHjRrFBx98YJqF5urqysGDBykoKGDjxo1MnTqV0NBQ0/JOvV5PbGwss2bNAuC6667j8OHDLFy4sM4E2iuvvMLUqVNNt/Pz82nfvn1jXrIQtbNzgnYxylYbQyWEDVESawWZkHNU2Q6vqh4zJQHcA5XraduU6dxeYdIFVAgLZ2urnGwWFRW1yCnqrU1RkVL02PhzFUJYmAsnoE2Q2lFYDu8w5TLnqFJ+RM4bm19OVfLS+LNoiJN7lTp2Lr7wjDpN8Yy1yuLMsHzTaEi0P8e2HGND4tkWlUCTcz7rZ67zvAYl0Ly9vdHpdJfNNsvKyrpsVppRQEAAgYGBpuQZQFRUFAaDgVOnTtGpk5Kx12q1hIUpv3x69OhBYmIis2fPNiXQAgICiI6uOV07KiqK7777rs547e3tsbe3b8hLFMK8ArrDQ98q1wuyqmepGbfi8+B2yR+X7x5XEm02jspMtktnq8kSUCEsik6nw8PDg6ws5RtcJyenOr9MEpbLYDBQVFREVlYWHh4e6HTyIVQIi3PhBLzbTSmT8eRWsLFTOyL1eQSBzk7p5ph3srommmg+MWPBr0vjlhT7REB5sZKEO5cMPuFmD68+uUVl7Dl+HlBql5nLkGhfPtxyjF+OZFFeqcdW1/jGBJZEzvmsl7nP8xqUQLOzsyMmJob4+Hjuuusu0/3x8fHccccdtT6nX79+fPPNNxQUFODionz4T05ORqvV0q5duzqPZTAYatQv69evH0lJSTXGJCcnExQk30QJK+HiC2GDlc2osrx6+WZZkfLNaulFpSDp6f3KZqKBez6FrqOUm7npoLUFV39ZAiqESvz9/QFMJ1TCenl4eJh+nkIIC2PsNOnkJckzI60OPDvCuURlGack0Jqfb5SyNYaDG4TcpNRQTlrT7Am0zUnnqNQbiPBzpYOXk9n226N9G7yc7cgpLGNv2nn6hnmbbd9qk3M+62au87wGL+GcOnUqDz/8MLGxsfTp04ePP/6Y9PR0Jk6cCCjLJk+fPs3SpUsBeOCBB3jzzTcZN24cb7zxBtnZ2bzwwguMHz/eNP1x9uzZxMbG0rFjR8rKylizZg1Lly5l4cKFpuNOmTKFvn37MmvWLEaPHs2ePXv4+OOP+fjjj6/5TRBCNZcWHLVzgsfWg14PF9Jq1lXLPAQXM2p2+Nn8bzi4HJy8q2apdVE6gto4gK0DeEdAh97K2OJc5RsuG3tldputgzLOxgFsHWXavxCNpNFoCAgIwNfXl/LycrXDEY1ka2srM8+EsGRJa5RLYw1aofAOUxJoOSnQKU7taERDRQyvTqDdOLlZDx2faFy+ad5OoDqthkGRvnyz/xTxiWdbVAJNzvmslznP8xqcQBszZgw5OTnMnDmTjIwMunTpwpo1a0wzwTIyMkhPTzeNd3FxIT4+nmeeeYbY2Fi8vLwYPXo0b731lmlMYWEhkyZN4tSpUzg6OhIZGcny5csZM2aMaUyvXr1YtWoVr7zyCjNnziQkJIT58+fz4IMPXsvrF8LyaLVKMVGvjtC5eqYnBefA0aP6dtlF0GihKBtSf1G2S/V8pDqBdnq/UmehzmPawAvHqvf/1YPKcokaiTYHJflmY6/8wY+saiGflQhHN9Z83NaxZoIusGf1sYpzlSUHNg7KaxWiBdDpdJKAEUKIplCcCyd2KNcjR6gaisUZ9KqytQm58lhhXjnHYPs8COgB109o3D4iRsCaaXByj3Ke7+Jj1hDrUlahZ0vSOQDizLh80ygu2o9v9p9iQ+JZXrs1usUtdZRzvtatUU0EJk2axKRJk2p97LPPPrvsvsjISOLj4+vc31tvvVUjoVaXW2+9lVtvvfWq4xSiRfnrH9XRS5XaCVmJygy1s4ehJBcqSqC8BPy6Vo/V2oBHB+X+ilKoKIbKsurH9RVKQsvoXFL9bdHdAqsTaKf3w/rpdY+1cYB/nK2+/X6MkvQD0Nlfkpyrmh034MXqZaqJq+Hkr9B/Ws3koRBCCCFah6MblPMUn0jwDFU7GsviE6F2BK1X5h9wYLlyztzYBJp7oFIvOeN3SFkH1z1k3hjrsDsth4LSCrxd7OnezsPs++/fyRt7Gy0nzxeTfLaACH9Xsx9DCLU0KoEmhLAQxtldl87wqk3oAJh8qOZ9er2SbDNuNpc03LjrIyi5oCTbyourx5RXXQb1rR7r0QG6jLpkX8bnVCXqdH9p5FFRXduQylJlK82rvq+soPr6uUTY+T78uQru+hBC+l/d+yKEEEKIluHIT8qlLN8UliS7qgOnV6dr20/ESCWBdmRNsyXQTN03o3zRas0/O8zJzoYbw7zZeCSL+IRMSaCJFkUSaEK0VlqtUnfNrpbCoe1irn4/ITcp29V66fglCblLEm3G5JzXJa3AO/RVliVcSIPPb4O+TytLFWyku64QQgjR4lWUKTPQQEk0iJoqy+GbsZCdAhM2gr0kKppNdrJy6X2tCbThsHkW5J8Gg6HJG4MZDAbiqxJoQ6LNv3zTKC7aT0mgJWbx9KBrfI+EsCCSQBNCNC+dDehcwN7lymOD+8HE7bDu7/Db58pstKOb4J5PwK9z08cqhBBCCPUYy0k4+0JgA77cay10tnByNxSeU5pFtb1O7YhaD+O/zWtNoPl3hWd+U2ofN4OEjHzO5JXgYKulXxMW+B8cqTQn+P1kLln5Jfi6OVzhGUJYB6ngLYSwbPYucPt7cN8KpeNo1mH4eKCSTDMY1I5OCCGEEE3Fr7PS5Gjsamk8VBfjEkLjkkLR9AwG8y3h1GiaLXkGsCEhC4D+nXxwsG26Qvi+bg50b+8BwMYjWU12HCGam/wlEkJYh8gRMOlXCB+uNEA4uVvtiIQQQgjR1GzspFh+fbyrSl8YlxSKpldwFsougkYLnmbqgGowwJkDSo3iJrQhsWr5ZhN03/yrIVHKLDRjzTUhWgJJoAkhrIeLD9y/Au5YALe+W10nojBHZqMJIYQQLUlhNhTIzJUr8g5XLuvrni7My5is9AgyT11egwEW3KCssDi9/9r3V4eMvGIOnc5Do4Gbq5ZYNqUh0f4AbD+aTVFZRZMfT4jmIAk0IYR10WjgugfB2Uu5XXoRPh0E346HovPqxiaEEEII89i7COaGQ/xrakdi2WQJZ/Ozc4boO6HTEPPsT6MB32jletIa8+yzFhsSlYR0zw5t8HFt+oZc4X4utPd0pLRCz7aU7CY/nhDNQRJoQgjrdnwH5J6Ew9/Dwn5w7Be1IxJCCCHEtUpaAxiuvcZUS2csYp9ztMmX/4kqgTEw+nMY8bb59hkxQrlM+tl8+/wL41LKuGZYvgmg0WhMx5JlnKKlkASaEMK6RdwCj8eDVxhcPAPL7oSfX4byYrUjE0IIIURj5J2GjIOABsKHqR2NZfMIAq0tVBRD/mm1oxGN1SkOtDZwLhHOp5p99wWlFew6lgPAkOimX75pZKy1tulIFpV6KbcirJ8k0IQQ1i8wBp7cBr0eV27vXqjUkcj4XdWwhBBCCNEIyVWzcNpfDy7N92HfKuls4MYpMGwW2DqpHU3rkLwOzh6GSjPW9XJsA0F9letNMAttW/I5yir1BHs50dHHxez7r0uvEE/cHGzIKSzj4MkLzXZcIZqKJNCEEC2DnROMfAce/BZc/ODcEfhkMJzap3ZkQgghhGiII1V1oCKGqxuHtRg0Hfo8VV0fVjSd8hL4cgws7AtFOebdd8RI5fKI+eugxSdWL9/UGJtwNQNbndbUsCA+QZqCCOsnCTQhRMvSaQj8bRdE3qp8c932OrUjEkIIIcTVKsmHtK3KdWNCQQhLcT4VMIC9u/lnR0bcolym7zJrY6yKSj2bjijJq7jo5ql/diljHbT4hMxmP7YQ5majdgBCCGF2zl4wZjmUFYJWp9yXdQRO74MeDyrdjoQQQghheY5tAn05eHasLpAv6ncxU1n2Z9BDr8fUjqZly0lRLr3DzH8+2SYYfDsrM9tyjoGTp1l2u//EBXKLyvFwsiU2qI1Z9tkQAyJ8sNFqOHaukNRzBYQ24xJSIcxNZqAJIVomjQbsq/5AV5TB9xPgv0/Byoeg0MxT7oUQQghhHheOK0XxI0fIF15X68IJWD0Zts1TO5KWL7sqgdZU3WEf/h6mJkL7Xmbb5Yaq5ZuDInyx0TX/x383B1tuCFWWF29MlGWcwrpJAk0I0fJpddD5LuWE/MhqWHADJK9XOyohhBBC/NWNk+HFVOg3We1IrIdxpl7+KWX2vWg6OUeVS++wptm/qz9ozfcR3WAwEJ9QVf9MheWbRnFRVXXQqpJ5QlgrSaAJIVo+rQ76T4UJG8E7Agqz4Mt7YfVUOdEUQgghLI2DGzh7qx2F9XDyBKeqBgLGBI9oGtnJymVTzUAzOp8K59OueTfHzhVyPKcIO52Wm8J9zBBY4xiTd/uOn+dCYZlqcQhxrSSBJoRoPQK6w5NboPfflNv7FsFHN8Gp/erGJYQQQgg4exhKC9SOwjoZEzrGJYbC/AwGyDbOQAtvuuP8Mhveuw52vnfNuzIu37yhoxcu9uqVP2/XxomoADf0BkwNDYSwRpJAE0K0LraOMPxf8PAqcA1QvqlNWKV2VEIIIUTrZjDAVw/CnFBI/1XtaKyPcUmhzEBrOsUXQF8BaMAztOmO0y5WuUz6Wfl/cQ2MyzeHqLh802hI1TLODbKMU1gxSaAJIVqnjoPgbzuh33Mw6NXq+ytK1YtJCCGa2IIFCwgJCcHBwYGYmBi2bdtW7/gtW7YQExODg4MDoaGhfPjhhzUe/+STT+jfvz9t2rShTZs2xMXFsWfPnhpjZs+eTa9evXB1dcXX15c777yTpKSkGmPGjh2LRqOpsd1www3medHCOpw7Aheqlqz5dVE3FmskM9CanpMn/P00PJ8Etg5Nd5zg/mDrDBcz4MyBRu8mu6CU39IvANU1yNRkXMa5JfkcJeWVKkcjRONIAk0I0Xo5ecKQmWBjr9zOO6VMmd+35Jq/8RNCCEuzcuVKJk+ezPTp0zlw4AD9+/dn+PDhpKen1zo+LS2NESNG0L9/fw4cOMDf//53nn32Wb777jvTmM2bN3P//ffzyy+/sGvXLjp06MDQoUM5ffq0acyWLVt46qmn+PXXX4mPj6eiooKhQ4dSWFizBuUtt9xCRkaGaVuzZk3TvBHCMiVV/bxDB1Z30RZXz9hIwFijSzQNjQZcm3g2l60DhA1Wrif93OjdbDqShcEAXQLdCHB3NFNwjdelrTt+bvYUlVXya2qO2uEI0SiSQBNCCKM9n0D+aaUV/Ir7oEBqNAghWo558+bx2GOP8fjjjxMVFcX8+fNp3749CxcurHX8hx9+SIcOHZg/fz5RUVE8/vjjjB8/nrlz55rGfPHFF0yaNIkePXoQGRnJJ598gl6vZ+PGjaYxa9euZezYsXTu3Jnu3buzZMkS0tPT2b+/Zv1Je3t7/P39TZunp2e9r6e0tJT8/Pwam7BiR6oSaBHD1Y3DWnmHg1s78OigdiQtl74ZZ01FjFAuryGBtsHYfTNK/eWbAFqthsFVscgyTmGtJIEmhBBGg1+HoW+Bzg6S18KCPtUn9EIIYcXKysrYv38/Q4cOrXH/0KFD2blzZ63P2bVr12Xjhw0bxr59+ygvL6/1OUVFRZSXl9eb/MrLywO4bMzmzZvx9fUlPDycCRMmkJVV/5cYs2fPxt3d3bS1b9++3vHCgl08C6f3KdfDb1E3Fmvl3QmmHob7vlA7kpbrqwfgnajmOTfsNBQ0Wjh7CC6caPDTS8or2ZaSDVhOAg2qa7FtSMjCIKs9hBWSBJoQQhhptdD3GXhiM/h2hqJs+Op++PEZ6QomhLBq2dnZVFZW4udX84OUn58fmZmZtT4nMzOz1vEVFRVkZ2fX+pyXX36ZwMBA4uLian3cYDAwdepUbrzxRrp0qa5zNXz4cL744gs2bdrEO++8w969exk0aBClpXXXpXzllVfIy8szbSdPnqxzrLBwyVWzbAJjwC1A3ViEqEt2Mlw80zxLjJ29oEMf5Xry2gY/fcfRbIrLK2nr7kDntm5mDq7x+oR64WSnIzO/hD9Py6xhYX3U62UrhBCWyq8zPPELbHoTdv4HflsKaduU+xzbqB2dEEI0mkajqXHbYDBcdt+Vxtd2P8CcOXNYsWIFmzdvxsGh9gLbTz/9NH/88Qfbt2+vcf+YMWNM17t06UJsbCxBQUH89NNP3H333bXuy97eHnt7+zpjF1bEuExNlm9eG71eSfBotODWVu1oWpaKsuqZYMaGDU0tYjhk/AGlFxv8VOMSybhov3p/xzc3B1sdN3XyYe3hTOITz9K1nbvaIQnRIDIDTQghamNjryznfPR/4N4e2veW5JkQwmp5e3uj0+kum22WlZV12SwzI39//1rH29jY4OXlVeP+uXPnMmvWLNavX0+3bt1q3d8zzzzDjz/+yC+//EK7du3qjTcgIICgoCBSUqSjYItnMEBZVUMJY90n0Tgb34D/6ww73lU7kpbnQhoYKsHOBVz9m+eYsePhxVS4aVqDnqbXG9iQqCyBt6Tlm0ZxpmWcUgdNWB9JoAkhRH1C+sPfdsCIt6vvS98N56TLlRDCetjZ2RETE0N8fHyN++Pj4+nbt2+tz+nTp89l49evX09sbCy2tram+95++23efPNN1q5dS2xs7GX7MRgMPP3003z//fds2rSJkJCQK8abk5PDyZMnCQiQ5XwtnkYDY1fD88ngG612NNbNq6NyKZ04zc/4nnqFKf9mm4OdM9jYNfhpf5zO49zFUlzsbegdWn8zFjUMivRFq4GEjHxO5xarHY4QDSIJNCGEuBIHd3Coqh9RfAG+GQsf3aR07ZQCqEIIKzF16lQ+/fRTFi9eTGJiIlOmTCE9PZ2JEycCSk2xRx55xDR+4sSJnDhxgqlTp5KYmMjixYtZtGgR06ZVz4aYM2cO//jHP1i8eDHBwcFkZmaSmZlJQUF13cinnnqK5cuX8+WXX+Lq6moaU1ysfHAqKChg2rRp7Nq1i+PHj7N582Zuu+02vL29ueuuu5rp3RGqc/VrvsRES2VcWph9VN04WqLsqtmw3uHNf+yCc0opkatknNk1INwHextdU0XVaJ7OdsQGKYk9mYUmrI0k0IQQoiEqy8E3EiqKYc00WH4P5GeoHZUQQlzRmDFjmD9/PjNnzqRHjx5s3bqVNWvWEBQUBEBGRgbp6emm8SEhIaxZs4bNmzfTo0cP3nzzTd577z3uuece05gFCxZQVlbGqFGjCAgIMG1z5841jVm4cCF5eXkMHDiwxpiVK1cCoNPpOHToEHfccQfh4eE8+uijhIeHs2vXLlxdXZvp3RGq0FdC2lblb6u4dt5VCbS8k1AuM3vMKqcqKendTPXPjM4ehrmd4KsHlTpsVyE+wVj/zLcpI7smxtiMtdqEsBYaQyvqH5ufn4+7uzt5eXm4uVlONxIhhJXR62HvJxD/GlSUKLXRbnsXou9QOzIhRBOQ8wfrID8nK3RiFyy5BdoEw7MHZQbatTIY4N/BUJILE3eAf5crPUNcrU+HwKk9MGoJdKm9sUmT0OvhnXAoPAcP/wAdb653eHpOETe9/Qs6rYb9/4jDw6nhS0CbQ+q5Aga9swVbnYb9rw7BzcH2yk8Sook05PxBZqAJIURDabXQ+0l4Ygv4d1OWdX79CKyaCCV5akcnhBBCWIekNcplu+sleWYOGk31DCmpg2Zet8yG295Tmko1J60Wwm9Rrhu71dbDOKOrV3Abi02eAYT6uBDq40x5pYGtyefUDkeIqyYJNCGEaCzfSHh8I/R/XmkZ//sKOPyD2lEJIYQQ1sGYEIgYrm4cLYmxRleO1EEzq3axEPMouAc2/7GN3WmTfr5i7V1jAs0Su2/+1ZAo6cYprI8k0IQQ4lrY2MHg12DczxAzFnpWF+BuFQ0GDAYozoWcY1BacMXhQgghBKAUZc9JAa0thMWpHU3L4RWmXBqL3gvrFzoQbBwhLx3O/lnnsLyicnannQdgSLQVJNCqYtx0JIvySr3K0QhxdWzUDkAIIVqEDjcom1HG77B6CtzxAfhGqRdXY+grlWWpBVlQmKV0fyo8pyxb1VZ1c1rxAGQcVO6vrCpqa+MIUbdC9/sg9ObqsUIIIcRfGZdvhvSv7nQtrl33+5UZfZ6hakfSciSvhyOrlURv9O3Nf3w7J6X2WdIaZRaaf9dah21OzqJSb6CTrwtBXs7NHGTDXdehDZ7OdpwvLGPv8fP07eitdkhCXFGjZqAtWLCAkJAQHBwciImJYdu2+tvqlpaWMn36dIKCgrC3t6djx44sXrzY9Pj3339PbGwsHh4eODs706NHD5YtW1ZjHzNmzECj0dTY/P39GxO+EEI0vbV/h9P74aMBsGuBUgRWTZXlkH8GzhyElA1w8Es4tqn68dP7YWE/eLsTvOkNb3eEhX1g6R3w/eOw7hUoOl89vuAs5J+umTyrKIZD3yidSbe906wvTwghhJUxLd8coW4cLY1bgPLFnY292pG0HMe3wW+fw/Ht6sVgXOZsTDzXwth90xpmnwHotBoGRVZ140zIUjkaIa5Og2egrVy5ksmTJ7NgwQL69evHRx99xPDhw0lISKBDhw61Pmf06NGcPXuWRYsWERYWRlZWFhUVFabHPT09mT59OpGRkdjZ2bF69WrGjRuHr68vw4YNM43r3LkzGzZsMN3W6WR2gxDCQo1aDD8+DSnrleRT8lq4c6F5a2eUFSkzwArPVc8WixgJLj7K4xveUL4xLTynzCj7q6jboeMg5bpGd/myAEdPcPEFZx/l0lBZ/diIOcqls4+y2TjA6d+UOnB/fgvRd1aP3f8ZVJRCl3vAWb5dFEKIVq8wG07uVq5L/TNh6Yz15IwNGtQQfgvo7JXO7xVlSgmRS5RV6NmSpBTjj7OSBBootdq+3X+K+MRMXr01Co00ExEWrsEJtHnz5vHYY4/x+OOPAzB//nzWrVvHwoULmT179mXj165dy5YtW0hNTcXT0xOA4ODgGmMGDhxY4/Zzzz3H559/zvbt22sk0GxsbBo066y0tJTS0lLT7fz8/Kt+rhBCXBNXP3jga9i3GNb/A9K2KDO6Rs6DrqNqf47BAKX5VUsms5TEl0anLIsEZQbYivuqkmXnoKyWmmOeodUJtMJzNbtwaXRKAsvZVxkT0K36Ma8weOi7qsd8wckbdPX8iQiMufy+djHKdsts0FW1I9frYetcyDsJ6/4OnYYpSzzDh8m340II0Vrlpit/r2ydwL2d2tG0PD+/BKlb4NZ5ENRX7Wisn/FcylhfTg0uvvBSGtjVvjRzT9p5LpZW4O1iR492Hs0b2zXo38kbOxstJ88Xk5JVQLifq9ohCVGvBiXQysrK2L9/Py+//HKN+4cOHcrOnTtrfc6PP/5IbGwsc+bMYdmyZTg7O3P77bfz5ptv4ujoeNl4g8HApk2bSEpK4t///neNx1JSUmjbti329vb07t2bWbNmERpa9/r+2bNn88YbbzTkJQohhPloNNDrMQgZAKueUJZJfvcYpMTDXR8qjx/8EvZ8rHwbX5AFlaU19+HVqTqBZudc/Y29kc6+epaYsw/YXnJi1XsidL23+nFHT6Udem3sXcxXxNmYPAPQV0DfZ5TXmXEQkn5SNsc2yoy07g9AYE/lvRBCCNE6BPaEZ/YrTWiE+Z1Pg3OJkJUoCbRrVVkOF44r140dTtVSR/IMqrtvDo70Q6u1nnMqZ3sbbgzzZtORLOITzkoCTVi8BiXQsrOzqaysxM+v5rRQPz8/MjMza31Oamoq27dvx8HBgVWrVpGdnc2kSZM4f/58jTpoeXl5BAYGUlpaik6nY8GCBQwZMsT0eO/evVm6dCnh4eGcPXuWt956i759+3L48GG8vLxqPfYrr7zC1KlTTbfz8/Np3759Q16yEEJcO+8wGL9OqQu2ZQ54dKhOGBXnwpkDNcfbuVTPFLu0CLCNPYxZDk5e1bPI7N3qTj75d2mSl9MgNnZK84HeTyon8r+vgD++hosZsPdTZRu7BoL7qR2pEEKI5ubooXYELZN3J0hZV730UDTehePKl4G2zuDWVu1ooKwQjv0CoQPAXkk2GQwGU/0za1q+aRQX5WdKoD11s4qz/IS4Co3qwvnXtckGg6HO9cp6vR6NRsMXX3yBu7s7oCwDHTVqFB988IFpFpqrqysHDx6koKCAjRs3MnXqVEJDQ03LO4cPr66P0LVrV/r06UPHjh35/PPPayTJLmVvb4+9vSwREkJYAJ0tDHxZqfXiG119f8Qt0Ca4qs6Yj5IYs3Oqez9RtzV5qE3GNwqGzITBr0PqZiWZduZAze6lP78Eba9TXmc937QKIYSwUtkpUF4E/t1k9nFTMS41zE5RN46WwPgeenW0jH+vnw6BrMNw7+fQ+U4AEjMucjq3GAdbLTeGWV+t2cFRvrAKDp7MJetiCb6uDmqHJESdGpRA8/b2RqfTXTbbLCsr67JZaUYBAQEEBgaakmcAUVFRGAwGTp06RadOSjFGrVZLWJjyy75Hjx4kJiYye/bsy+qjGTk7O9O1a1dSUuQPgxDCigR0r3nbM7T1tZrX6iBssLLpK5XbANlHYfeHyvXVUyH6DuhxPwTdWPfSUyGEENZl5/tKR8Mbp0DcDLWjaZmMxe4vrYMqGien6rOmmg0ELtXxZiWBlvSzKYFmXL55Y5gPjnbW12TPz82B7u3c+f1UHpsSs7jv+tobEwphCRr0icTOzo6YmBji4+Nr3B8fH0/fvrWvr+/Xrx9nzpyhoKC62HVycjJarZZ27eouGmowGGo0APir0tJSEhMTCQgIaMhLEEIIYUm0l5zoObjDzdOVhGJ5Ifz+JXx+G7zbDTa+Kd+kCyGEtdPrla7UAMH91Y2lJTPW6spNh/ISdWOxdm1ClFnxQRZSaiJihHKZsg4qK4DqBNqQaF+1orpmcVHKZBzjaxHCUjX4K/2pU6fy6aefsnjxYhITE5kyZQrp6elMnDgRUOqOPfLII6bxDzzwAF5eXowbN46EhAS2bt3KCy+8wPjx403LN2fPnk18fDypqakcOXKEefPmsXTpUh566CHTfqZNm8aWLVtIS0tj9+7djBo1ivz8fB599NFrfQ+EEEJYAhcfGPAiPPMbjF8PMePA3l3p4LltLnw0AMqL1Y5SCCFEY535DQrOgp2rJNCakrOP8vcTA5xPVTsa6xZ9u1J/ttdjakeiaN9baQpVfAFO/kpmXgl/nMpDo4FBkdZX/8zIWLttW0o2xWWVKkcjRN0aXANtzJgx5OTkMHPmTDIyMujSpQtr1qwhKCgIgIyMDNLT003jXVxciI+P55lnniE2NhYvLy9Gjx7NW2+9ZRpTWFjIpEmTOHXqFI6OjkRGRrJ8+XLGjBljGnPq1Cnuv/9+srOz8fHx4YYbbuDXX381HVcIIUQLodFAh97Kdsu/IPlnOLhCSbDZVnVvLjoPq6dAtzHQaUjNzp9CCCEsU9Ia5bJTnNJkRjQNjUZpYHR6v7IE0S/6ys8R1kFnA+HDlDqyST+z0UNpkNejvQc+rtZb+zvS35V2bRw5daGYbSnnGNrZX+2QhKiVxmAwGNQOornk5+fj7u5OXl4ebm5uaocjhBCiIQyG6gK+ez+Fn55Xrjt5Q9d7oft9So05SyjyK1oUOX+wDvJzsgIL+kBWAtz9CXQbrXY0Ldtvy5RZShEjlGSaaLjSAkj9RVkS6xOhdjTVEv4LXz8CbUIY6/oRm5OzeWFYhNV3sJzx42E+23mc0bHtmDOq+5WfIISZNOT8QaoyCyGEsA6XJsaCb4I+TytdS4uyYfdC+HgALOwLO96F/Az14hRCCHG582lK8kyjU2YOi6bV82Ho96wkz65FVgKsfAiW3aV2JDV1HAw6O7iQxtljfwAwNNp6l28aDal6DRsTs6jUt5o5PsLKSAJNCCGE9fEJh2H/hKmJ8MA30Pku0NkrJ7vxr8EPE9WOUAghxKWSflYug/qCYxt1YxHiahi7mHpZWBLS3gVCB5LbpitO+osEeTkR5uuidlTX7PoQT1wdbMgpLOPgyVy1wxGiVpJAE0IIYb10NhA+FO79DKYlw63zof0N0P3+6jFHN8B/n4LjO5QOcEIIIVRgUGYNG7sIiqZVXgy/LYWNM5USCKLhjN2/jV1NLcmYL3gz4AP2GyKIi/JD0wLKV9jqtAyMUDqJSjdOYakkgSaEEKJlcPSA2HHw2DqluYDR/s/hwHL4bAS81wN+mSVdyYQQorn1eQqeT4LY8WpH0kpo4MdnYds7UHhO7WCsU85R5dK7k7px1KJSa8umI0qSKS7K+pdvGhmXccYnSAJNWCZJoAkhhGh5Lv0m9oa/wXUPgZ0r5J6ALf+G966DRcNg/2dQnKtWlEII0bpotWDroHYUrYOtA3h0UK4bZ1KJhjG+b5a2hBP4Lf0C+UUlDHJIppfNUbXDMZsB4T7YaDUczSogLbtQ7XCEuIwk0IQQQrRsQX3hjg+UJZ53fwIdBwEaOPkr/O85paOnEEKIppO8Hi4cVzuK1sc4c8pYy0tcvcqK6tnqFjgDLT7hLE/ofmIxM7DZ8X9qh2M27o629A71BGCjLOMUFkgSaEIIIVoHOyfoNhoeXgVTEyDuDfCNrrncc8scWDcdzh5WL04hhGhJKkrh2/HwbnfIPKR2NK2LsXZXTsuZodRsck+AvhxsHMGtndrRXGZDwlk26XsoN1J/gbKWM1vLuCRVlnEKS2SjdgBCCCFEs3NrCzdOVjajynLY/SEU5cCu/4B/V4i+E9oEg2sAuAWAWyDY2KsTsxBCWKPj26DsIrj4g29ntaNpXYxLD2UJZ8NdzABbJ/DsqCw9tiDHzhWQml2Ira4DevcOaPPSIXUzRI5UOzSziIvy443/JbDvxAUuFJbRxtlO7ZCEMJEEmhBCCAGABm7/D/z+JSStVWZK/HW2xJgvIOpW5fq+JXByN7j6Kwm2Sy9d/MFGTviEEIKkn5XLiFssLhHR4hmXHuZIAq3Bgm+EV05DSa7akVxmQ9XMrBtCvdG2HaF8+Ze0psUk0Np7OhHp78qRzIv8kpTF3T0tbwagaL0kgSaEEEIA6GwgcoSyFZ2HP7+D9F1w8azyTfTFDCVBZnR8O/z5bd37i7oNxixXrhecg72fXJ5sc/YBra5pX5cQQqjFYLgkgdYyPtxbFa+qBNqFE8pSWplB3TBaLTh5qh3FZTZU1QYbEu0HfsYE2lrQV7aYc4oh0X4cybzIhsSzkkATFkUSaEIIIcRfOXnC9ROUzchgqDmmxwPg3wUuZlYl2DIhvyrRpi9Xun4anT+mdP/8K40WXPyUhNr9XymXAEc3KMdzDVA2J8+anUWFEMIaZPwO+afB1hlCblI7mtbH1V+pg+bqDyV54OKrdkTWo6LMImeS5xSUsv/EBQAGR/mBayA4uENRNpzaBx16qxyhecRF+fH+pqNsSTpHaUUl9jYtIzEorJ8k0IQQQoir8dcEVthgZfsrg0GZwWaorL7PsQ3EPlY9k+1iJhScBYO++j47l+rx8a/D2T+rb+vslGWhrlVb13sh+nblscIc5cTZ1R/s3STRJoSwHMbZZ2GDwNZB3VhaI40Gnt6rdhTWaX4X5W/vwz+Ad5ja0ZhsOpKF3gCd27oR6OGo3NlpKBz6RlnG2UISaF0D3fF1tSfrYim/pp5nQLiP2iEJAUgCTQghhDAvjQacvWre5xMBt86reZ++EgrPVSXQzoL9JQk0n0hAozxWlA2VZZCXrmwAgTHVYxP/C6unKNdtnapnrRmTbR4doPeT1ePLS+SDrBCieST9pFxGjFA3DiEaojhX+ZILwNVP1VD+yrh809ipEoCI4XB6v1IWooXQajUMjvJjxZ50NiSclQSasBiSQBNCCCHUoNVVJ7n+atSi6usVZcqJ/KVLRS/9hrmyHOzdoTQPyouU5aLnj1U/7n5JAk2vh3+1h3a94LqHIPoOsHNumtcnhGjdKsrAMxQupEOnYWpH03oZDMrfjdKL4BOudjTWIeeocukaAPau9Y9tRiXllWxNzgaq6p8ZRd8Fne9ucTPQh0T7Kgm0xLPMvKMzmhb2+oR1kgSaEEIIYcls7MCjvbLVpveTylZWWJVkuyTRdjFDmZVmVJSjzGY7sUPZ1rwIXe6C6x6BdrEt7uRbCKEiGzsYvVRJ8uts1Y6m9Ur4L3zzKLS7Hh6PVzsa65Bd1bXUy3KWbgLsOpZDcXkl/m4OdG7rVv1AC+1u27ejN462OjLySjh8Jp8uge5qhySEJNCEEEKIFsHOGbw6KltdnL1h8p/wx0o4sBwupMFvS5XNO0KZlXb9E7LEUwhhPpI8U5fxb0J2sjIbTb4oubKcqgSadyd14/iL9QlVyzejfS+fjWUwKLVTzxyEng83f3BNwMFWx03h3qw7fJb4hLOSQBMWoWWmq4UQQghxOY1Gmcl20zR45jcY+xN0vx9sHCE7CXZ/WPPDrr6y7n0JIURdSvLg4JdKQxWhLs+OgAZKcpVZyOLKspOVSy/LSaDp9QY2VtU/GxJdS+mH/NPw4Y3wv2ehMLuZo2s6xlpvxtpvQqhNEmhCCCFEa6TVQvCNcNeHMC0JbnsXBr6s1GYDyD4K86KVjqDG5SxCCHE1jm6AH/4GS6R5gOrsnMC9qgSA/C6/OtlVNdC8Ladm3KHTeWRdLMXZTscNoZ6XD3BvB/5dle7eKeubP8AmMijSF40GDp/J50xusdrhCCEJNCGEEKLVc3CHmLHQ85Hq+w59DQWZsGM+/CcWFt+iLPssLVArSiGEtTiyRrkMH6puHELhXVXLK0cSaFekr4Tzqcp1b8upgWacgTUgwgd7G13tgyJGKpdHfmqmqJqel4s9MR3aAJhm4AmhJkmgCSGEEOJy/afB6GVK9zyNFtJ3wX+fgnci4L9Pw8m9akcohLBEleWQUlWs3viBXqjLuBRRZqBdnYe+g1v/r3rmngWIN9Y/i/Kre1DEcOXy2CYoL2mGqJqHseOosQacEGqSBJoQQgghLmdjB9G3w4Nfw5QEGPwaeIZCWQEcWAZ7P1U7QiGEJTqxA0rzwMlb6e4r1OctCbSrptVBSH+IHV9d0kBlJ88XcSTzIloN3BzhW/fAgO7gFgjlRZC2pfkCbGJxVQm0X1NzuFhSrnI0orWTBJoQQggh6ucWAP2fVxoPjPsZejwIMY9WP/7bUlhxv7Jsq1JOboVo1ZJ+Vi4jbrGYBESrZ0yg5RxVNw7RKMblm7HBnrRxtqt7oEZTPQstaU0zRNY8Ovq4EOrtTHmlga3JLadBgrBOkkATQgghxNXRaCCoL9y5QLk02v+ZcrL+1f1K44H1r8K5ZNXCFEKoxGCo/uAeIQ0ELEZgLEzcARO3qR2J5du1AP43GdJ/VTsSE2MCbWh0Pcs3jUwJtJ9Br2/CqJqXcRaadOMUapMEmhBCCCGuzR0LoO8z4OwDhVmw8z34oBcsGgq/LYPSi2pHKIRoDmcPQ2462DhA6M1qRyOM7F3AvwvYOqodieU78hPsXwIXjqsdCQB5xeXsTj0PwOD66p8ZBfeHNiHQaYhScqGFMNZ+23Qki4rKlpMYFNbHRu0AhBBCCGHlfCNh6Fsw+HVIXqd060xZDyd3K5tGC9c9qHaUQoimVlEKIQPAwQ3snNSORoiGM3YqNTZeUNmW5HNU6A2E+boQ4u185SfY2MOzB5QZ4y1ITFAb2jjZcqGonL3HL9Cno5faIYlWSmagCSGEEMI8dLYQdSs88BVMOQxxMyCgB3S+s3rMuumwbR5czFQpyNZtwYIFhISE4ODgQExMDNu21b+ka8uWLcTExODg4EBoaCgffvhhjcc/+eQT+vfvT5s2bWjTpg1xcXHs2bOnwcc1GAzMmDGDtm3b4ujoyMCBAzl8+PC1v2DRvNrFwKM/wr1L1Y5E/NWvH8LCfsqlqF1JPhRULRH0DlM3liobrqb75l+1sOQZgE6rYVCkLOMU6pMEmhBCCCHMzy0AbpwCT24Bu6pvzQtzYPdHsPENpVbal2MgcbU0HmgmK1euZPLkyUyfPp0DBw7Qv39/hg8fTnp6eq3j09LSGDFiBP379+fAgQP8/e9/59lnn+W7774zjdm8eTP3338/v/zyC7t27aJDhw4MHTqU06dPN+i4c+bMYd68efznP/9h7969+Pv7M2TIEC5elOW/VkkrHzEsTkkunP0Tzh5SOxLLZZx95uwLDu7qxgKUV+r5JSkLgCHR9XTfrE1uuvL3NvdkE0SmDuN7sCHxLAaDQeVoRGslf92EEEII0TxsHeHW/4P2N4ChEpLXwsoHYV6UMjMt64jaEbZo8+bN47HHHuPxxx8nKiqK+fPn0759exYuXFjr+A8//JAOHTowf/58oqKiePzxxxk/fjxz5841jfniiy+YNGkSPXr0IDIykk8++QS9Xs/GjRuv+rgGg4H58+czffp07r77brp06cLnn39OUVERX375ZdO+KcJ8jm9XOvGWFakdiaiNV9WMqmzpxFkn43vjHa5uHFX2pJ3nYkkF3i529GjfpmFP/mES/PwiHFndNMGpoH8nH+x0Wk7kFHE0q+XUdxPWRRJoQgghhGgedk7Q82F4bB08tRf6Pad80194Dnb9BxbcIEs7m0hZWRn79+9n6NChNe4fOnQoO3furPU5u3btumz8sGHD2LdvH+Xltc8aLCoqory8HE9Pz6s+blpaGpmZmTXG2NvbM2DAgDpjAygtLSU/P7/GJlS0fb7SiXe3LBG0SN5VNb2Ms6zE5YzvjYUs34yvWr45KNIXnbaByzKNXXCP/GTmqNTjbG9D3zCl9lm8LOMUKpEEmhBCCCGan084DJkJUxPgvhUQMRLC4sDVX3m8shx+eh6O7wBZqnHNsrOzqaysxM+vZh0dPz8/MjNrT1pmZmbWOr6iooLs7Oxan/Pyyy8TGBhIXFzcVR/XeNmQ2ABmz56Nu7u7aWvfvn2dY0UTKy2AtC3KdeMHd2FZjDPQinKg6Ly6sViqbMtpIGAwGEy1vhpU/8wo4hbl8sROKL5gxsjUNSRaeS+MyUUhmpsk0IQQQgihHp0tRI6A+7+EB1ZW35+yHvZ+Cp+NgPd7wrZ3IP+MenG2EJq/FJc2GAyX3Xel8bXdD0odsxUrVvD999/j4ODQ4OM2NLZXXnmFvLw803byZNPX+pG6O3U4thEqy6BNCPhEqB2NqI2dM7gFKtezZRZarYL6QuSt0PY6tSMh6exFTl0oxt5Gy42dvBu+A89Q8IlSyiWkxJs/QJUMrmokcPBkLlkXS1SORrRGkkATQgghhGXQ6qqve4ZCz0fAzgXOp8LGmfB/neGL0ZDwI1SUqRenFfL29kan0102oysrK+uymV9G/v7+tY63sbHBy8urxv1z585l1qxZrF+/nm7dujXouP7+yqzDhsQGyjJPNze3GltT2Zp8jvs+3sW/fpY6fbVK+lm5jBzZIjsAthiyjLN+vZ+E+76A4H5qR0L8YWWG1Y1h3jjZ2TRuJ5FVs0GT1pgpKvX5uzvQrZ07BgP8ciRL7XAsTkZeMU9/+RvrDks5jKbSqARaQ1ugl5aWMn36dIKCgrC3t6djx44sXrzY9Pj3339PbGwsHh4eODs706NHD5YtW1bn/mbPno1Go2Hy5MmNCV8IIYQQls43Cm5/H55PgjsWQIc+YNBDyjr4+mH4/Da1I7QqdnZ2xMTEEB9fcyZCfHw8ffv2rfU5ffr0uWz8+vXriY2NxdbW1nTf22+/zZtvvsnatWuJjY1t8HFDQkLw9/evMaasrIwtW7bUGVtzKyqr5NfU86w9nCmz0P6qsgKS1ynXI4arG4uon3FposxAs3im5ZvRjVi+aWRcTp2yASpKzRCVZTAuaY1PkATapS4UlvHwoj2s/iODV74/REl5pdohtUgNTmcbW5EvWLCAfv368dFHHzF8+HASEhLo0KFDrc8ZPXo0Z8+eZdGiRYSFhZGVlUVFRYXpcU9PT6ZPn05kZCR2dnasXr2acePG4evry7Bhw2rsa+/evXz88cc1vt0UQgghRAtl7wLXPahs2Ufh4HI4uKK6vgso3Tu1NhZT+NlSTZ06lYcffpjY2Fj69OnDxx9/THp6OhMnTgSUJZGnT59m6dKlAEycOJH//Oc/TJ06lQkTJrBr1y4WLVrEihUrTPucM2cOr776Kl9++SXBwcGmWWQuLi64uLhc1XGNX4rOmjWLTp060alTJ2bNmoWTkxMPPPBAc75FderfyRs7G6X7W0pWAeF+rmqHZDlO7obi8+DYRumwKyxX1K3gHgjB/dWOxPKcT4OsBPCNBs8QVUM5m1/C76fyABgc5dv4HbXtCS5+UHBW6ZIbNthMEaorLsqPefHJbD96juKyShztdFd+UgtXWFrB2M/2mrqTni8s45v9p3j4hiCVI2t5GpxAu7QVOcD8+fNZt24dCxcuZPbs2ZeNX7t2LVu2bCE1NdXUkSk4OLjGmIEDB9a4/dxzz/H555+zffv2Ggm0goICHnzwQT755BPeeuuthoYuhBBCCGvmHQZxM+Dmfyj1loz+WAn9n1ctLGsxZswYcnJymDlzJhkZGXTp0oU1a9YQFKScYGdkZJCenm4aHxISwpo1a5gyZQoffPABbdu25b333uOee+4xjVmwYAFlZWWMGjWqxrFef/11ZsyYcVXHBXjxxRcpLi5m0qRJXLhwgd69e7N+/XpcXS0jUeVsb0O/jl78knSO+ISzkkC7lHF5WKdhoGvkUjPRPEIHKpu4XPJaWPuyUgPtvi9UDWVjojKzqkd7D3xdHa4wuh5aLXS5B3LTwb7l/M6KCnAl0MOR07nF7DiafW2z9FqAsgo9E5fv5/eTubRxsuX27m35fNcJPt2WygPXd2h4B1dRrwYt4WxMC/Qff/yR2NhY5syZQ2BgIOHh4UybNo3i4uJaxxsMBjZu3EhSUhI33XRTjceeeuopRo4caersdCXS3lwIIYRogXQ2YOdUfTv8FmWmmriiSZMmcfz4cUpLS9m/f3+Nc63PPvuMzZs31xg/YMAAfvvtN0pLS0lLSzPNGjM6fvw4BoPhss2YPLua44IyC23GjBlkZGRQUlLCli1b6NKli1lf+7UyfkgzLq0SVTxDIaB7db0lIayRcVmrt/odOI2/Y4aYIzF0y2wlIdj++mvfl4XQaDTSjbOKXm/g+W9+Z1tKNk52OpaMu56Xhkfi4WTLiZwi1kstNLNrUAKtMS3QU1NT2b59O3/++SerVq1i/vz5fPvttzz11FM1xuXl5eHi4oKdnR0jR47k/fffZ8iQIabHv/rqK3777bdaZ7nVRdqbCyGEEK1Ah95qRyBaAWPdHen+9he9HoMnt0LU7WpHIq7GH99A/OuQd1rtSCxLdrJy6aVuAq2wtILtR7OB6t854nLG92bjkbPo9a2zLqXBYOCN/x3mf7+fwVan4aOHY+jR3gMnOxvT0s0Pt6ZK3U4za1QTgYa0Gdfr9Wg0Gr744guuv/56RowYwbx58/jss89qzEJzdXXl4MGD7N27l3/+859MnTrV9C3oyZMnee6551i+fPllbdHro0Z7cyGEEEII0fL4uTnQXbq/1U26b1qHHe/CjvmQ+YfakViWnKPKpXe4qmFsS8mmrEJPe09Hwv3MNLO6+AL8vhIO/2Ce/VmA60M8cbW3IbugjIOnctUORxXvbTzK57tOoNHAvNE96N/Jx/TYI32CsbPR8vvJXPaknVcxypanQQm0xrRADwgIIDAwEHd3d9N9UVFRGAwGTp06VR2IVktYWBg9evTg+eefZ9SoUabZZvv37ycrK4uYmBhsbGywsbFhy5YtvPfee9jY2FBZWXuHieZsby6EEEIIIVq26u5vrXvZkMmvH8KJnaCXbm9Ww9hsRTpxViu9CBczlOsqN6MxLd+M8q9zgkqDHVkDq56Abe+YZ38WwM5Gy4AIJWG0oRX+Pl726wn+b4Mya3Lm7Z25rXvbGo/7uNpzT892AHy8NbXZ42vJGpRAa0wL9H79+nHmzBkKCgpM9yUnJ6PVamnXrl2dxzIYDJSWKu12Bw8ezKFDhzh48KBpi42N5cEHH+TgwYPodNJ5QwghhBBCNC1jHbRtKdkUl7XypFFBllJ0fclwuCh1dqyGcYlijiTQTIyzz5y8lW6yKqnUG9hUNbs1Lvoaum/+VfgwQKPMOsw7dcXh1mJIK61LufqPM7z23z8BeG5wJx7uE1zruAn9Q9BoYOORLFLOXmzGCFu2Bi/hnDp1Kp9++imLFy8mMTGRKVOmXNYC/ZFHHjGNf+CBB/Dy8mLcuHEkJCSwdetWXnjhBcaPH4+joyOg1CqLj48nNTWVI0eOMG/ePJYuXcpDDz0EKMs7u3TpUmNzdnbGy8vL4grMCiGEEEKIlinSX+n+VlqhN9UparWS1wEGCOgB7oFqRyOulrFIfvZRdeOwJMb3QuUGAgfSL3C+sAw3Bxt6BXuab8fO3tC+qlZo0s/m26/KBob7YqPVkHy2gBM5hWqH0yy2pZxjysqDGAzw8A1BTI6r+99sqI8LQ6uSjDILzXwanEAbM2YM8+fPZ+bMmfTo0YOtW7fW2wLdxcWF+Ph4cnNzTbPGbrvtNt577z3TmMLCQiZNmkTnzp3p27cv3377LcuXL+fxxx83w0sUQgghhBDi2l3a/a01LhuqIWmNchk5Ut04RMN4ywy0yxRfABsH8FJ3+WZ81UyqmyN9sdU1qlR53Yxdco3/b1sAdydbrg9REo2tYVn97ydzeXLZfsorDdzaLYAZt3e+4jLfJ27qCMAPB09zNl+a35iDxtCK2jLk5+fj7u5OXl6e1EMTQgghxFWR8wfr0Fw/p+0p2Ty0aDfeLnbs+XscWm0rLJ5fVgRzQqGiGCZuB/+uakckrlbpRZhdVUbnpRPg6KFqOBZDr4fyIrA3U+H+Rhj0zmZSzxXy/v3XXVbT6pplp8B/YkFrCy8eAwf3Kz/HCizensbM1QncEOrJV0/0UTucJnM0q4B7P9zJhaJy+nfyZtGjvbCzubok670f7mTv8QtMHNCRl4dHNnGk1qkh5w9mTm0LIYQQQgjRcvUO9cTVoXV3fyN1s5I8c+8AflJOxarYu4JrgHI9R5Zxmmi1qibPjp0rIPVcIbY6jak4vll5d1Lq3+nL4ehG8+9fJcbGLnuPXyC3qEzlaJpGRl4xjyzazYWicrq39+DDh2KuOnkG1bPQvth9gosl5U0VZqshCTQhhBBCCCGukq1Oy8AIpcB3a1g2VCvjMrCI4WCuToGi+XS4AYJuBINe7UjUZzBAufpL2zZWLd+8IdQLNwfbpjlIxHDlsgXVQevg5USEnyuVegObk86pHY7ZXSgs4+FFeziTV0JHH2eWjO2Fs71Ng/YxONKXjj7OXCyp4Ks9J5so0tZDEmhCCCGEEEI0QFyUkkBrlXXQ9JWQvFa5bvxALqzLvZ/BuJ+g/fVqR6K+vFPwT394P1ZZxqmSDQlV3TerZlQ1iajbILg/BPdrumOowFiXsqV9oVFYWsG4z/ZyNKuAAHcHlj7WG09nuwbvR6vVMKF/KACLd6RRXimJ82shCTQhhBBCCCEawNj9LSWrgOPZraP7m0l5EXQZpdQ9C2pZH8RFK5SdDFSVBNeq89H4fGEZ+06cB2BwVXK+SbS/HsauhpixTXcMFcRVJdC2JJ+jtKJS5WjMo6xCz9+++I2DJ3PxcLJl2WPXE+jh2Oj93XldID6u9mTklfC/38+YMdLWRxJoQgghhBBCNMCl3d82JLasWQ9XZO8Kw/+lNA+wafhsCGEBDAYoyILT+9WORH3GOnDe4aqFsOlIFnoDRAW40a6Nk2pxWKtuge74uNpTUFrB7tTzaodzzfR6A9O++Z2tyedwtNWxZGwvwnxdr2mfDrY6xvYNBuDjram0oj6SZicJNCGEEEIIIRrIuGyo1SXQhPXLPQFzO8HiW5Qlua1Zdopy6R2mWgjGpeBDmnL2mVFlOSSthfWvKonUFkCr1VQvq7fy38cGg4E3/neYH38/g61Ow4cPx3BdhzZm2fdDvYNwstNxJPMiW5JbXr245iIJNCGEEEIIIRqoNXR/u8yFE7B1LmQlqh2JuBbu7UFnD5VlSjKtNcupSqB5dVLl8CXllWxNUZIZQ6L9m/6AlWXw9SOw870W9f/Y+Pt4Q8JZq55d9f6mo3y+6wQaDcy9tzsDws3XkdXdyZb7r+8AKLPQRONIAk0IIYQQQogGau/pRKS/0v3tl6QstcNpHon/g01vws8vqR2JuBZaHXh1VK4bZ2C1VtnGJZzqJNB2peZQVFaJn5s9XQLdmv6Ads4QOlC5nvRT0x+vmfQL88bRVseZvBIOn8lXO5xGWf7rCebFJwMw47bO3NEj0OzHGH9jCDqthp3Hcjh0Ks/s+28NJIEmhBBCCCFEI1TPemglCbSkNcpl5Eh14xDXzpgwas0JtLJCyD+lXFdpBppx+WZclB8ajaZ5Dho5QrlM+rl5jtcMHGx19O/kDVjnMs6f/sjg1f/+CcCzgzvxaFW9MnML9HDktm4BAHy09ViTHKOlkwSaEEIIIYQQjdASu7/Vqeg8pO9Sroffom4s4toZE0Y5rTiBZmwg4OgJzl7Nfni93mBK9hh/lzQL4//f0/shP6P5jtvE4qy0LuX2lGwmrzyAwQAP3dCBKXFNm8x94iZl9umaQxmcPF/UpMdqiSSBJoQQQgghRCN0C3THtwV1f6tX8jow6MGvC7QJUjsaca1MM9COqhuHmjw7wiM/wm3vqnL4P8/kcTa/FCc7HX1CmzGB5+oPgbHK9eS1zXfcJjYo0heNBv48nU9GXrHa4VyV30/m8sSyfZRXGhjZLYA3bu/S5DMRo9u60b+TN3oDLNqe1qTHaokkgSaEEEIIIUQjaLUaBlct44xPsK5ZDw1mXL4ZMULdOIR5yAw0sHeB0AEQfbsqhzcu3xwQ7oODra55Dx4xXLk0/r9uAbxd7OlZ1bFyQ6LlL6s/mlXAuM/2UlRWyY1h3swb3R2dtnmW8T5ZNQtt5d6TXChsJU1wzEQSaEIIIYQQQjTSkGhfQFk2ZM3d3+pVXgJHNyrXjR+8hXXzDlMuC85CiXUWXbd28VVJHmMtxWZlrGOYugVKC5r/+E0kzkq+0MjIK+bRxXs4X1hG93bufPhwDPY2zZdE7RfmRXSAG8XllSz7tZV34m0gSaAJIYQQQgjRSH07Kt3fMqy4+9sVHd8G5YXgGgBtr1M7GmEODu7wt53w9zPg0AzdHy3R/ybDT8/D+eZfxnbqQhGJGfloNXBzpG+zHx+fSGU26Y1TQF/e/MdvIkOq6qDtOpbNxRLLfF0XCst4ZNEeTucWE+rjzOKxvXCxt2nWGDQaDU8OCAXg853HKSlv4TU8zUgSaEIIIYQQQjSStXd/uyrOPtD9Aeh+PzRXp0DR9Pw6g52z2lGow2CAP76GvZ9CZfMnWjZWzT6LDfLE09mu2Y+PRgP3r4CbXwHHNs1//CbS0ceZEG9nyisNbEvJVjucyxSVVTD+872kZBXg7+bAssd64+Vir0osI7sGEOjhSE5hGd/9dkqVGKyRJNCEEEIIIYS4BkOstPvbVWvbA+5aCHGvqx2JEOaRf0aZVanRQZvgZj+8cYlhXLQKs89aMI1GQ1xU1bJ6C1vGWVahZ+Ly3ziQnouHky3LHrueQA9H1eKx0Wl57MYQAD7ZmkqlvoWWIDAzSaAJIYQQQghxDS7t/nYm1zq6vwlB8jr48EZYNVHtSJqfsXlCm2Cwad4ZYPkl5fyamgOoVP/sUqf2w/pX4fRv6sZhRsb3dFNSFhWVepWjUej1BqZ98ztbk8/haKtj8dhedPJzVTssxvRqj7ujLcdziohPyFQ7HKsgCTQhhBBCCCGugZeLPTFV3d82trRZaL9/BXs+gYvy4apFyjwEGb+rHUXzy65KoHl3avZDb0k6R4XeQEcfZ0J9XJr9+DXs+Qh2vgeHv1c3DjOKCWqDh5MtuUXl7D9xQe1wMBgMzFydwI+/n8FGq2HhQz1N3ULV5mxvw8M3BAHw4ZbUltsIx4wkgSaEEEIIIcQ1iqtaxmnsrNdi7Hwf1kyD1M1qRyLMzZg8yjkG+lZWRNyYQPMKa/ZDG5d6G39nqMrYVTfpZ3XjMCMbnZZBVY0ZLKEb5382HeWznccBeGd0dwZGWNay3Uf7BmNno+XgyVz2Hlc/4WjpJIEmhBBCCCHENTIuG7Lk7m8NduE4nP1TqRPVaaja0Qhz8wgCnR1UlkLeSbWjaV7GJZze4c162PJKPb8cUZLsQ9RevgnQcTBobSHnKJxLVjsaszG+t/GJZ1WdVfXF7hO8E6+8rzNui+aOHoGqxVIXH1d77umpxPXx1mMqR2P5JIEmhBBCCCHENQrzdSHUgru/NUrSWuWyQx9w8lQ3FmF+Wh14hirXs4+qG0tzM77eZl7CuTftPPklFXg623GdJSzjc3CDkJuU60lr1I3FjPqH+2Cn03Iip4hj5wpUiWHNoQz+8cOfADw7KIyx/UJUieNqPN4/FI0GNiRmcTTrotrhWDRJoAkhhBBCCGEGxiVZltb9rdGMH6iNy7xEy2NcwmickdUaGAzQ5S4Iv6XZZ6DFVy3fHBTpi06radZj18m0jLPlJNBc7G3o09ELgPiE5l9Wv+NoNpO/OojBAA/07sCUIc3776yhOvq4mGbtfbI1TeVoLJsk0IQQQgghhDADS+z+1mjFuXBih3I9coSqoYgmZJyBld1ylu9dkUYDQ2bCAyvB2bvZDmswGEz1z4ZYQv0zo4iq/98n90DBOXVjMSPTFxrN3Njlj1O5PLF0H2WVekZ09efNO7qg0VhIsrQeTw5QZqOuOnCarPwSlaOxXJJAE0IIIYQQwgx6dvCgTVX3t30W0P3tmhzdAPoK8ImsXuYnWh7jDKzsVjQDTSXJZws4eb4YOxst/Ts1X+LuitwDIaA7YICUdWpHYzZxUUqx/t/SL3DuYmmzHPPYuQLGLtlLYVkl/cK8+L8xPSxnpuEVxAR5EhPUhrJKPUuqmh6Iy0kCTQghhBBCCDOw0Wm5uar7m9Uv4zzyk3Ipyzdbtva9YfBr0G+y2pE0n5N7lfp+FzOb9bDGmVA3hnnjZGfTrMe+op6PQO+J4N9N7UjMJsDdka6B7hgMmBo3NKXMvBIeWbSH84VldGvnzkcPx2Jvo2vy45rTkzcpX5Ys//UEBaUVKkdjmSSBJoQQQgghhJlYSve3axY5EiJGQtRtakcimpJXR+j/PHSKUzuS5rPnY1gxBg5+2ayHXV+VVI+zhO6bf9XrcRj+bwhoOQk0qH6v45t4GWduURkPL9rN6dxiQr2dWTK2Fy72FpYkvQpxUX6E+jhzsaSCr/akqx2ORZIEmhBCCCGEEGZykwV0fzOLrqPg/i8hMEbtSIQwL2PDhGbswJmVX8LvJ3MBGFy1tFA0vbho5b3elnKOkvLKJjlGUVkF4z/bS0pWAf5uDix97Hq8XOyb5FhNTavVMKG/Mgtt8fY0yq29lmcTkASaEEIIIYQQZuJsb0PfMKX723prX8YpWoejG2HjTDixU+1Imp7BANlHletezZdA21i1hLB7O3f83Bya7bgNkncKtr0D2+erHYnZRAe4EejhSEm5nh1Hs82+//JKPZO++I3f0nNxd7Rl6WPX066Nk9mP05zuui4Qbxd7zuSVsPqPM2qHY3EkgSaEEEIIIYQZGZcNWWUdNIMBVk+FP76BcunE1iok/KAkTo79onYkTe9iJpRdBI0WPEOa7bDG3wUW1X3zr7KTlUTqrg9A3zJmHmk0GlMzgXgz/z7W6w1M++Z3Niedw9FWx+KxvQj3czXrMdTgYKtjXL9gAD7akmrdpQiagCTQhBBCCCGEMCPjEq0DJ3Obrfub2Zz9E/Ytgh+fAUPL+BAtrsA4EyunFXTiNL5GjyCwaZ5ldkVlFWyvmv0UZ8kJtKAbwd4NCrPg9H61ozEb43u+ITELvd48ySCDwcDM1Qn89+AZbLQaFjzUk5igNmbZtyV4qHcQTnY6jmReZGuK+WfuWTNJoAkhhBBCCGFGzd39zayOrFEuOw4CO+teiiSukrEWmHFpY0uW3fz1z7anZFNaoaddG0ciLHmGko0dhFU1k0hao24sZtQ7xAsXexuyC0r5/VSuWfb5wS9H+WzncQDm3tudmyNaVl07dydbxvRqD8DHW4+pHI1lkQSaEEIIIYQQZmZcqtXU3d/MzvjBOWK4unGI5mOagXa0xSzdq1NO89c/i7+k+6ZGo2m24zZKxAjlsgUl0OxstAyI8AFggxl+H3+5O52565MBeP22aO68LvCa92mJHrsxBJ1Ww46jOfx5Ok/tcCyGJNCEEEIIIYQwM2MdtG0p5ygua5rub2aXdxoyDgIaCL9F7WhEc2kTBFpbqCiG/FNqR9O0yotBZwfeYc1yuEq9gU1Vs1Atuv6ZUac40NrAuSOQ03JmHg0x1aW8thnBPx/K4B8/HALgmUFhjOvXfHX0mlu7Nk7c2i0AgI+2pqocjeVoVAJtwYIFhISE4ODgQExMDNu2bat3fGlpKdOnTycoKAh7e3s6duzI4sWLTY9///33xMbG4uHhgbOzMz169GDZsmU19rFw4UK6deuGm5sbbm5u9OnTh59//rkx4QshhBBCtEoNPYfbsmULMTExODg4EBoayocffljj8cOHD3PPPfcQHByMRqNh/vz5l+3D+Nhft6eeeso0ZuzYsZc9fsMNN5jlNaslKsC1Sbu/NYnkqnPr9teDi4+6sYjmo7OtLqif3cLroN02H6ZnQo8Hm+VwB09eIKewDFcHG64P8WyWY14TxzYQ1Fe5nrxW3VjMaGCEDzqthqSzF0nPKWrUPnYezea5rw6iN8ADvTswdUi4maO0PE/cFArAmkMZnDzfuPetpWlwAm3lypVMnjyZ6dOnc+DAAfr378/w4cNJT0+v8zmjR49m48aNLFq0iKSkJFasWEFkZKTpcU9PT6ZPn86uXbv4448/GDduHOPGjWPdunWmMe3ateNf//oX+/btY9++fQwaNIg77riDw4cPN/QlCCGEEEK0Og09h0tLS2PEiBH079+fAwcO8Pe//51nn32W7777zjSmqKiI0NBQ/vWvf+Hv71/rfvbu3UtGRoZpi4+PB+Dee++tMe6WW26pMW7NGuteQnRp9zdzLBtqFkdk+WardekyzpZOq2u2BgLxVTOebo7wxVZnJYu/IkYql0es+3fwpTyc7Lg+WElgNmZZ/aFTeUxYuo+ySj3Du/jz5h1dLH85rhl0butO/07eVOoNLNqepnY4FkFjaGBf0t69e9OzZ08WLlxoui8qKoo777yT2bNnXzZ+7dq13HfffaSmpuLpefVZ9549ezJy5EjefPPNOsd4enry9ttv89hjj9X6eGlpKaWl1Z2P8vPzad++PXl5ebi5uV11LEIIIYRovfLz83F3d7f684eGnsO99NJL/PjjjyQmJprumzhxIr///ju7du26bHxwcDCTJ09m8uTJ9cYxefJkVq9eTUpKiukDyNixY8nNzeWHH35o3IvDMn9O21LO8fCiPXi72LPn74PRai34A1dJPswJBX05PLUXfFr+7ApxiW3vQMoGiB0P3e698nhrVFH1ubCZkmcAcfO2cDSrgPfuv47bu7dttuNekwsn4JdZEHUrRN2mdjRms2h7Gm+uTqBPqBcrnrj6Gc6p5wq498Nd5BSW0bejF0vG9cLeRteEkVoW498xR1sdO18eRBtnO7VDMruGnD80KA1eVlbG/v37GTp0aI37hw4dys6dO2t9zo8//khsbCxz5swhMDCQ8PBwpk2bRnFxca3jDQYDGzduJCkpiZtuuqnWMZWVlXz11VcUFhbSp0+fOuOdPXs27u7upq19+/ZX+UqFEEIIIVqOxpzD7dq167Lxw4YNY9++fZSXlzc6juXLlzN+/PjLvr3fvHkzvr6+hIeHM2HCBLKy6q9VU1paSn5+fo3N0vQO8cLVzN3fmo4B4l6Hrvc2a4dCYSH6Pw/jf265yTNQliT+0x9WPtQsh0vLLuRoVgE2Wg0Dwq1oSXSbILj7oxaVPANMM4L3HD9PXtHV/Q3LzCvh4UV7yCkso2ugOx8/EtuqkmcAN4Z5Ex3gRnF5Jct/PaF2OKprUAItOzubyspK/PxqFkD08/MjMzOz1uekpqayfft2/vzzT1atWsX8+fP59ttva9S9AMjLy8PFxQU7OztGjhzJ+++/z5AhQ2qMOXToEC4uLtjb2zNx4kRWrVpFdHR0nfG+8sor5OXlmbaTJ0825OUKIYQQQrQIjTmHy8zMrHV8RUUF2dmNq+n1ww8/kJuby9ixY2vcP3z4cL744gs2bdrEO++8w969exk0aFCNlQR/ZQ1flJq7+1uTcnCHvs/APZ9CK1iaJFqh7BQw6MHWuVkOt6Gq+2bvUE/cHW2b5ZiibkFezoT7uVCpN7A5+crNBHKLynhk8W5O5xYT6u3MknG9cLG3aYZILYtGozHVQvt813FKyq2kKU4TadRC7L9+Y2gwGOpcA6zX69FoNHzxxRdcf/31jBgxgnnz5vHZZ5/VmIXm6urKwYMH2bt3L//85z+ZOnUqmzdvrrGviIgIDh48yK+//srf/vY3Hn30URISEuqM097e3tR0wLgJIYQQQrRWDTmHq2t8bfdfrUWLFjF8+HDatq25lGnMmDGMHDmSLl26cNttt/Hzzz+TnJzMTz/9VOe+rOWLUmPnvfgEC0+gCVGYDSd2QnmJ2pE0DWODhGbqwGmstWXsyGtVKspg7yL46sHqpa8tgPFncaXfx8VllYz/bC/JZwvwc7Pn8/HX4+3SfEt/Lc3IbgEEejiSXVDG97+dVjscVTUogebt7Y1Op7vsm8qsrKzLvqE0CggIIDAwEHd3d9N9UVFRGAwGTp2qbpOs1WoJCwujR48ePP/884waNeqyehx2dnaEhYURGxvL7Nmz6d69O++++25DXoIQQgghRKvTmHM4f3//Wsfb2Njg5eXV4BhOnDjBhg0bePzxx684NiAggKCgIFJS6u4IaC1flA4M90Wn1ZB8toATOYVqh1O7k3th9RRI26p2JEJNH1wPS4ZDdpLakTSNHGMCrenr+10oLGPf8fOAlSbQtDawZQ4cWQ1p9XdrtibGLzS2JJ2jrEJf65jySj1/+2I/v6Xn4u5oy9LxvWnv6dScYVocW52W8TcqnXo/2ZZKpb5BZfRblAYl0Ozs7IiJiTF1TzKKj4+nb9++tT6nX79+nDlzhoKCAtN9ycnJaLVa2rVrV+exDAZDvdP2r3aMEEIIIURr15hzuD59+lw2fv369cTGxmJr2/DlSEuWLMHX15eRI0decWxOTg4nT54kICCgwcexNO5OtqbubxsSr7xsSBWHV8G+xXBwhdqRCDUZO3Fm1524tloGA2RXdRj1avoaf78kZaE3QKS/q3UmX7Ta6m68SS2nG2f3dh54u9hzsbSC3Wk5lz2u1xt44Zvf2Zx0DgdbLYvHxhLh76pCpJbnvl7tcXOwIS27sFXPqG7wEs6pU6fy6aefsnjxYhITE5kyZQrp6elMnDgRUKbTP/LII6bxDzzwAF5eXowbN46EhAS2bt3KCy+8wPjx43F0dASUGhbx8fGkpqZy5MgR5s2bx9KlS3nooeoCj3//+9/Ztm0bx48f59ChQ0yfPp3Nmzfz4IMPXut7IIQQQgjR4jX0HG7ixImcOHGCqVOnkpiYyOLFi1m0aBHTpk0zjSkrK+PgwYMcPHiQsrIyTp8+zcGDBzl69GiNY+v1epYsWcKjjz6KjU3NGjIFBQVMmzaNXbt2cfz4cTZv3sxtt92Gt7c3d911VxO+I83HOOthgyV+6DAYqj8gGz8wi9bJuLQx52j946xR4TkozQM04Bna5IczJhiM//etUsQI5TLpZ+X3RAug1WpMzQT++vvYYDDw5k8J/HDwDDZaDQsfiiEmyFONMC2Ss70ND/cJAuDjrcdUjkY9Da6CN2bMGHJycpg5cyYZGRl06dKFNWvWEBSkvJkZGRmkp6ebxru4uBAfH88zzzxDbGwsXl5ejB49mrfeess0prCwkEmTJnHq1CkcHR2JjIxk+fLljBkzxjTm7NmzPPzww2RkZODu7k63bt1Yu3btZY0GhBBCCCHE5Rp6DhcSEsKaNWuYMmUKH3zwAW3btuW9997jnnvuMY05c+YM1113nen23LlzmTt3LgMGDKhRy3bDhg2kp6czfvz4y+LS6XQcOnSIpUuXkpubS0BAADfffDMrV67E1bVlfPMfF+XHzNUJpu5v7k4WVFD83BG4kAY6e+g4SO1ohJpMM9CS1Y2jKRhn1Xl0AFuHJj1USXklW5LPAVa6fNMo5Cal4cLFM5BxENped8WnWIO4KD++2nuSDYlZzLi9ug7ogs3HWLLjOABz7+3OzRG+KkZpmR7tG8wnW9P4LT2XfcfPExvc+hKMGoOhhaSTr0J+fj7u7u7k5eVZbJ0MIYQQQlgWOX+wDpb+cxr2f1tJOnuR+WN6cOd1gWqHU23bO7BxJnQaCg9+o3Y0Qk1HfoKvHgD/bjCx5dS9AmDfElg9GcLi4KHvmvRQm5OyGLtkL76u9vz6ymC0WivuarvyIUj8H9z0IgyarnY0ZlFcVsl1b66npFzPmmf7E93WjRV70nnl+0MAvHZrtKnel7jcy9/9wVd7TxIX5cenj8aqHY5ZNOT8oVFdOIUQQgghhBBXLy5amc1g7MxnMY7I8k1RxTgDLedYi1myZxJ1Ozy8Cvo/3+SH2mDsvhntZ93JM4CIqpqVST+rG4cZOdrp6N/JB1CW2q79M4Ppq5Tk2VM3d5Tk2RU83l9ZAr0h8SxHswquMLrlkQSaEEIIIYQQTcy4lKu+7m/N7uJZOL1PuR4uCbRWr00waHRQXgj5Z9SOxrycvZQlykG1N00xF4PBwIYEpVnIEGtevmnUaShotHD2EOSmX3m8lTD+bFbsSefZFQfRG+D+69szbWiEypFZvjBfF1Ntv0+3paocTfNrcA00IYQQQgghRMN0b+eBj6s95y6WsjstxzQDQlXJa5XLtj3Bzfo7noprZGOnJNHOH1Pq4rk37VLjP0/nsf5wJi1prtvFkgoy80twtNXRp6OX2uFcO2cv6PcctAkBB3e1ozGbmyN90WggM78EgFs6+/PWnV1N9dBE/Z68KZT4hLN8/9tppg4Jx9etaesKWhJJoAkhhBBCCNHEjN3fVuw5yYaEs5aRQOvQB/pPU5ImQgA89C04+4B90zbwKK2oZNxnezl3sbRJjwNgRzmf2L5DqiGAf1XcTyl2TX7MAeE+ONjqmvw4zSJuhnJZWgAb3qh/bMyj1b9PDq+CjD/qHuvdCXo8oFwvyIJfF9a/7xsmgUvV780DX9TfLTagO3S+U7l+PhV+W1bjYR9gbpvTZOYXE+jhyC13z0VnXG6755P6Z2AG9YVOVY0MM/+EP69QUy/u9err2+dDSV7dY8PiILifcv3knvqXzto5wU0vVN/+ZRZUltc9PvoOaNtDuX7sF0jbWvdYZx/oM0m5XlEGm2fXeDgWeMfzFJn5JRz76md8735G+XmCUkvx1L66990mWPl3AlB0Hna+X/dYgF6PN3kyvyEkgSaEEEIIIUQziIvyY8Wek8QnnGXG7Z3Vn+3gEw6DX1U3BmFZPEOb5TA/HDjNuYuleLvYM7Krf5Mey6c4jQFH/uAGbQrHY1+FJv5/Z2ej5ZE+wU16DFWUF8P2efWPCRtcnUBLXge/r6h7bKeh1Qm0opwr77vbmOoE2uFVcDS+7rE9HqxOoOWdqnXf94CSDSkAmFX9wMEv4MyBuvetr6hOoGUnXznuSxNoez+FvJN1j3X0qE6gZfxe/76dvGsm0Ha8BxXFdY/37lSdQEvfVf++fSKrE2j68vrfvzNQfHY4jsYE2tGNsG9R3fsO7l+dQCvNv/L7F3WrJNCEEEIIIYRobfqFeeNgq+VMXgkJGfl0bttylkQJcbX0egMfbVVqJz15UygTbmripF1iGhwBe/9I3riza9MeqyWzdVBmgdXH9ZKl4B0Hg2Obusd6h1dfd2xz5X1fuq/IkdUznmoTGFN93S3wyvu2uWQJYpdRyuzculz6mFfHK+/7Utc9DCW5dT8e0KP6ul+X+vdt9//t3Xlc1VX+x/HX5bLvArKKiIK4b+C+VkqpbTOWVpOVrf5aZsypmWn8zUzb5Ew1Tb+mtCxbzBanaXPKUrTccs1cUUFERZHFC7Ij6/39cYUiEQWB7wXez8fjPr73fu/5nvO5XIVzP/csHnUfD7vHltw7n4CfrO/WZWjDdXsG/njfwbHeslarlY9/OEF+aSW+x52Y1vfsE5HjwNHl/HX/NEnv4n3hn59HYMPPtzKT1dretlg5P3vf3lxERETsj/oPbUNbeZ/uWfI9CfuzeHhiT34zsYEPgC1t7d+g8gwMua3VRh1JG2BJgY/vsk0Fu39TizSRsD+Le5Z8j5eLI5seuxwvV6cWaafWhhdgzRPQfzpMe71l2xLpQN7fmsYfP91LqI8r6353GU7mtrlHZWP6D23zFYqIiIiItEE1u7+tPpBlXBDVVbZ1fjb+E/IamE4kHY+bL2Tsguz9UF7SIk0sWn8YgFtGdG355Bn8uFZWQyOWRKTRfjkkjABPZ07mn+HLPRlGh9MqlEATEREREWkll/e27f62Nz2fjPwG1qtpSSe2Q4nFtqtexChjYhD75O4Prr6A1bYbZzPbcew024+exsls4s7Rkc1ef70sh2xH/6jWaU+kg3B1MnPHqG4AvLY+lY4wuVEJNBERERGRVhLg6cKQrra1fFYfyDYmiKQVtmN0PJhbYQSQtB0m049rU9UknppRzeiz6weFEeTteoHSzcBqtS30DnXX3BKRZnHriAjcnc0cyChgwyGL0eG0OCXQRERERERa0cSaaZz7DZrGefBsAi1msjHti32rmepYM/WxmaSeKmLV2X/z97b0xgE1SnLOLtpusi34LiLNytfdmelx4QAsOrs5SHumBJqIiIiISCua1Me2q9jmwzkUlTWwa1pLsByCnEPg4ARRE1u3bWkbaqY6NvMItDc2HsFqhct7BRId5NWsdZ+XtRpGPAADbwYnt9ZpU6SDuWtMJGYHExtTLOxLzzc6nBalBJqIiIiISCvq0dmTbv7ulFdVsyH5VOs2XjN9s9sY2xpoIj9XOwKt+RJopwrL+M+OEwDc11qjzwA8A+GqZ+AXC1uvTZEOJtzPnan9QwB4fUP7HoWmBJqIiIiISCsymUxM6mObxpnQ2rtxJn1lO/aa2rrtStvhfzaBZjlkW0OsGSzZfJTyymoGhvsyLNKvWeoUEftRMy37iz0ZnDjdMjv42gMl0EREREREWlnNOmjfHsymsqq69Roe/3sYdq/WP5Pz8+sOl82D6162TYG8RCXllby75RhgG31mMpkuuc6LdvBLSF4Fpadbr02RDqhfmA+jo/ypqrayeOMRo8NpMUqgiYiIiIi0stiITvi6O3G6pIIdx1rxw32Py2DKc+DTpfXalLbF0RnG/w76/gIczJdc3b+3HyevpIIIf3eu7BvcDAE2QsKf4f0bIWN367Yr0gHdN862Ucey7cfJKyk3OJqWoQSaiIiIiEgrczQ7cHmMbTOB1a09jVOklVRWVfPG2dEod4/tjtmhFUefVVXA6aO2+zXTUkWkxYyNDqB3iDcl5VUsPTvqtL1RAk1ERERExAATa9ZB25+FtZnWmjqvijPwzjWw8Z9QWdaybUnbd3InfPM07Hr/kqpZsS+TE6dL8fNw5sbYVh71ePooVFeCkwd4h7Zu2yIdkMlk4t5xkQC8vekYZyqqDI6o+SmBJiIiIiJigHE9O+NsduBoTgmHTxW3bGNH1sGR9bDtdTA7t2xb0vad3AXrn4N9Hze5CqvVyqL1hwG4bWQErk6XPh20USxndxH17wGtue6aSAd29YBQQn1csRSV8enOdKPDaXZKoImIiIiIGMDTxZGRPfyBVpjGmbTCdoyZrGSCXFhAT9uxJgnVBJsO57AvvQBXJwduG9mteeJqDEuy7VjzWkSkxTmZHbhzjG0U2uvrU6mubuHR1a1MCTQREREREYP8dBpni6muhqSvbPdjprRcO9J+BJxdMywvzTb9twleW58KwPS4cPw8DBj1mHM2+Reg9c9EWtNNw7ri5epIqqWYhHa2xqcSaCIiIiIiBpnY27aRwA9pp7EUtdDaZCd3QlEWOHtBtzEt04a0Lx6dwcUHsEJuaqMvP5BRwPrkUziY4O4x3Zs/vothSbEd/aOMaV+kg/J0cWTmiAgAFq1v/O8Pe6YEmoiIiIiIQUJ83OgX5o3VCt8czG6ZRpK+tB2jrgBHl5ZpQ9oXkwkCziaecho/jfP1sx+aJ/cPoau/e3NGdvFMJnBw1Ag0EQPcMaobzmYHdhw7zfdHc40Op9kogSYiIiIiYqBJvYMBWN1S0zhrpm/2mtoy9Uv7VLsOWnKjLjuZV8ry3ScBuG+cQaPPAO78GuZlQlA/42IQ6aACvV35xeAw4Mfp3O2BEmgiIiIiIgaa2Mc2jXPDIQtnKqqat/LcI5C9H0xmiJrYvHVL+1Yz9bFmKuRFenPjESqrrYzo7seALr7NH1djmJ3AoZV3/xQRAO4ZZ9tMYPWBLA6fKjI4muahBJqIiIiIiIH6hHgT6uNKaUUV36VYmrdyz0C44S247DFw92veuqV9Cx4A4cN/nMp5EfJLK/hgWxoA943r0VKRXVhZIVRVGNe+iBAV6MXE3kFYrfDGhvYxCk0JNBERERERA5lMptrdOFc3945lzh7Q75cw7tHmrVfav57xcNeqRv3beW/rMYrLq4gJ8mJCTOcWDO4C1j8Pfw2GtX8zLgYR4b7xtmncH/+QTnZh03b0tSdKoImIiIiIGGxi75oEWjbV1VaDoxFpvLLKKt767igA94zrjslkMi6YnBSorgS3TsbFICLERXRicFdfyiureWfTUaPDuWRKoImIiIiIGGxEd388XRw5VVjGnvT85qk08VN4bzrsX9489UnHU5ILaVuhMPOCRT/feZJThWUEe7ty7cDQVgiuAZazO4f6X/z0UxFpfiaTqXY699ItaRSXVRoc0aVRAk1ERERExGDOjg6MPzvlrdl249z/ORxaCRm7mqc+6Xg+uRfejP9xJ9fzqK62sujsGkd3jumGs6OBHzOrKiH37HpLNTuJiohhJvUJIjLAg/zSCpZtP250OJdECTQRERERETswqXczroNWWQaHVtvux0y59PqkYwqIth1zGt6J85uD2aRkF+Hl4sjNw7q2QmANyDsG1RXg6AbeYcbGIiKYHUzcPda2I+fijUeoqKo2OKKmUwJNRERERMQOTIjpjNnBxMHMQo7nllxaZUc3QnkheAZB6JDmCVA6npopkDVTIs9j0XrbiK9bhnfFy9WppaNq2E+nbzro466IPZg2pAv+Hs6k55WyYm+G0eE0WZN+oyxYsIDIyEhcXV2JjY1lw4YNDZYvKytj3rx5RERE4OLiQo8ePXjzzTdrn//kk0+Ii4vD19cXDw8PBg0axLvvvlunjvnz5zN06FC8vLwIDAzk+uuvJykpqSnhi4iIiIjYHV93Z4Z2sy16nnCp0ziTVtiOPa9SEkGarnYE2vkTaD+knWbb0VyczCZmjY5spcAaUBNrgNY/E7EXrk5m7hjVDYDX1qVitbbNzXIa/dd02bJlzJkzh3nz5rFz507Gjh3L5MmTSUtLO+8106dPZ82aNSxevJikpCQ++OADevXqVfu8n58f8+bNY/PmzezZs4dZs2Yxa9YsVq5cWVtm3bp1PPDAA2zZsoWEhAQqKyuJj4+nuLi4sS9BRERERMQuTWyOaZxW649rVvWa2gxRSYflfzaBdvqobVpwPRats40+u25QGME+rq0UWANqR6BFGxuHiNRx64gI3JzM7M8o4LuUHKPDaRKTtZGpv+HDhzNkyBAWLlxYe653795cf/31zJ8//5zyX3/9NTfddBOpqan4+flddDtDhgxh6tSpPPXUU/U+f+rUKQIDA1m3bh3jxo27qDoLCgrw8fEhPz8fb2/vi45FREREOi71H9qG9vI+HcspZvxzazE7mPjhfyfh496E6XAZu+G1ceDkDr9LBSe35g9UOgarFeaH26YD378VAnvVefqIpZjL/7EWqxVWPTyOnkFeBgX6EwUnIXs/+HSFztpEQMSePL48kbc3HWVsdADv3jXc6HCAxvUfGjUCrby8nB07dhAfH1/nfHx8PJs2bar3muXLlxMXF8ezzz5LWFgYPXv25JFHHqG0tLTe8larlTVr1pCUlNRgYiw/37a9d0NJubKyMgoKCurcRERERETsVYS/Bz2DPKmqtrI2ObtplRw8O32zx+VKnsmlMZl+nApZzzTONzakYrXCZTGd7SN5BuAdClETlTwTsUN3jYnE7GBiwyELiSfzjQ6n0RqVQLNYLFRVVREUFFTnfFBQEJmZmfVek5qaysaNG9m3bx+ffvopL774Iv/5z3944IEH6pTLz8/H09MTZ2dnpk6dyr/+9S8mTZpUb51Wq5W5c+cyZswY+vXrd95458+fj4+PT+0tPDy8MS9XRERERKTV1UzjbPI6aLF3wNX/hKF3N19Q0nHVTIXMO17ntKWojP/sOAHAfeN7tHZUItIGhfu5M6V/CACvn918pC1p0oqiJpOpzmOr1XrOuRrV1dWYTCbee+89hg0bxpQpU3jhhRd4++2364xC8/LyYteuXWzfvp2//vWvzJ07l7Vr19Zb54MPPsiePXv44IMPGozzscceIz8/v/Z2/PjxBsuLiIiIiBhtYh9bAm1d0inKK6sbX4F3CMTdCT0ua+bIpEOKfwr+kAYj769zesmmo5RVVjOwiw/DIy9+qZ4WlbEH3v0lrP270ZGIyHncN647AP/dk8GJ05e443Qra1QCLSAgALPZfM5os+zs7HNGpdUICQkhLCwMHx+f2nO9e/fGarVy4sSJHwNxcCAqKopBgwbx29/+lhtuuKHeNdUeeughli9fzrfffkuXLl0ajNfFxQVvb+86NxERERERezaoiy8Bni4UllWy7Uiu0eFIR+cVDK4+dU6VlFeyZMsxAO4d1+O8gylaXdY+OLwGjm4wOhIROY9+YT6M6uFPVbWVNzceNTqcRmlUAs3Z2ZnY2FgSEhLqnE9ISGDUqFH1XjN69GhOnjxJUVFR7bnk5GQcHBwaTIBZrVbKysrqPH7wwQf55JNP+Oabb4iMtIMtkkVEREREmpmDg4mJvQOBJuzG+Z874ZN74VRyC0QmHZrVarsBH31/grySCrr6uXNVv2CDA/uJmh04A7T+mYg9q5n2/eH2NPJLKgyO5uI1egrn3LlzeeONN3jzzTc5cOAADz/8MGlpacyePRuwTZu87bbbasvfcsst+Pv7M2vWLPbv38/69et59NFHufPOO3Fzsy1qOn/+fBISEkhNTeXgwYO88MILLFmyhFtvvbW2ngceeIClS5fy/vvv4+XlRWZmJpmZmefdjEBEREREpK366Tpo1rNJiws6kw/7l8OeZWBq0kotIueqroLXr4C/RUBJDpVV1byx0bZ20T1jbQuC242ajQ4Coo2NQ0QaNC46gF7BXpSUV7F06zGjw7lojf7LOmPGDF588UWefPJJBg0axPr161mxYgUREREAZGRkkJaWVlve09OThIQE8vLyiIuL41e/+hXXXHMNL730Um2Z4uJi7r//fvr27cuoUaP4z3/+w9KlS7n77h8XPl24cCH5+flMmDCBkJCQ2tuyZcsu5fWLiIiIdBgLFiwgMjISV1dXYmNj2bCh4WlO69atIzY2FldXV7p3786rr75a5/nExESmTZtGt27dMJlMvPjii+fU8fjjj2MymercgoPrjlixWq08/vjjhIaG4ubmxoQJE0hMTLzk19uWjY4KwNXJgfS8Ug5kFF7cRSlroLrCNvqmZudEkUvlYIaibCjLB8shvtqXyfHcUvw8nLkh1s42abOk2I7+SqCJ2DOTycS9Z9dCe+u7o5ypqDI4oovj2JSL7r//fu6///56n3v77bfPOderV69zpn3+1NNPP83TTz/dYJsX/c2biIiIiJxj2bJlzJkzhwULFjB69Ghee+01Jk+ezP79++nates55Y8cOcKUKVO45557WLp0Kd999x33338/nTt3Ztq0aQCUlJTQvXt3brzxRh5++OHztt23b19Wr15d+9hsNtd5/tlnn63dZKpnz548/fTTTJo0iaSkJLy8vJrpJ9C2uDmbGRPVmdUHslh9IIs+oRexlm/SCtsxZnLLBicdT0AU5KdhtSSzaJPtg+7MERG4OZsvcGErqq6C3MO2+0ogi9i9awaG8vzKJE7mn+GznencNOzcvoi90dhuERERkQ7ghRde4K677uLuu++md+/evPjii4SHh7Nw4cJ6y7/66qt07dqVF198kd69e3P33Xdz55138vzzz9eWGTp0KM899xw33XQTLi4u523b0dGR4ODg2lvnzp1rn7Narbz44ovMmzePX/7yl/Tr14933nmHkpIS3n///eb7AbRBk/o0Yh20qgo4tMp2P2ZKC0YlHdLZNcVOHt7D3vR8XJ0cuG1khMFB/UzeMagqB7ML+NjZyDgROYeT2YE7x9jWtl+0IZXqavsfNKUEmoiIiEg7V15ezo4dO4iPj69zPj4+nk2bNtV7zebNm88pf+WVV/L9999TUdG4BX8PHTpEaGgokZGR3HTTTaSmptY+d+TIETIzM+u05eLiwvjx488bG0BZWRkFBQV1bu3N5b2CMJlgz4l8MvPPNFz42CbbGmjuAdBlaOsEKB2Hv21EV/aRfQDcGBuOv+f5k+aGqJ2+GWWbdioidu+mYV3xcnUk9VRx4zfNMYASaCIiIiLtnMVioaqqiqCgoDrng4KCyMzMrPeazMzMestXVlZisVguuu3hw4ezZMkSVq5cyeuvv05mZiajRo0iJyentp2aui82NrBtQuXj41N7Cw9vfyNOOnu5MDjcF4A1By/wwSLpK9ux51VKHkjzO7sov3fxMRxMcPfYSIMDqod3CAz/H+j3C6MjEZGL5OniyK+G20azLlqfeoHSxlMCTURERKSDMJnq7pZntVrPOXeh8vWdb8jkyZOZNm0a/fv3Z+LEiXz55ZcAvPPOO5cU22OPPUZ+fn7t7fjx4xcdU1sysY8tsbh6fwMJNKtV659Jyzq7KH9XUzZT+3Ymwt/D4IDqEdwfJv8Nxj1qdCQi0gizRnfD2ezA98dOs+PYaaPDaZASaCIiIiLtXEBAAGaz+ZwRXdnZ2eeM/KoRHBxcb3lHR0f8/f2bHIuHhwf9+/fn0KFDte0AjYoNbNM8vb2969zao0m9bT+D7w7nUFxWef6Cv/oIrvgL9LislSKTjiTD2oliqwtOpioeGKiPkCLSfIK8Xbl+cCgAi9YfNjiahum3n4iIiEg75+zsTGxs7Dm7oickJDBq1Kh6rxk5cuQ55VetWkVcXBxOTk5NjqWsrIwDBw4QEhICQGRkJMHBwXXaKi8vZ926deeNrSOJCvQkwt+d8spqNhw6VX8hkwk6x8DYueBshyODpM1787uj/LXyVl70fYxeUXa6w+X3b0HKaqi4wHqBImJ37h3XHYBV+7NIPVVkcDTnpwSaiIiISAcwd+5c3njjDd58800OHDjAww8/TFpaGrNnzwZsUyJvu+222vKzZ8/m2LFjzJ07lwMHDvDmm2+yePFiHnnkkdoy5eXl7Nq1i127dlFeXk56ejq7du0iJSWltswjjzzCunXrOHLkCFu3buWGG26goKCA22+/HbBN3ZwzZw7PPPMMn376Kfv27eOOO+7A3d2dW265pZV+OvbLZDLVjkJL2J9dfyGr/e9cJm1XwZkKPth2nPerrmDAVbPAzdfokM51pgC+mANLp0FVmdHRiEgjRQV6MbF3IFYrvL7hiNHhnJej0QGIiIiISMubMWMGOTk5PPnkk2RkZNCvXz9WrFhBRIRt8d6MjAzS0tJqy0dGRrJixQoefvhhXnnlFUJDQ3nppZeYNm1abZmTJ08yePDg2sfPP/88zz//POPHj2ft2rUAnDhxgptvvhmLxULnzp0ZMWIEW7ZsqW0X4He/+x2lpaXcf//9nD59muHDh7Nq1Sq8vLxa+KfSNkzsE8QbG4/wzcEsqqqtmB1+sjZcQQa8GQ8xU+HKZ8BB349L83p/axpFZZX0DPJkQs9Ao8OpX45tSjieQeDqY2wsItIk947rweoD2Xz8wwnmTupJZy872+kXMFmtHecrq4KCAnx8fMjPz2+362SIiIhI81L/oW1oz+9TZVU1sU+vJr+0go9mj2RoN78fn/z+TfjiYegyFO5ebVyQ0i6VVVYx9u/fkl1YxstTO3N15RrACpf90ejQ6tq9DD69FyLGwKwvjY5GRJrAarXyiwWb2HU8jwcvi+KRK2Napd3G9B/0FZWIiIiIiB1zNDtweS/byJ+En+/GmfSV7ajdN6UFfL7rJNmFZQR5u3BlpDOs+xtsf8PosM5VMwItwE7XZxORCzKZTNx3di20d7cca3jjHIMogSYiIiIiYucmnl0HbfVPE2hlRZC6znY/ZooBUUl7Vl1t5fX1qQDcOToSp8Bo2xMlOVCSa2Bk9bCcTaD5Rxsbh4hckvi+wXTzdye/tIJ/f3/c6HDOoQSaiIiIiIidGx/TGWezA6mWYg7X7FB2+BvbgumdIqFzL2MDlHZnbXI2h7KL8HRx5ObhXW07vHp3sT1Zk7CyFzXxBPQ0Ng4RuSRmBxN3j7WNQntjwxEqq6oNjqgubSJgsOpqK1VWK1XVVqprjtWcc672ZrXWvaaesufWCVXV1bZjzfU/q+vHc3Vjqqq2EuHvzpV9g3F1Mhv94xIRERHpkDxdHBnRw5/1yadYvT+LHuM9fzJ9cwqYTA1XINJIr66zjT67ZXhXvF2dbCcDoqDghG3KZNfhBkb3E9XVkHvYdl9TOEXavBtiu/DPhGTS80r5cm8G1w0KMzqkWkqgNZOPd5zg5W9TapNO1fUkwKqt1EmEVVW3nf0bfN2duDG2C7cMjyAywMPocEREREQ6nEm9A20JtANZ3DcmApK/tj2h9c+kme06nse2I7k4OpiYNbrbj0/4R0PqWvsagVaaC95hUJgJvhEXLi8ids3Vyczto7rxQkIyi9ancu3AUEx28iWREmjNpPBMBUcsxc1er9nBhNlkwsGBs0cTZgcTjg4mHEy2+zVH233qOWeqW4+DCbODA+aflXU4W+bHc2DCxMYUC+l5pby+4QivbzjC6Ch/bh0ewcQ+QTiZNQtYREREpDVc0TuIP32eyI5jp8k/9B0+pbng6gtdRxodmrQzi9bbRnRdNyiMEB+3H58IOLvGWE6KAVGdh0cA/PoHqKoEB82YEWkPZo6IYOHawySeLGDT4RxGRwUYHRKgBFqzmdw/hL5hPvUnq0w/S079LAH207J1z9lHlrWq2sq65Gze25LGN0nZfJeSw3cpOQR6uXDT0HBuGtaVUF+3C1ckIiIiIk0W6utG31BvEk8WsKowghvvSoD842BWl16az1FLMV/tywTg3rM74tWqSaBZkls5qoug/wci7UYnD2emx3Xhnc3HeHXdYSXQ2psgb1eCvF2NDqNFmB1MXN4riMt7BXHidAkfbjvOh9uPk11YxkvfpPDytylc3iuQX42IYFx0Z8x2kvgTERERaW8m9g4i8WQBqw9auHHoMAgfZnRI0s68sTEVqxUmxHQmJtir7pMBMRAWB0F9jAmuPiW54OKtBJpIO3P32O68u+UYGw5Z2H+ygD6h3kaHpF04pXG6dHLnkStj2PSHy3nlliGM7O5PtRVWH8hm1lvbGf/ctyxYm4KlqMzoUEVERETanUl9gjBRzfpkC2cqqowOR9qZnKIyPvr+BAD3jetxbgGfMLhnDVz7r1aOrAEf3Q5/DYb9y42ORESaUbifO1P6hwDw+oZUg6OxUQJNmsTZ0YGpA0L44N4RrJ47nrvGROLj5sSJ06U8+3USI+ev4aEPdrIlNQerte1sliAiIiJiz/qGevOIx9d8bPodqWveNDocaWeWbD5GWWU1A7r4MKK7n9HhXBxLClRXgFeI0ZGISDOrSeT/d/dJ0vNKDY5GUzilGUQFevKnq/vw6JUxfLEng6VbjrHreB7/3X2S/+4+SVSgJ78a3pVfDumCj5uT0eGKiIiItFkmk4mrXXcTUXWMz45nYEcT6aSNKy2vYsnmo4Bt7bPz7np3psC2BpqzJwT2ar0A61NWBIUnbfcDooyNRUSaXf8uPozs7o+DAxSXVRodjhJo0nxcnczcENuFG2K7sC89n/e2pvH5rnRSsot44r/7+fvXB7lmQCi3johgYLiv0eGKiIiItD1F2XQt3gfAG1kxXFtttZuNp6Rt+2jHcU6XVBDu58ZVfYPPX3Dzy7Du7zDkNuOnctbsBuoeAG6djI1FRFrEm3cMxc3ZPnbY1RROaRH9wnyY/8v+bP3jFTx1XV9igrw4U1HNRztOcN0r33HNvzby4bY0SsqNzyKLiIiItBnJKzFhJdHanX1FnuxNzzc6ImkHKquqeWPDEQDuGdsdR3MDHxP9a3biTGmFyC7Acsh2DOhpbBwi0mLsJXkGSqBJC/NydWLmyG58PWcsH//PSH4xOAxnRwf2pufzh0/2Mvyva/jL5/tIyiw0OlQRERER+5f0FQBH/McBsPpAlpHRSDvxdWImabkldHJ34sbY8IYL10yVzDnU8oFdSE0Mmr4pIq1ACTRpFSaTidgIP/45YxBbHruCeVN6083fncKySt7ZfIwrX1zPja9u4vNd6ZRVakcpERERkXOUl8DhbwBwG3ANAAn7lUCTS2O1Wlm03rbD3cyR3S482sP/bLKq+BSUnm7h6C6gZgRazag4EZEWpASatDo/D2fuGdedb347gaV3DeeqvsGYHUxsP3qa33y4i5Hzv2H+Vwc4llNsdKgiIiIi9uPIOqgsBZ9wYoeOxexg4mBmIcdzS4yOTNqwLam57DmRj4ujA7ePjLjwBS5eP+54afQ0ztoRaEqgiUjLUwJNDOPgYGJMdACvzoxl0x8u5+GJPQnxcSW3uJzX1qUy/rm1zFy8lZWJmVRWVRsdroiIiIixklfajjGT8fVwIS7Ctmi6pnHKpVi0/jAAN8Z1wd/T5eIuqklYGT2Nc8ZSuOUjCB9ubBwi0iFoF06xC0HervxmYjQPXNaDb5NOsXTLMdYfOsWGQxY2HLIQ7O3KTcPCuWloV4J9XI0OV0RERKT1XfU3iJkCvrY1qib1CWLrkVxWH8hi1uhIg4OTtigps5Bvk05hMsHdY7pf/IX+0XBk/Y9TKI3SqZvtJiLSCjQCTeyKo9mBSX2CeOfOYax75DJmj++Bv4czmQVneHH1IUb//Rvue/d71ieforraanS4IiIiIq3HyRV6xkNgb8CWQAPYmppLfmmFkZFJG1Wz9tlVfYPpFuBx8RfWjEArzGyBqERE7JMSaGK3uvq784fJvdj02OX8302DGBbpR1W1lZWJWdz25jYu+8daXlt3mJyiMqNDFREREWlZVecmyCL8PYgO9KSy2sq65FMGBCVtWUZ+Kct3pwNw77hGjD4DGHQL/P4o/GJh8wd2sfZ8BO/dCDuXGheDiHQoSqCJ3XNxNHPdoDD+fd9IEh4exx2juuHl4sixnBLmf3WQkfO/Yc6HO9l+NBerVaPSREREpB1aNAHemnrOlLmJZ0ehrdZunNJIb313lIoqK8Mi/RjctVPjLnb1AbdGXtPcTmyDQ6vAkmxsHCLSYSiBJm1KdJAXj1/bl63zruDv0/ozoIsP5VXVfLbrJDe+upmrXtzAks1HKTijaQwiIiLSTuQchqx9cHwLeHSu89TE3rYE2rdJ2VRo0yW5SAVnKnh/axoA9zV29NlPWa1QbdC/u5pksr924BSR1qEEmrRJ7s6OzBjaleUPjmH5g6OZEReOq5MDSVmF/PnzREY8s4bHPtnDvvR8o0MVERERuTRJX9mOEaPBzbfOU4PCfQnwdKbwTCXbjuS2fmzSJn2wNY2iskqiAz25LCawaZV8ch/8vRukrG7W2C5aTQItQAk0EWkdSqBJmzegiy9/v2EAW/84kcev6UN0oCcl5VV8sO04V/9rI9e9vJF/f3+c0vIqo0MVERERabyaBFrMlHOeMjuYuKKXbRRagqZxykUor6zmze+OAHDPuO44OJiaVlFlKZzJgxwDduIsL4aCE7b7AT1bv30R6ZCalEBbsGABkZGRuLq6Ehsby4YNGxosX1ZWxrx584iIiMDFxYUePXrw5ptv1j7/ySefEBcXh6+vLx4eHgwaNIh33323Th3r16/nmmuuITQ0FJPJxGeffdaU0KUd83Fz4o7Rkax6eBzL7h3BtQNDcTKb2H0in9/9Zw/Dn1nNE/9NJCW70OhQRURERC5OSS6kbbLdj5lcb5HaddAOZGk9WLmgz3elk1VQRqCXC9cNCm16RTVTJy0GJNByDtuObn7g7tf67YtIh+TY2AuWLVvGnDlzWLBgAaNHj+a1115j8uTJ7N+/n65du9Z7zfTp08nKymLx4sVERUWRnZ1NZWVl7fN+fn7MmzePXr164ezszBdffMGsWbMIDAzkyiuvBKC4uJiBAwcya9Yspk2b1sSXKx2ByWRieHd/hnf3x1LUh4++P8H7245xPLeUt747ylvfHWV4pB/TYrtwVb9gvF2djA5ZREREpH6HVoG1GoL6QaeIeouMiQrAxdGBE6dLScoqpFewdysHKW2F1Wrl9Q2pAMwaHYmLo7nplQUYmUDT9E0RaX2NTqC98MIL3HXXXdx9990AvPjii6xcuZKFCxcyf/78c8p//fXXrFu3jtTUVPz8bN8OdOvWrU6ZCRMm1Hn8m9/8hnfeeYeNGzfWJtAmT57M5Mn1f+t2PmVlZZSVldU+LigoaNT10vYFeLrwPxN6cN+47qw/dIr3tqax5kAWW4/ksvVILn/6bB8Tewdx/eAwxvfsjLOjZjWLiIiIHUlaYTueZ/QZgJuzmbHRAaw+kE1CYpYSaHJea5NOkZxVhKeLI7cMr3/ww0WrSV4ZMYVTGwiIiAEalS0oLy9nx44dxMfH1zkfHx/Ppk2b6r1m+fLlxMXF8eyzzxIWFkbPnj155JFHKC0trbe81WplzZo1JCUlMW7cuMaEd4758+fj4+NTewsPD7+k+qTtcnAwMSEmkNdvi2Pj7y/n0StjiAr0pKyymi/3ZnDPku8Z9sxq5n26l+1Hc6mu1vQHERERMVhVJaSutd2vZ/2zn6rZjXP1Aa2DJuf36jrb1Mebh4Xj43aJszBqkldFWXCmlQcqdBkKw+6FqCtat10R6dAaNQLNYrFQVVVFUFBQnfNBQUFkZmbWe01qaiobN27E1dWVTz/9FIvFwv33309ubm6dddDy8/MJCwujrKwMs9nMggULmDRpUhNe0o8ee+wx5s6dW/u4oKBASTQh1NeNBy6L4v4JPUg8WcBnO9NZvvsk2YVlvLc1jfe2ptGlkxvXDQrlF4PDiAr0MjpkERER6YjMjvDrXXD4GwgZ1GDRK3oHYTLtZfeJfLIKzhDk7doqIUrbsft4HluP5OLoYGLW6MhLr9DVGzyDbAm0nEMQFnvpdV6sqCuUPBORVtfoKZxgW2Pqp6xW6znnalRXV2MymXjvvffw8fEBbNNAb7jhBl555RXc3NwA8PLyYteuXRQVFbFmzRrmzp1L9+7dz5ne2RguLi64uLg0+Xpp30wmE/3CfOgX5sNjU3qz+XAOn+1K5+t9mZw4Xcor3x7mlW8P0zfUm18MDuOagaHqjIqIiEjrcveD/jdcsFhnLxcGhfuyMy2PNQeyL316nrQ7i9bb1j67dlAoob5uzVOpf7QtgWZJad0EmoiIARqVQAsICMBsNp8z2iw7O/ucUWk1QkJCCAsLq02eAfTu3Rur1cqJEyeIjrYN/XVwcCAqKgqAQYMGceDAAebPn39JCTSRi2V2MDEmOoAx0QE8fX0/Vh/I4rOd6axNOkXiyQISTxbw1xUHGN0jgOsGhXJVv2C8tPmAiIiItJTqaqgqA6eLT3RM7B3EzrQ8Vh/IUgJN6jiWU8xX+zIAuHdc9+areNSDEHsHRIxqvjovpCQX9v4HOsdA9/Gt166IdHiNWgPN2dmZ2NhYEhIS6pxPSEhg1Kj6f2mOHj2akydPUlRUVHsuOTkZBwcHunTpct62rFZrnQ0ARFqLq5OZqweE8sbtQ9k2byJPXd+PuIhOWK2wMcXCo//ZQ9zTq3ng/R9YvT+L8spqo0MWERGR9iZjFzzbHT6576IvmdTH9oX2xhQLJeWVFygtHckbG45QbYXxPTs37yYTMZNhwI3gE9Z8dV5I5l746lH44uHWa1NEhCZM4Zw7dy4zZ84kLi6OkSNHsmjRItLS0pg9ezZgW3csPT2dJUuWAHDLLbfw1FNPMWvWLJ544gksFguPPvood955Z+30zfnz5xMXF0ePHj0oLy9nxYoVLFmyhIULF9a2W1RUREpKSu3jI0eOsGvXLvz8/OjaVd+wScvw83Bm5ogIZo6I4HhuCZ/vSufTnekcPlXMl3sy+HJPBr7uTlw9IITrB4URG9HpvNOZRURERC5a0gqoKLHdLlJ0oCdd/dxJyy1hfbKFq/oFt2CA0lbkFpfz0Y7jANzXnKPPjGJJth0Dehobh4h0OI1OoM2YMYOcnByefPJJMjIy6NevHytWrCAiIgKAjIwM0tLSast7enqSkJDAQw89RFxcHP7+/kyfPp2nn366tkxxcTH3338/J06cwM3NjV69erF06VJmzJhRW+b777/nsssuq31csznA7bffzttvv93oFy7SWOF+7jx4eTQPXBZF4skCPj27+cCpwjKWbklj6ZY0wv3cuG5gGNcPDiMq0NPokEVERKStSvrKduw19aIvMZlMTOoTxOKNR1h9IEsJNAFgyeajnKmopn+YDyN7+Ddv5eUlsOlfkJsK1y8Eh0ZNcGqanLODKgKiWr4tEZGfMFmtVqvRQbSWgoICfHx8yM/Px9u7GYcuS4dVVW1l02ELn+08ydf7Migur6p9rl+YN9cPCuPagaEEavMBEZE2S/2HtqFdvU+nj8L/DQSTGR5NsW0kcJE2H87h5te34OfhzPZ5EzE7aGR8R1ZaXsXov39DbnE5/7p5MNcMDG3eBqqr4K/BUFUOv9kDnSKat/76vPtLOLwGrnkJYm9v+fZEpF1rTP+hSbtwioiN2cHE2OjOjI3uXGfzgXXJp9iXXsC+9AKeWXGA0VEBXDcojKv6BePpov92IiIi0oCkr23HriMblTwDiOvWCR83J3KLy9mZdpq4bo27XtqX/+w4Tm5xOV06uTG5JUYkOpjBrzucOgg5h1ongZZzyHYMiG75tkREfqIVxtiKdAxuzmauGRjK4jvObj5wXV9iIzpRbYUNhyw88tFu4p5O4MH3f2DNgSwqqrT5gIiIiNQjaYXtGDO50Zc6mR24LKYzAAkHspozKmljqqqtvLHxCAD3jO2Oo7mFPvrVJLIsh1qm/p+qKIU823pu+CuBJiKtS0NhRFqAn4czM0d2Y+bIbqTlnN18YFc6qaeK+WJPBl/syaCTuxNXDwjl+sGhDOmqzQdEREQEKM2DY9/Z7vea0qQqJvYJ4rNdJ0nYn8Vjk3s3X2zSpqxMzORYTgm+7k7cGNel5Rryb8UEWs5hwAquvuAR0PLtiYj8hBJoIi2sq787D10RzYOXR7Ev/cfNByxFZby75RjvbjlGVz93rhsUynWDtPmAiIhIh5a2BaoroXMv29S4JhjfszNOZhOpp4o5fKqIHp3Vt+horFYrr607DMBtIyJwd27Bj301I9ByWiGBVlYInSLBKxj05bOItDIl0ERaiclkon8XH/p38eGPU3qx6XAOn+1KZ+W+TNJyS/jXNyn865sUBnTx4bpBYVwzMIRAL20+ICIi0qHEXAVz9kFhRpOr8HJ1YkR3fzYcsrDmQJYSaB3Q1iO57D6Rj4ujA7eN6tayjdWOQEtp2XYAIkbCb3ZBtZZCEZHWpzXQRAzgaHZgXM/OvDB9EN//7yReunkwl/cKxNHBxJ4T+Tz1xX5GPLOGmYu38vGOExSVVRodsoiItAMLFiwgMjISV1dXYmNj2bBhQ4Pl161bR2xsLK6urnTv3p1XX321zvOJiYlMmzaNbt26YTKZePHFF8+pY/78+QwdOhQvLy8CAwO5/vrrSUpKqlPmjjvuwGQy1bmNGDHikl9vm+UbDuHDLqmKSX2CAFi9P7s5IpI2ZtH6VABuiO1CgKdLyzYWEGU7Fp60jRBrDQ76GCsirU+/eUQM5uZs5tqBobx5x1C2/vEKnryuL0O6+tZuPvDbs5sPPPTBTr45qM0HRESkaZYtW8acOXOYN28eO3fuZOzYsUyePJm0tLR6yx85coQpU6YwduxYdu7cyR//+Ed+/etf8/HHH9eWKSkpoXv37vztb38jOLj+Hf7WrVvHAw88wJYtW0hISKCyspL4+HiKi4vrlLvqqqvIyMiova1YsaL5XnxbcaYAqquapaoretsSaN8fyyW3uLxZ6pS2ITmrkG8OZmMywd1jmzYNuFHcOkGPy2HATbZF/ltSwclm+z8iItJYJqvVajU6iNZSUFCAj48P+fn5eHt7Gx2OSIOO5RTz+a6TfLYznVTLjx8y/DycuXpACNcNCmNIV19tPiAi0sLaS/9h+PDhDBkyhIULF9ae6927N9dffz3z588/p/zvf/97li9fzoEDB2rPzZ49m927d7N58+Zzynfr1o05c+YwZ86cBuM4deoUgYGBrFu3jnHjxgG2EWh5eXl89tlnTXtxtJP3acWjsO9jmPQkDL71kqub8n8b2J9RwPM3DuSG2BZcRF7syiMf7eY/O05wVd9gXp0Za3Q4zcdqhfnhUFUO928G/x5GRyQi7UBj+g8agSZipyL8Pfj1FdGs+e14lj84mlmjuxHg6UJucTlLNh9j2sJNTHh+LS8kJJN6qsjocEVExI6Vl5ezY8cO4uPj65yPj49n06ZN9V6zefPmc8pfeeWVfP/991RUVDQ5lvz8fAD8/PzqnF+7di2BgYH07NmTe+65h+zshqcelpWVUVBQUOfWplmtkPQVlOSAu3+zVDmxdhpnVrPUJ/YvM/8Mn+9KB+C+8a0w+qw1FWVBeSFUV4CPEsIi0vqUQBOxcyaTiQFdfPnLNX3Z8tjlvHPnMH45OAx3ZzPHckp4ac0hLv/HOq59eSNvbEglPa+Fh86LiEibY7FYqKqqIigoqM75oKAgMjMz670mMzOz3vKVlZVYLJYmxWG1Wpk7dy5jxoyhX79+tecnT57Me++9xzfffMM//vEPtm/fzuWXX05ZWdl565o/fz4+Pj61t/Dw8CbFZDey9kH+cXB0g8jxzVJl/NkE2vpDpzhToWlvHcFbm45QUWVlWDc/Bnft1HoNl5dA+g44Vn9CvllYzu7y6RsBji28rpuISD20C6dIG+JodmB8z86M79mZp8srSdifxWc701l/yMKeE/nsOZHP018eYGC4L1P7BzO5Xwjhfu5Ghy0iInbi59P+rVZrg0sB1Fe+vvMX68EHH2TPnj1s3LixzvkZM2bU3u/Xrx9xcXFERETw5Zdf8stf/rLeuh577DHmzp1b+7igoKBtJ9EOnl3zrcfl4Nw8f7v7hnoT7O1KZsEZNqfmcFlMYLPUK/ap8EwF72+xrWl477hWHn12ZB18cBME9Yf/2Xjh8k2RczaBFhDdMvWLiFyAEmgibZS7syPXDQrjukFhWIrKWLE3gy/3ZLDtaC67j+ex+3gez6w4yIAuPkzpH8KUfiF09VcyTUSkIwoICMBsNp8z2iw7O/ucUWY1goOD6y3v6OiIv3/jpxg+9NBDLF++nPXr19OlS8PTr0JCQoiIiODQoUPnLePi4oKLSzsahZJ0NoEWM7nZqjSZTEzsE8jSLWms3p+lBFo798G2NArLKunR2YPLe7Xyex3Q03bMSYHq6pbZJbNmBJq/EmgiYgxN4RRpBwI8XbhtZDeW3TeSrX+8gqeu78fI7v44mGDPiXz+9tVBxj33LVf/awML1qZwLKf4wpWKiEi74ezsTGxsLAkJCXXOJyQkMGrUqHqvGTly5DnlV61aRVxcHE5OThfdttVq5cEHH+STTz7hm2++ITIy8oLX5OTkcPz4cUJCQi66nTYtPx0ydgEm6HlVs1Y98exunKsPZFFd3WH2DutwyiureXPjUQDuG9cDB4dW3mTKNwIcnKCyFArSW6YNi0agiYixNAJNpJ0J9HJl5ogIZo6I4FRhGSsTM/lqXwabD+ewL72AfekFPPt1En1DvW0j0/qHEBngYXTYIiLSwubOncvMmTOJi4tj5MiRLFq0iLS0NGbPng3YpkSmp6ezZMkSwLbj5ssvv8zcuXO555572Lx5M4sXL+aDDz6orbO8vJz9+/fX3k9PT2fXrl14enoSFRUFwAMPPMD777/P559/jpeXV+2oNh8fH9zc3CgqKuLxxx9n2rRphISEcPToUf74xz8SEBDAL37xi9b8ERkn+SvbMXwYeHZu1qpH9vDHw9lMVkEZ+07mM6CLb7PWL/bhv7tPkllwhkAvF64bHNr6AZgdwS8SLMm2qZa+LTCdWlM4RcRgSqCJtGOdvVy4dUQEt46IIKeojJWJWXy1L4NNh3NIPFlA4skCnluZRO8Qb6b0C2bKgBB6dPY0OmwREWkBM2bMICcnhyeffJKMjAz69evHihUriIiIACAjI4O0tLTa8pGRkaxYsYKHH36YV155hdDQUF566SWmTZtWW+bkyZMMHjy49vHzzz/P888/z/jx41m7di0ACxcuBGDChAl14nnrrbe44447MJvN7N27lyVLlpCXl0dISAiXXXYZy5Ytw8vLq4V+Gnbm+HbbsRmnb9ZwcTQzPqYzK/Zmsnp/lhJo7ZDVamXR+lQA7hjdDRdHszGB+EfbEmiWFNtafs2psgzy0n5sR0TEACZrzWqwHUBBQQE+Pj7k5+fj7e1tdDgihsktLmdVYiYr9mWyKcVC5U+mdPQK9mJyvxCmDggmKrCDfHAREWmA+g9tQ5t+n6xWyEoEj87gVf+adJfikx9OMPffu+kd4s1Xvxnb7PWLsb5NymbWW9vxcDaz6bEr8HG7+CnWzSrhz/Dd/8HQe2Dq881bd3U1nD5im8bZ80po4kYmIiI/15j+g0agiXRAfh7O3DSsKzcN68rp4nIS9mexYl8GGw9ZOJhZyMHMQv65OpmeQZ5nk2kh9AxSMk1ERKRFmEwQ3K/Fqr8sJhAHExzIKODE6RK6dNKmQu3JonW20Wc3D+tqXPIMfrKRwPk3/2gyBwfw72G7iYgYRAk0kQ6uk4cz04eGM31oOPklFazan8mKvRlsTLGQnFVEctYh/m/NIaICPc+umRZMTJAXJn3zJyIicukKM8EzqEVH1HTycCaumx/bjuSy5kA2t4/q1mJtSevacyKPzak5ODqYuHPMhTfoaFE1UyuLc4yNQ0SkhSiBJiK1fNyduDEunBvjwskvrWD1/ixW7M1gwyELKdlFvLTmEC+tOUT3zh5M7R/C5H4h9A5RMk1ERKRJqirhleHg1glu+xw6RbRYU5N6B7HtSC4J+7OUQGtHXju79tk1A0MJ9XUzNpjQwfBoKnj4N3/dq5+A7AMwYjZ0n9D89YuIXAQl0ESkXj5uTkyL7cK02C4UnKlgzYEsvtyTyfpDp0g9Vcy/vknhX9+kEBngwZT+wUzuF0LfUG8l00RERC7W8S1wJg9MDuAd1qJNTewTxF9XHGBLag4FZyrwdjVwqp80i7ScEr7amwHAveO6GxwN4OgMji2QPAM4sh7Sv4eBN7VM/SIiF0EJNBG5IG9XJ34xuAu/GNyFwjMVfHMwmy/3ZLA2+RRHLMW88u1hXvn2MN383ZncP4Sp/ZVMExERuaCDK2zHnleBuWW75ZEBHkQFepKSXcS6pFNcMzC0RduTlrd4YyrVVhjXszO9Q+xs44zqKnBopt1ArdYf11UL0A6cImIcJdBEpFG8XJ24blAY1w0Ko6iskm8OZrNiTwbfJmVzNKeEhWsPs3DtYbr6uTO5fzBT+4fQP8xHyTQREZGfsloh6WwCLWZyqzQ5sXcQKdlFrD6QpQRaG5dbXM6y748DcJ89jD6rseEfsPkVGHYvTPhD89RZfArO5AMm8LOj1yoiHY4SaCLSZJ4ujlw7MJRrB4ZSXFbJt0nZrNibwTcHs0nLLeG1dam8ti6VLp3czm5AEMLALkqmiYiIcOognD4CZhfocXmrNDmpTyCvrjvMtwezqaiqxsns0CrtSvN7d/MxzlRU0y/Mm1E9WmjaZFOYzFCSA5Zm3Imzpi7fruBk8DpvItKhKYEmIs3Cw8WRqweEcvWAUErKK/n24ClW7MvgmwPZnDhdyqL1qSxan0qYrxuT+wUzZUAIg7r44uCgZJqIiHRANaPPuo8HF89WaXJQeCf8PZzJKS5n+5FcRkUFtEq70rzOVFTxzuajANw7rod9fTFZM8UypxkTaJq+KSJ2Qgk0EWl27s6OTB0QwtQBIZSWV7E2KZsV+zJZcyCL9LxS3th4hDc2HiHEx5XJ/UKYOiCYweGdlEwTEZGO42DrTt8EMDuYuKJ3IP/+/gQJB7KUQGujPtpxgtzictsI/37BRodTl39NAu2wbZpycyT3akag+SuBJiLGUgJNRFqUm7OZyf1DmNw/hDMVVaxNOsWKvRmsOZBFRv4Z3vzuCG9+d4Rgb1eu6hfM1AEhxHZVMk1ERNqxsiLb9E2Anq2XQAPbOmj//v4Eqw9k8eer+9jX6CW5oKpqK29sSAXg7jGRONrbNNxO3WzTOMuLoDADvJthrb2cFNsxIOrS6xIRuQRKoIlIq3F1MnNVv2Cu6hfMmYoq1ifbkmmrD2STWXCGtzcd5e1NRwn0crFN8+wfQlw3P8xKpomISHvi4gm/TYbMPeAd0qpNj4kOwMXRgeO5pSRnFRET7NWq7culWZWYybGcEnzdnZg+NNzocM7l6Ax+kbaklyW5eRJofX8JPl2gy9BLr0tE5BIogSYihnB1MhPfN5j4vsGUVVaxIdnCir0ZJOzPIruwjHc2H+OdzcdwdXLA38OFTh5O+Hm44OfuRCcPZ/zcnfHztB07eTjj5+FMJ3dnOrk72d+3sSIiIj9ndoSwIa3erLuzI2OiAlhzMJvVB7KUQGtDrFYrr663jT6bOSICd2c7/SjnH302gXYIuk+49PoGzrDdREQMZqe/dUWkI3FxNDOxTxAT+wRRVlnFdykWvtyTScL+TArOVJKeV0p6XulF1+fj5nQ2oWY7+nk41ybd6ku+ebs6agqLiIi0joozUJrbPCNzmmhinyDWHMxm1f4sHrhM0+Laim1Hctl9PA9nRwduG9nN6HDOLyAKkvlx6qWISDuhBJqI2BUXRzOX9wri8l5BVFT152ReKadLKsgtLiO3uILTxeXklpRzuricnOLyOo/zSiuwWiG/tIL80gqOXGSbjg4mfN2d8fdwPjvSzTaarSb5Vt9jVydzi/4cRESknTq8Bj68BXpfAzOWGhLCFb0CAdh9PI/sgjMEersaEoc0zqKzo89uiO1CZy8Xg6NpQL9pENQfQgdfel2Z++D4FggdYsiITRGRn1ICTUTslpPZgQh/DyL8L658VbWVvJJyTpeUk1tcQW5xzX3brb7kW3F5FZXVVixFZViKyi46Njcn89mRbedOLe3kUZOM09RSERH5mZrdN727GBZCoLcrg8J92XU8jzUHs7l5WFfDYpGLcyirkDUHszGZ4J6x3Y0Op2Ghg5sneQaQkgCrH4f+N8K0N5qnThGRJlICTUTaDbODCX9PF/w9L/5b2TMVVeSVVJBTXMbp4oraBFtN0u2nj2uScRVVVkorqpo8tTTA05mx0Z2Z0j+EqEDPprxUERFpi6qrIPlr2/2Y1t198+cm9Qli1/E8Vu/PUgKtDagZfRbfJ4jIAA+Do2lFlkO2o3+0sXGIiNDEBNqCBQt47rnnyMjIoG/fvrz44ouMHTv2vOXLysp48sknWbp0KZmZmXTp0oV58+Zx5513AvDJJ5/wzDPPkJKSQkVFBdHR0fz2t79l5syZl9SuiMiFuDqZCfYxE+xzcdNXrFYrRWWVdZJtP59K+vPk2zlTSy3FbD96mhcSkukV7MXU/iFMHRBC985KpomItGsntkOJBVx9IGKUoaFM7B3EcyuT2JhiIT2vlDBfN0PjkfNLyizks13pANw3vofB0VykLQvh5C4Y/zvwv4SYaxJoAUqgiYjxGp1AW7ZsGXPmzGHBggWMHj2a1157jcmTJ7N//366dq3/26vp06eTlZXF4sWLiYqKIjs7m8rKytrn/fz8mDdvHr169cLZ2ZkvvviCWbNmERgYyJVXXtnkdkVEmpvJZMLL1QkvVye6+rtf1DVV1VbyS39cx+2IpYiv92Wy4ZCFg5mFHMws5B8JyfQO8WZq/2Cm9FcyTUSkXUo6O30zOh7MToaG0jPIk/5hPuxNz2fm4q38Z/Yo/DycDY1JznXidAm3vbmViiorY6MDGNK1k9EhXZx9H9sSxjFXXVoCLUcJNBGxHyar1WptzAXDhw9nyJAhLFy4sPZc7969uf7665k/f/455b/++mtuuukmUlNT8fPzu+h2hgwZwtSpU3nqqaea1G59CgoK8PHxIT8/H29v74uORUSkJeSVlLNqfxZf7snguxQLldU//jruE+LN1AEhTO0fQreONFVDxA6p/9A2tIn36V9xtoTADW/aFlo32Mm8Um5YuImT+WcY2MWH9+4ZgaeLVnixFzlFZdz46mZSLcVEB3ry0eyR+Lq3kSTnp/8Du9+Hy/4Xxj/atDqKc+C5s+u9/TEDnC/ui0sRkcZoTP+hUStal5eXs2PHDuLj4+ucj4+PZ9OmTfVes3z5cuLi4nj22WcJCwujZ8+ePPLII5SW1r9ukNVqZc2aNSQlJTFu3Lgmtwu2qaMFBQV1biIi9sLX3ZnpceG8c+cwvv/fiTw7bQDjenbG0cHE/owCnluZxITn1zL1pQ0sWJvCsZxio0MWEZGmshyyJc8cnCBqotHRABDq68aSu4bTyd2J3Sfymf3uDsoqq4wOS4CiskrueGs7qZZiwnzdWHLXsLaTPAMIiLIdLclNr6Nm9JlPuJJnImIXGvUVk8VioaqqiqCgoDrng4KCyMzMrPea1NRUNm7ciKurK59++ikWi4X777+f3Nxc3nzzzdpy+fn5hIWFUVZWhtlsZsGCBUyaNKnJ7QLMnz+fJ554ojEvUUTEEL7uzkwfGs70oeGcLi5n1f5MvtiTwabDOSSeLCDxZAHPfp1EvzBvpvYPZWr/kIueQioiInYgez84ukLXkbY10OxEVKAnb80axi2vb2FjioW5y3bz0s2DMTuYjA6twyqrrOLeJd+zNz0fPw9n3r1rGCE+bWyNuoCetmNNEqwpapJv/lGXHo+ISDNo0hhtk6nuH1Sr1XrOuRrV1dWYTCbee+89fHxsnYUXXniBG264gVdeeQU3N9sfAy8vL3bt2kVRURFr1qxh7ty5dO/enQkTJjSpXYDHHnuMuXPn1j4uKCggPDy8Ua9VRKS1dfJwZsbQrswY2pXc4nJWJWby5V5bMm1fegH70gv4+9cH6R/mUzvNM9xPyTQREbvW5zrbyLNii9GRnGNQuC+vzYzlzre38+XeDDp5OPHUdf0a7GdLy6iqtjLnw11sOpyDh7OZt2cNbZvrotbsmmlJAasVmvJvyWoF367QuVfzxiYi0kSNSqAFBARgNpvPGfWVnZ19zuiwGiEhIYSFhdUmz8C2dpnVauXEiRNER9t+uTo4OBAVZft2YdCgQRw4cID58+czYcKEJrUL4OLigouLS2NeooiIXfHzcOamYV25aZgtmbYyMZMv92Sw6bCFven57E3P529fHWRgFx+m9A9hipJpIiL2y9nDdrNDY6M7888Zg3jog50s3ZKGv4cLD0/qaXRYHYrVauVPn+/jq32ZOJsdWHRbHAO6+BodVtP4RYLJAcoLoSgLvIIbX0fs7bZb45bsFhFpMY1aA83Z2ZnY2FgSEhLqnE9ISGDUqPq34h49ejQnT56kqKio9lxycjIODg506dLlvG1ZrVbKysqa3K6ISHvj5+HMzcO6svTu4WybN5G//qIfo6P8cTDB7hP5zP/qIGOf/ZbrXvmOResPc+J0idEhi4gIwOmjUJpndBQXdPWAUJ68rh8A/7fmEO9sOmpsQB3MCwnJvL81DZMJXrxpEKOjAowOqekcXcA3wnbfcgnTOKFpo9dERFpAo6dwzp07l5kzZxIXF8fIkSNZtGgRaWlpzJ49G7BNm0xPT2fJkiUA3HLLLTz11FPMmjWLJ554AovFwqOPPsqdd95ZO31z/vz5xMXF0aNHD8rLy1mxYgVLliyps+PmhdoVEelIAjxd+NXwCH41PAJLURlf77ONTNt6JIfdx/PYfTyPZ1YcZFC4L1cPCGFy/xDCfNvY+ikiIu3FynmQ/DVc8xIM/pXR0TRo5ogIcorKeHH1IR7/byK+7k5cNyjM6LDavbe+O8K/vkkB4Onr+zGlf4jBETWDmMlQfApcmjAFtboK8k/YNhBwaNSYDxGRFtPoBNqMGTPIycnhySefJCMjg379+rFixQoiImzfMGRkZJCWllZb3tPTk4SEBB566CHi4uLw9/dn+vTpPP3007VliouLuf/++zlx4gRubm706tWLpUuXMmPGjItuV0SkowrwdOHWERHcOiKCU4VlfJ2YyZd7TrL1SC67juex63geT395gMFdfZl6dppnqJJpIiKto+IMHP4GqishqK/R0VyU31wRTW5xOUs2H+O3/96Nr7sz43t2NjqsduvzXek88d/9APx2Uk9+NbydfL65an7Tr809Ai/Hgpsf/C5Vo9BExC6YrNaOM6m8oKAAHx8f8vPz8fb2NjocEZEWlV14hpX7bLt5bjuaW2cJkSFdfZk6IJQp/YPb3s5eIq1M/Ye2wW7fp+SV8P508A6DhxPbTCKgutrKrz/cyRd7MnBzMvP+PcMZ3LWT0WG1O2uTsrn7ne+prLZyx6hu/OWaPtq8AeDgCvjwZggeALM3GB2NiLRjjek/NGkXThERsX+BXq7MHNmNmSO7kV1whq8Tbcm07Udz+SEtjx/S8njqi/3ERnSqHZkW7ONqdNgiIu1L0grbMWZym0meATg4mHhh+iDySyvYcMjCrLe385/ZI4kK9DI6tHZjx7HT/M/SH6istnLdoFD+fHU7S55VVUBWIhSkQ6+pjbs25+y6aQHRzR+XiEgTaUK5iEgHEOjtym0ju/Hv+0ay5bErePyaPgzr5ofJZOvAP/nFfkbMX8ONr27ire+OkFVwxuiQRUTavupqSPrKdj9msrGxNIGzowOv3hrLwHBf8koqmLl4G+l5pUaH1S4kZxVy59vbKa2oYnzPzjx3w0AcHNpR8gygJBcWjYdlt0JlWeOurdl4wF8JNBGxH0qgiYh0MEHertwxOpJ/zx7J5j9cwV+u6cPQbrZpOduPnuaJ/9qSadNf3cw7m46SrWSaiEjTnNwJRVng7AXdxhodTZN4uDjy1h1D6dHZg4z8M9y2eCu5xeVGh9WmnThdwm2Lt5FfWsHgrr4svHUIzo7t8GOZZyC4eIO1GnJTG3dtjm1DBY1AExF70g5/U4uIyMUK9nFl1uhIPpo9is2PXc6fr+5DbEQnrFbYdjSXvyxPZPj8NUx/bTNLNh8lu1DJNBGRi5b0pe0YdQU4uhgbyyXw83Dm3buGE+LjyuFTxcx6ezvFZZVGh9Um5RSVcdvibWQWnCE60JO37hiKu3M7XVXHZPoxAWZJbty1NeX9o5o3JhGRS6AEmoiIABDi48adYyL5+H9GsekPl/Onq/swpKuvLZl2JJc/f57I8GfWMOO1zbyrZJqIyIUVZYHJofHrP9mhUF833r1rGJ3cndh9PI/ZS3dQVllldFhtSlFZJbPe3k6qpZgwXzeW3DUMX3dno8NqWTVTMGumZF6MklwoyTl7vRJoImI/lEATEZFzhPq6cdeYSD65fzSb/nA5/zu1N4PPJtO2HsnlT58nMuKZNdy0aDPvbjnGqcJGrm0iItIRXPcKPHKoXSTQAKICvXhr1jDcnc1sOGTht//eTVW19cIXCmWVVdz37vfsOZGPn4czS+4a1jF2wQ44mwCrmZJ5MWrKeoeBi2fzxyQi0kQmq9XaYf7q2e325iIibUR6Xilf7c3giz0Z7DqeV3vewQTDIv2I7xNMfN8gunRyNy5IkWam/kPboPep9Ww4dIo7395ORZWVmSMiePK6vu1r98hmVlVt5aEPfmDF3kw8nM18cO8IBnTxNTqs1pH4GXx0O4TFwT1rLu6aqgo4fdQ2Cq3riJaMTkSkUf0HJdBERKRJjueW8NW+DL7cm8nunyTTAPqFeRPfJ5gr+wbTM8hTH6ykTVP/oW2wu/fpxPcQ3L9Nr33WkP/uPsmvP9yJ1QpzJkYzZ2JPo0OyS1arlf/9bB/vbU3D2ezAm3cMZUx0gNFhtZ6sRFg4Clx84A/HbOuiiYjYkcb0H9rpipUiItLSwv3cuXdcD+4d14PjuSWs2p/FysRMvj+ay770AvalF/BCQjIR/u5c2TeY+D5BDOnaCQcHdZ5FpJ0rPQ2L48HJDX6zGzzaX8LkmoGh5JWU86fPE3lx9SH8PZyZObKb0WHZnX+uPsR7W9MwmeCfMwZ1rOQZgF8PwAQm4EweuHUyOCARkaZTAk1ERC5ZuJ87d42J5K4xkeQUlbHmQDar9mey/pCFYzklLFqfyqL1qQR4ujCpTyDxfYMZ1cMfF0ez0aGLiDS/QwlgrQLfru0yeVZj5shuWIrK+b81h/jz8kR83Z25ZmCo0WHZjbe/O8JLa2yL5z91XT+mDggxOCIDOLnC71JtibOLHX32/k22zTfinwL/Hi0bn4hIIyiBJiIizcrf04XpQ8OZPjSc4rJK1iefYmViJmsOZmMpKuODbcf5YNtxPF0cmRDTmSv7BjMhpjNerk5Ghy4i0jySVtiOMZONjaMVzJkYTW5xOe9uOcbcf+/Cx82JcT07Gx2W4T7flc7j/90PwNxJPbl1RITBERnI3e/iy1ZVQspqqK6AyX9ruZhERJpACTQREWkxHi6OTO4fwuT+IZRXVrP1SA4rEzNZlZhFdmEZX+yxbUjgbHZgVJQ/8X2CmdQniM5e7XPNIBHpACrL4NBq2/2YKcbG0gpMJhOPX9uX0yXlfLEng9lLd/De3cMZ3LXjTtVbm5TNb/+9G4DbR0bw0OVRBkdkJyrLwdG54TJ5x2zJM0c38O7SOnGJiFwkJdBERKRVODs6MDa6M2OjO/Pktf3YfSKPlYlZrErMJNVSzNqkU6xNOsW8z/YS27UT8X2DuLJvMBH+HkaHLiJy8Y5uhPJC8AyC0CFGR9MqzA4mXpg+iPzSCjYcsnDn29v5aPZIogK9jA6t1f2Qdpr/WfoDldVWrh0Yyl+u0Q6lHPgCvphj+//wq383XDYnxXb07wEODi0emohIYyiBJiIirc7BwcTgrp0Y3LUTf5jci5Tswtpk2u4T+Xx/7DTfHzvNMysOEhPkxZV9g4jvG0zfUG99EBER+1YzfbPnVR0qAeDs6MCrt8Zyyxtb2X08j5mLt/Hx/4wi1NfN6NBazaGsQu58ezulFVWM69mZ528cqI1zAFy9ofgU5By6cFlLsu3or1F7ImJ/Os5fdRERsVtRgV48cFkUnz84hs2PXc6T1/VlTFQAjg4mkrIKeembFK7+10bG/P1bnvhvIpsP51BZVW102CJtzoIFC4iMjMTV1ZXY2Fg2bNjQYPl169YRGxuLq6sr3bt359VXX63zfGJiItOmTaNbt26YTCZefPHFJrVrtVp5/PHHCQ0Nxc3NjQkTJpCYmHhJr9UQViskfWW732uqsbEYwMPFkbfuGEqPzh5k5J9h5uKtnC4uNzqsVpGeV8ptb24jr6SCQeG+vHrrEJwd9VELAP9o2/H0Mds0zoZYzibZAnq2bEwiIk2g3+oiImJXQnzcuG1kN5bePZwd/zuJF6YP5Kq+wbg5mUnPK+Wt745y8+tbGPrX1Tzy0W4S9mdxpqLK6LBF7N6yZcuYM2cO8+bNY+fOnYwdO5bJkyeTlpZWb/kjR44wZcoUxo4dy86dO/njH//Ir3/9az7++OPaMiUlJXTv3p2//e1vBAcHN7ndZ599lhdeeIGXX36Z7du3ExwczKRJkygsLGzeH0JLKz4Frj7g5A6R44yOxhB+Hs4suWs4IT6uHD5VzB1vb6e4rNLosFpUbnE5MxdvJSP/DFGBnrx1x1DcnTXRp5ZXMDh72namPX2k4bI1UzgDols+LhGRRjJZrVar0UG0loKCAnx8fMjPz8fb29vocEREpBHOVFSx4ZDFtqPngSxOl1TUPufmZGZ8z87E9w3iil5B+LhrR09pPu2l/zB8+HCGDBnCwoULa8/17t2b66+/nvnz559T/ve//z3Lly/nwIEDtedmz57N7t272bx58znlu3Xrxpw5c5gzZ06j2rVarYSGhjJnzhx+//vfA1BWVkZQUBB///vfue++++p9PWVlZZSVldU+LigoIDw83D7ep6JT4Nmxd6JMyS7khlc3k1dSwdjoABbfPrRdjsgqKqvkV69vYfeJfEJ9XPlPB5u2etFeGw8Zu2DGe9D76vOXey4airPhnm8hrGOsISgixmpMP6/9/RUTEZF2ydXJzKQ+QTx/40C2z5vIB/eM4I5R3QjzdaO0ooqvEzOZ++/dxD6dwK/e2MKSzUfJzD9jdNgidqG8vJwdO3YQHx9f53x8fDybNm2q95rNmzefU/7KK6/k+++/p6Kiot5rmtLukSNHyMzMrFPGxcWF8ePHnzc2gPnz5+Pj41N7Cw8Pv6iYWkUHT56BbWr+W3cMxc3JzIZDFub+exfV1e3re/uyyipmv7uD3Sfy6eTuxJK7hit5dj41I8pq1jirT3U1jJkDsbM0Ak1E7JLGFouISJvjaHZgZA9/Rvbw5y/X9CHxZAGrEjNZmZhFUlYh36Xk8F1KDn/+PJGBXXyI7xvMlX2DOuSOcCIAFouFqqoqgoKC6pwPCgoiMzOz3msyMzPrLV9ZWYnFYiEkJKRZ2q051lfm2LFj5637scceY+7cubWPa0agGabYAqeP2nYa7ECbBzRkcNdOvDozlrvf2c4XezLw83DmiWvbx66UVdVW5i7bzcYUC+7OZt6aNYyoQE+jw7JfNWua1UzRrI+DA4x8oHXiERFpAiXQRESkTTOZTPQL86FfmA9z42M4ailm1X5bMu2HtNPsPpHP7hP5PLcyie6dPYjvY0umDeziq93RpMP5eeLCarU2mMyor3x955uj3cbG5uLigouLS6PiaFGJn8KKRyBmCtz8gdHR2I3xPTvzj+mD+M2HO1my+Rj+Hi78ZmLbHl1ktVr5y/J9fLk3AyeziUUz4xgU7mt0WPatZldNy0XsxCkiYqeUQBMRkXalW4AH947rwb3jepBdeIbV+7NZtT+TTSk5pJ4q5tV1h3l13WGCvF2Y1CeIK/sGMzzSv12uzSNSIyAgALPZfM5os+zs7HNGftUIDg6ut7yjoyP+/v7N1m7N5gOZmZl1RrU1FJtdSlphO3YdYWwcdujagaGcLi7nL8sT+efqZPw8nZk5IsLosJrsxdWHWLolDZMJ/jljEGOiA4wOyf51HQHXL4TA3ucvk7wKCk5At7GawikidkmfFkREpN0K9HLlluFdeXvWMHb8aSL/unkwVw8IwdPFkayCMpZuSWPm4m3EPp3AnA93smJvRrvfLU46JmdnZ2JjY0lISKhzPiEhgVGjRtV7zciRI88pv2rVKuLi4nByuriNOi6m3cjISIKDg+uUKS8vZ926deeNze6cyYcjG2z3Y6YYG4udun1UN359hS0p8ufP9/HFnpMGR9Q072w6yv+tsY2ievK6flw9INTgiNoI71AYdAuEDj5/mZ1L4IuH4VDC+cuIiBhII9BERKRD8HJ14pqBoVwzMJSyyio2Hc5hVWImCfuzsBSV89muk3y26yTOjg6Miw4gvk8wl/cOJMDTjqaIiVyCuXPnMnPmTOLi4hg5ciSLFi0iLS2N2bNnA7Y1xdLT01myZAlg23Hz5ZdfZu7cudxzzz1s3ryZxYsX88EHP05PLC8vZ//+/bX309PT2bVrF56enkRFRV1UuyaTiTlz5vDMM88QHR1NdHQ0zzzzDO7u7txyyy2t+SNqupQ1UF0B/tEaOdOAhydGk1ts+/Li4WW78HFzYmx029lwYfnukzz+30QAHp7Ys02PorNLlrPro+n/kIjYKSXQRESkw3FxNHNZTCCXxQTy9PVWdqadZtX+LFYmZnIsp4TVB7JZfSAbgCBvF2KCvekV7EVMkBcxwV5EBXri6mQ2+FWINM6MGTPIycnhySefJCMjg379+rFixQoiImxJgIyMDNLS0mrLR0ZGsmLFCh5++GFeeeUVQkNDeemll5g2bVptmZMnTzJ48I8jSp5//nmef/55xo8fz9q1ay+qXYDf/e53lJaWcv/993P69GmGDx/OqlWr8PJqIxt/1EzfjJlsbBx2zmQy8cS1/ThdUsGXezK4790dvH/PiDaxftj65FP89t+7sFrh9pER/PqKKKNDanv2fQIpq6H3tRBzVd3nqqsgN9V2318/WxGxTyZrzWqwHUBBQQE+Pj7k5+fj7e1tdDgiImJnrFYryVlFrEzMZGViJoknC+otZ3YwERngQUywF73OJtV6BXvTpZObNiZoh9R/aBsMe5+qKuC5HrZpnHeu1BpoF6Gssoq73v6ejSkWOrk78dHsUXa9g+XOtNPc8vpWSiuquGZgKP83Y5B+1zfFit/Bttdg1K8h/qm6z+UegZcGgdkF5mWAg76kEpHW0Zj+g0agiYiInGUymYgJtiXEfn1FNIVnKkjOKiIps5CkzAIOZhZyMLOQ/NIKUrKLSMku4ksyaq/3cDbTM9jrJ6PVbCPXOnk4G/iqRKRFpW22Jc/cA6DLUKOjaRNcHM28OjOWX72+hd0n8rlt8Vb+8z+jCPV1Mzq0c6RkFzLr7e2UVlQxNjqAf9w4UMmzpqqZmpmTcu5zNef8eyh5JiJ2Swk0ERGR8/BydSI2ohOxEZ1qz1mtVrIKyjiYWXA2sWZLqqVkF1FcXsXOtDx2puXVqSfQy+XsKDXbSDVNAxVpR0pywSsUelyuD/6N4OniyFuzhnHDq5tIPVXMbW9u46P7RtrVFw7peaXMXLyNvJIKBoX78uqtsdqx+VLUTM20HDr3OUty3TIiInZICTQREZFGMJlMBPu4EuzjyoSYwNrzFVXVHLUUc/AnSbWkrAKO55aSXVhGdmEZGw5ZasubHUx083evTajFBHvRW9NARdqevtdDn+ugvMjoSNocPw9n3r1rONMWbCIlu4hZb2/nvbuH4+Fi/EeU3OJyblu8lYz8M0QFevLWHUPtIq42rWYE2ukjtqnP5p/s5luTVNMGAiJix/RXQEREpBk4mR2IDvIiOsiLawb+eL6orLJ2pFrNNNCkrELySio4fKqYw6eK+XLvj9NA3Z3N9Aw6Ow00+Mf11fzsaFSGiPyMyQQubWTDAzsT5uvGu3cN48bXNrPreB6zl+5g8e1DDR3pVVxWyay3t3P4VDGhPq4suXOYXY2Ma7O8QsHJAyqK4fTRuskyFy/wCYeAGMPCExG5EG0iICIi0sqsVivZhWVnR6sVcDDjx2mg5VXV9V7z02mgNWuraRpo61D/oW0w5H06vh3cOkGApp1dqh/STvMrO1iov7yymrve2c6GQ21jg4M259WxkLkHbvoAek0xOhoREW0iICIiYs9MJhNB3q4Eebsyvmfn2vOVVdUczflxGuiBjIangTqYoFuAR5211XoFexHeyV3TQEVaw9e/h/Qd8Ms3YMCNRkfTpg3p2omFtw7h7ne+57+7T+Ln7sTj1/bFZGq932VV1Vbm/nsXGw5ZcHc289asYUqeNbeAaFsCLaeeddBEROycEmgiIiJ2wtHsQFSgF1GBXlw94MfzRWWVJGcV/mTTAttU0LySClJPFZN6qpgVezNry7s7m4kO8qJX0NkpoCGaBirS7AoybMkzgMixxsbSTkyICeQf0wfymw938c7mY/h7uvDrK1pnTSyr1coT/03kiz0ZOJlNvDYzlkHhvq3SdocSOQ6s1rqbBZzJhzMF4B0GDtqkQUTslxJoIiIids7TxZEhXTsxpGvd3UDrTAM9m1w7lF1ESXkVu4/nsft4Xp16Onu50CvYixHd/bksJpDeIV6tOrpDpF1J/tp2DIsDr2BjY2lHrhsUxunich7/735eSEimk4czM0dEtHi7/7fmEEs2H8NkghemD2JsdOcLXySNF3uH7fZTSV/Dp/dC98vgts8MCEpE5OI0KcW/YMECIiMjcXV1JTY2lg0bNjRYvqysjHnz5hEREYGLiws9evTgzTffrH3+9ddfZ+zYsXTq1IlOnToxceJEtm3bVqeOwsJC5syZQ0REBG5ubowaNYrt27c3JXwREZE2r2Ya6Pienbl3XA9emD6IL389lv1PXMnqueN5+ZbBPHR5FJP6BNHVzx2AU2engD63MokpL21gxPw1/OHjPXy9L5PCMxUGvyKRNiZphe0YM9nYONqhO0ZH8uvLbSOU/vz5Pr7ck3GBKy7Nu5uP8uJq25TCJ6/tyzUDQ1u0PfkZS7Lt6NvV2DhERC6g0SPQli1bxpw5c1iwYAGjR4/mtddeY/Lkyezfv5+uXev/pTd9+nSysrJYvHgxUVFRZGdnU1lZWfv82rVrufnmmxk1ahSurq48++yzxMfHk5iYSFhYGAB33303+/bt49133yU0NJSlS5cyceJE9u/fX1tGRESko7NNA/UkKtCzzjTQ4rPTQPel57Mu2cJ3KRayCsr4cPtxPtx+HCeziaHd/JgQ05nLYgKJCvTU6DSR8ykrgtR1tvu9phobSzv18KSe5BSX897WNOYs24mPmxNjogOavZ3/7j7Jn5cnAjBnYjQzR3Zr9jbkZzL32pJm0fG23Tdr1kML6GlsXCIiF9DoXTiHDx/OkCFDWLhwYe253r17c/311zN//vxzyn/99dfcdNNNpKam4ufnd1FtVFVV0alTJ15++WVuu+02SktL8fLy4vPPP2fq1B87KYMGDeLqq6/m6aefrreesrIyysrKah8XFBQQHh6uXbRERKTDO1NRxfajuXx78BRrk7JJtRTXeT7M143LetmSaSN7+OPu3HFXfdAunG1Dq75P+5fDv2dCp27w612gZHOLqKq28tAHP7BibybuzmY+uGcEA5txXbL1yae4653tVFRZuW1kBE+08qYFHdYLfaAgHe5cBV2Hw4JRkJ0It/wbel5pdHQi0sE0pv/QqCmc5eXl7Nixg/j4+Drn4+Pj2bRpU73XLF++nLi4OJ599lnCwsLo2bMnjzzyCKWlpedtp6SkhIqKitqEW2VlJVVVVbi6utYp5+bmxsaNG89bz/z58/Hx8am9hYeHX+xLFRERaddcncyMje7Mn6/pwzePTGDtIxN4/Jo+jO/ZGWdHB9LzSlm6JY273vmeQU8mcNub23jruyMc/VmiTaRDSvrKdoyZquRZCzI7mPjnjEGMjvKnpLyKO97aRkp2UbPUvet4HrOX7qCiysrVA0J4/Bolz1pNzQYCOYeguhpyD9c9LyJipxr1dbLFYqGqqoqgoKA654OCgsjMzKz3mtTUVDZu3IirqyuffvopFouF+++/n9zc3DrroP3UH/7wB8LCwpg4cSIAXl5ejBw5kqeeeorevXsTFBTEBx98wNatW4mOPv/OPI899hhz586tfVwzAk1ERETq6hbgwR0BkdwxOpLS8io2p1r49uApvjmYTXpeKeuTT7E++RRP/Hc/kQEetVM9h0X64epkNjp8kdbl4gkuPlr/rBW4OJp5bWYct7y+hT0n8rlt8VY+vn8UIT5uTa4zJbuQWW9to6S8irHRAbwwfRAODkqetZqAaDiyDiyHIP84VJ4BByfwbfnNIkRELkWT5mP8/NsZq9V63m9sqqurMZlMvPfee/j4+ADwwgsvcMMNN/DKK6/g5lb3j9+zzz7LBx98wNq1a+uMOHv33Xe58847CQsLw2w2M2TIEG655RZ++OGH88bp4uKCi4tLU16iiIhIh+XmbObyXkFc3iuIJ61WDp8q4tuDp/g2KZttR3I5YinmiKWYt747ipuTmdFR/kyICWRCTGe6dHI3OnyRljflObjyGTA1aT8uaSRPF0feumMoN766mVRLMTMXb+Oj+0bSycO50XWdzCvltsXbOF1SwcBwX169NRZnR72PrapmrbOcFFsSDcCvO5g77lIBItI2NOq3VEBAAGaz+ZzRZtnZ2eeMSqsREhJCWFhYbfIMbGumWa1WTpw4UWcE2fPPP88zzzzD6tWrGTBgQJ16evTowbp16yguLqagoICQkBBmzJhBZGRkY16CiIiINILJZCIq0IuoQC/uGdedwjMVfJeSw9qkbL5NyiaroIzVB7JZfSAbgJ5BnlwWE8iEmEDiunXCyawPptJOmZ2MjqBD8fd0Ycldw7hh4WZSsou4853tvHf38Eatz3i6uJzb3tzGyfwz9OjswVt3DMXDRUmbVlczVdNy6CcbCJx/VpGIiL1o1F8MZ2dnYmNjSUhI4Be/+EXt+YSEBK677rp6rxk9ejQfffQRRUVFeHp6ApCcnIyDgwNdunSpLffcc8/x9NNPs3LlSuLi4s4bg4eHBx4eHpw+fZqVK1fy7LPPNuYliIiIyCXwcnXiqn7BXNUvGKvVyoGMQr5NymZtUjY7jp0mOauI5KwiXlufipeLI2OiA7gsJpDxMZ0J8na9cAMi9m7PRxA5Drzq//JYWk6XTu4suWsYN766mZ1pecxe+gNv3BZ3USPIissquePt7aRkFxHi48qSu4bj14QRbNIMapJluakw5DbofhnQqH3tREQM0ehdOJctW8bMmTN59dVXGTlyJIsWLeL1118nMTGRiIgIHnvsMdLT01myZAkARUVF9O7dmxEjRvDEE09gsVi4++67GT9+PK+//jpgm7b5pz/9iffff5/Ro0fXtuXp6VmbdFu5ciVWq5WYmBhSUlJ49NFHcXFxYePGjTg5Xdw3gNpFS0REpOXkl1Sw/pBtque6pFPkFJfXeb5vqDeXxQRyWa/ODArvhLmNrDmk/kPb0Crv06lkeGUomF3g90fBWVOWjbDj2GlufWMrpRVVXDswlBdnNLyGWXllNXe9s50Nhyz4ujvxn9kjiQr0asWIpY7qangmFCpL4aEfwL+H0RGJSAfWmP5Do8csz5gxg5ycHJ588kkyMjLo168fK1asICLCtuhjRkYGaWlpteU9PT1JSEjgoYceIi4uDn9/f6ZPn87TTz9dW2bBggWUl5dzww031GnrL3/5C48//jgA+fn5PPbYY5w4cQI/Pz+mTZvGX//614tOnomIiEjL8nF34pqBoVwzMJTqait70/P5Nimbb5NOsedEHoknC0g8WcDL36bg6+7EuOjOXNarM+OiO+PvqTVLpQ1IWmE7dhuj5JmBYiM6sfDWIdz9zvcs330SPw9n/nJNn3rXZK6utvLbj3az4ZAFd2czb90xVMkzozk42JJmeWlQlKUEmoi0GY0egdaW6RtkERERY1iKyliffIpvk2y7eeaXVtQ+ZzLBwC6+Z9dO60z/MB+72hFP/Ye2oVXep8XxcHwrTHkeht3TMm3IRft8Vzq/+XAXAL+d1JOHrqi7jpbVauXx5Ym8s/kYTmYTi28fyrienQ2IVM5xpsD2y/+NSRAQBdPeBEdNqRWR1teiI9BEREREGivA04VfDunCL4d0obKqmt0n8mp39kw8WcCu43nsOp7HP1cnE+DpzLienbksJpBx0Z3xcddoc7EDRafg+Dbb/ZgpxsYiAFw3KIzc4nKe+O9+/pGQTCcPZ24dEVH7/EtrUnhn8zFMJvjH9EFKntkTV284uRNOHYDiU0qeiUiboASaiIiItCpHswOxEX7ERvjxyJUxZBWcYV2SLZm24ZAFS1E5n/yQzic/pGN2MDGkqy8TYgK5LCaQ3iFe9U7TEmlxyV8DVggZCD5hRkcjZ80aHUlucTn/+iaFP32+Dz8PZ6b0D+HdLcf45+pkAJ64ti/XDgw1OFI5hyXFdtQOnCLSRiiBJiIiIoYK8nZl+tBwpg8Np7yymh3HTrM2KZtvk7JJzipi+9HTbD96mudWJhHs7cqEmM5MiAlkTHQAni7qykgrSfrKdoyZamwcco65k3qSU1zO+1vTmPPhLnafyGPR+lQAfnNFNLeN7GZsgHIuSwp8crftvn+UsbGIiFwk9TpFRETEbjg7OjCyhz8je/jz2JTenDhdwtqkU6xNyua7lBwyC87w4fbjfLj9OE5mE0O7+dXu7Nmjs6dGp0nLqCiFw9/Y7sdMNjYWOYfJZOKp6/qRV1LOir2ZvLbOljybOSKCORM1uskueQX9eN8z6PzlRETsiBJoIiIiYre6dHLn1hER3DoigjMVVWw7ksu3SdmsTTrFEUsxmw7nsOlwDn9dcYBvH5lAZICH0SFLe1RsgW6jIfcIBPc3Ohqph9nBxD9nDCK/dDvfpeQwdUAIj1/bV0l1e+Xyk51QvYKNi0NEpBG0C6eIiIi0SUcsxWenep4iM7+UlXPGtciHZfUf2oZWeZ+qKsGs75/tWUVVNfvS8xnQxRezHe3mK/X4/k04tBpuWAxObkZHIyIdVGP6D0qgiYiISJtXVW1tsQ/L6j+0DXqfREREpLEa039waKWYRERERFqMRpqIiIiISEtSAk1ERERERERERKQBSqCJiIiIiIiIiIg0QAk0ERERERERERGRBiiBJiIiIiIiIiIi0gAl0ERERERERERERBqgBJqIiIiIiIiIiEgDlEATERERERERERFpgBJoIiIiIiIiIiIiDVACTUREREREREREpAFKoImIiIiIiIiIiDRACTQREREREREREZEGKIEmIiIiIiIiIiLSACXQREREREREREREGuBodACtyWq1AlBQUGBwJCIiItJW1PQbavoRYp/UzxMREZHGakw/r0Ml0AoLCwEIDw83OBIRERFpawoLC/Hx8TE6DDkP9fNERESkqS6mn2eydqCvU6urqzl58iReXl6YTKZmr7+goIDw8HCOHz+Ot7d3s9cvzUPvU9uh96rt0HvVdui9ajyr1UphYSGhoaE4OGj1C3ulfp6A3qe2RO9V26H3qu3Qe9V4jenndagRaA4ODnTp0qXF2/H29tY/1jZA71Pbofeq7dB71XbovWocjTyzf+rnyU/pfWo79F61HXqv2g69V41zsf08fY0qIiIiIiIiIiLSACXQREREREREREREGqAEWjNycXHhL3/5Cy4uLkaHIg3Q+9R26L1qO/RetR16r0SaRv932ga9T22H3qu2Q+9V26H3qmV1qE0EREREREREREREGksj0ERERERERERERBqgBJqIiIiIiIiIiEgDlEATERERERERERFpgBJoIiIiIiIiIiIiDVACTUREREREREREpAFKoDWTBQsWEBkZiaurK7GxsWzYsMHokORn5s+fz9ChQ/Hy8iIwMJDrr7+epKQko8OSC5g/fz4mk4k5c+YYHYrUIz09nVtvvRV/f3/c3d0ZNGgQO3bsMDos+ZnKykr+93//l8jISNzc3OjevTtPPvkk1dXVRocm0iaon2f/1M9rm9TPs3/q69k/9fNajxJozWDZsmXMmTOHefPmsXPnTsaOHcvkyZNJS0szOjT5iXXr1vHAAw+wZcsWEhISqKysJD4+nuLiYqNDk/PYvn07ixYtYsCAAUaHIvU4ffo0o0ePxsnJia+++or9+/fzj3/8A19fX6NDk5/5+9//zquvvsrLL7/MgQMHePbZZ3nuuef417/+ZXRoInZP/by2Qf28tkf9PPunvl7boH5e6zFZrVar0UG0dcOHD2fIkCEsXLiw9lzv3r25/vrrmT9/voGRSUNOnTpFYGAg69atY9y4cUaHIz9TVFTEkCFDWLBgAU8//TSDBg3ixRdfNDos+Yk//OEPfPfddxqJ0QZcffXVBAUFsXjx4tpz06ZNw93dnXfffdfAyETsn/p5bZP6efZN/by2QX29tkH9vNajEWiXqLy8nB07dhAfH1/nfHx8PJs2bTIoKrkY+fn5APj5+RkcidTngQceYOrUqUycONHoUOQ8li9fTlxcHDfeeCOBgYEMHjyY119/3eiwpB5jxoxhzZo1JCcnA7B79242btzIlClTDI5MxL6pn9d2qZ9n39TPaxvU12sb1M9rPY5GB9DWWSwWqqqqCAoKqnM+KCiIzMxMg6KSC7FarcydO5cxY8bQr18/o8ORn/nwww/54Ycf2L59u9GhSANSU1NZuHAhc+fO5Y9//CPbtm3j17/+NS4uLtx2221Ghyc/8fvf/578/Hx69eqF2WymqqqKv/71r9x8881GhyZi19TPa5vUz7Nv6ue1HerrtQ3q57UeJdCaiclkqvPYarWec07sx4MPPsiePXvYuHGj0aHIzxw/fpzf/OY3rFq1CldXV6PDkQZUV1cTFxfHM888A8DgwYNJTExk4cKF6lTZmWXLlrF06VLef/99+vbty65du5gzZw6hoaHcfvvtRocnYvfUz2tb1M+zX+rntS3q67UN6ue1HiXQLlFAQABms/mcbyGzs7PP+bZS7MNDDz3E8uXLWb9+PV26dDE6HPmZHTt2kJ2dTWxsbO25qqoq1q9fz8svv0xZWRlms9nACKVGSEgIffr0qXOud+/efPzxxwZFJOfz6KOP8oc//IGbbroJgP79+3Ps2DHmz5+vjpVIA9TPa3vUz7Nv6ue1LerrtQ3q57UerYF2iZydnYmNjSUhIaHO+YSEBEaNGmVQVFIfq9XKgw8+yCeffMI333xDZGSk0SFJPa644gr27t3Lrl27am9xcXH86le/YteuXepU2ZHRo0eTlJRU51xycjIREREGRSTnU1JSgoND3T/5ZrNZ25uLXID6eW2H+nltg/p5bYv6em2D+nmtRyPQmsHcuXOZOXMmcXFxjBw5kkWLFpGWlsbs2bONDk1+4oEHHuD999/n888/x8vLq/bbZB8fH9zc3AyOTmp4eXmds16Jh4cH/v7+WsfEzjz88MOMGjWKZ555hunTp7Nt2zYWLVrEokWLjA5Nfuaaa67hr3/9K127dqVv377s3LmTF154gTvvvNPo0ETsnvp5bYP6eW2D+nlti/p6bYP6ea3HZLVarUYH0R4sWLCAZ599loyMDPr168c///lPbZltZ863Vslbb73FHXfc0brBSKNMmDBB25vbqS+++ILHHnuMQ4cOERkZydy5c7nnnnuMDkt+prCwkD/96U98+umnZGdnExoays0338yf//xnnJ2djQ5PxO6pn2f/1M9ru9TPs2/q69k/9fNajxJoIiIiIiIiIiIiDdAaaCIiIiIiIiIiIg1QAk1ERERERERERKQBSqCJiIiIiIiIiIg0QAk0ERERERERERGRBiiBJiIiIiIiIiIi0gAl0ERERERERERERBqgBJqIiIiIiIiIiEgDlEATERERERERERFpgBJoIiIiIiIiIiIiDVACTUREREREREREpAFKoImIiIiIiIiIiDTg/wGUP+g+tYfG7AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.lineplot(data=res_df[[\"train_loss\", \"test_loss\"]], ax=axs[0])\n",
    "sns.lineplot(data=res_df[[\"train_acc\", \"test_acc\"]], ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.889180</td>\n",
       "      <td>0.636762</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.889224</td>\n",
       "      <td>0.636459</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.887727</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.886415</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.885200</td>\n",
       "      <td>0.635829</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.884910</td>\n",
       "      <td>0.634972</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.883681</td>\n",
       "      <td>0.635082</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.882526</td>\n",
       "      <td>0.634628</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.881879</td>\n",
       "      <td>0.634503</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.880818</td>\n",
       "      <td>0.634032</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  test_loss  train_acc  test_acc\n",
       "0    1.889180   0.636762   0.059322  0.008475\n",
       "1    1.889224   0.636459   0.033898  0.016949\n",
       "2    1.887727   0.636100   0.025424  0.025424\n",
       "3    1.886415   0.635870   0.033898  0.025424\n",
       "4    1.885200   0.635829   0.050847  0.008475\n",
       "5    1.884910   0.634972   0.050847  0.025424\n",
       "6    1.883681   0.635082   0.076271  0.016949\n",
       "7    1.882526   0.634628   0.050847  0.016949\n",
       "8    1.881879   0.634503   0.059322  0.016949\n",
       "9    1.880818   0.634032   0.042373  0.016949"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Define the LightningModule\n",
    "class PLClassifier(pl.LightningModule):\n",
    "    def __init__(self, classifier):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x[\"tokens\"], x[\"masks\"], x[\"interventions_masks\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        tokens = x[\"tokens\"]\n",
    "        masks = x[\"masks\"]\n",
    "        interventions_masks = x[\"interventions_masks\"]\n",
    "        \n",
    "        output = self.classifier(tokens, masks, interventions_masks)\n",
    "        \n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size=4):\n",
    "    train, val = InterventionsDataset(train_data), InterventionsDataset(val_data)\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "    total_acc_trains = []\n",
    "    total_loss_trains = []\n",
    "    total_acc_vals = []\n",
    "    total_loss_vals = []\n",
    "    \n",
    "    pbar = tqdm(range(epochs), leave=False)\n",
    "    for epoch_num in pbar:\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        inner_pbar = tqdm(train_dataloader, leave=False, total=len(train_dataloader), desc=\"Training\")\n",
    "        for train_input, train_label in train_dataloader:\n",
    "            train_label = train_label.to(device)\n",
    "            tokens = train_input[\"tokens\"].to(device)\n",
    "            masks = train_input[\"masks\"].to(device)\n",
    "            interventions_masks = train_input[\"interventions_masks\"].to(device)\n",
    "\n",
    "            output = model(tokens, masks, interventions_masks)\n",
    "\n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            inner_pbar.update()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inner_pbar.reset(total=len(val_dataloader))\n",
    "            inner_pbar.set_description(\"Validation\")\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                tokens = val_input[\"tokens\"].to(device)\n",
    "                masks = val_input[\"masks\"].to(device)\n",
    "                interventions_masks = val_input[\"interventions_masks\"].to(device)\n",
    "\n",
    "                output = model(tokens, masks, interventions_masks)\n",
    "\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "                inner_pbar.update()\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_path = Path(\"../models/2017-2022\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(classifier.state_dict(), model_path / \"camembert_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), model_path / \"camembert_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, batch_size=4):\n",
    "\n",
    "    test = InterventionsDataset(test_data)\n",
    "\n",
    "    test_dataloader = DataLoader(test, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            val_label = test_label.to(device)\n",
    "            tokens = test_input[\"tokens\"].to(device)\n",
    "            masks = test_input[\"masks\"].to(device)\n",
    "            interventions_masks = test_input[\"interventions_masks\"].to(device)\n",
    "\n",
    "            output = model(tokens, masks, interventions_masks)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "    print(f\"Test Accuracy: {total_acc_test / len(test_data): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3331f370d3744e8b3c0105457a9706d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  11.000\n"
     ]
    }
   ],
   "source": [
    "evaluate(classifier, test_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-11-7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290de4d201867099a7cec8aa5bca78d01a6c85d7bcab7b52f8e97114ad853450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
