{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NLP and French Politic \u00b6 This site contains the documentation for NLP project for the Deep Learning course of the MVA. This is a group project with Gabriel Watkinson and J\u00e9remie Stym-Popper. Project Layout \u00b6 There are mutliple distinct parts in this project. Getting the Data \u00b6 First of all, we made use of an API made available by nosdeputes.fr to fetch the textual retranscription of every session at the French Assembl\u00e9e Nationale. The API class is available in the api.py module. Building the Models \u00b6 Running Experiments \u00b6 Analysing the results \u00b6","title":"Home"},{"location":"#nlp-and-french-politic","text":"This site contains the documentation for NLP project for the Deep Learning course of the MVA. This is a group project with Gabriel Watkinson and J\u00e9remie Stym-Popper.","title":"NLP and French Politic"},{"location":"#project-layout","text":"There are mutliple distinct parts in this project.","title":"Project Layout"},{"location":"#getting-the-data","text":"First of all, we made use of an API made available by nosdeputes.fr to fetch the textual retranscription of every session at the French Assembl\u00e9e Nationale. The API class is available in the api.py module.","title":"Getting the Data"},{"location":"#building-the-models","text":"","title":"Building the Models"},{"location":"#running-experiments","text":"","title":"Running Experiments"},{"location":"#analysing-the-results","text":"","title":"Analysing the results"},{"location":"api/api/","tags":["data","api","scrapping"],"text":"Getting the data \u00b6 For this project, we used data made openly available by assemblee-nationale.fr/ . The site nosdeputes.fr wrote a API to simplify the acces to the data. We used this website to fetch the textual data used in the project. Wrapper function \u00b6 Wrapper function We defined a wrapper function to simplify the use of the API. It fetches all the urls, interventions and the deputies information for a given legislature. It is used to fetch the data in the notebook . Fetch data for a given legislature, including interventions and deputies data. Parameters: Name Type Description Default legislature str the legislature to fetch data for. '2017-2022' root_dir str the root directory to save data. './data/' max_interventions int the maximum number of interventions to fetch. 500 Returns: Name Type Description uresults tuple containing the urls of the interventions, the interventions data and the deputies data Source code in nlp_assemblee/api.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 async def fetch_data_for_legislature ( legislature = \"2017-2022\" , root_dir = \"./data/\" , max_interventions = 500 ): \"\"\"Fetch data for a given legislature, including interventions and deputies data. Args: legislature (str): the legislature to fetch data for. root_dir (str): the root directory to save data. max_interventions (int): the maximum number of interventions to fetch. Returns: uresults (tuple): containing the urls of the interventions, the interventions data and the deputies data \"\"\" api = CPCApi ( legislature = legislature ) deps_df = api . deputies_df urls = await api . async_get_all_interventions_urls ( all_pages = False , max_interventions = max_interventions , sort = 0 , count = 500 , object_name = \"Intervention\" , page = 1 , verbose = True , save = f \" { root_dir } /interventions_urls_by_deputies.json\" , ) interventions = await api . async_fetch_all_interventions ( urls , save = root_dir , max_interventions = max_interventions ) return urls , interventions , deps_df CPCApi \u00b6 API class The wrapper function uses an API class inspired by the GitHub of nosdeputes.fr . Bases: object The CPCApi class provides an interface for interacting with nosdeputes.fr's API. It allows users to retrieve information about parliamentarians, as well as the interventions they have made. The class provides asynchronous methods for fetching data, as well as methods for processing and saving the data. Additionally, the class contains methods for handling pagination and error handling. The class also provides a way to save the result of the request to a folder or file. The class uses aiohttp, asyncio and json library to make the request, parse the json and save the result to a file. Source code in nlp_assemblee/api.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 class CPCApi ( object ): \"\"\"The CPCApi class provides an interface for interacting with nosdeputes.fr's API. It allows users to retrieve information about parliamentarians, as well as the interventions they have made. The class provides asynchronous methods for fetching data, as well as methods for processing and saving the data. Additionally, the class contains methods for handling pagination and error handling. The class also provides a way to save the result of the request to a folder or file. The class uses aiohttp, asyncio and json library to make the request, parse the json and save the result to a file. \"\"\" def __init__ ( self , ptype = \"depute\" , legislature = \"2017-2022\" , format = \"json\" ): \"\"\"Initializes the API object. Args: ptype (str): The type of parliamentarian to retrieve data for. Valid values are \"depute\" (deputy) or \"senateur\" (senator). legislature (str): The legislature to retrieve data for. Valid values are \"2007-2012\" or \"2012-2017\", \"2017-2022\", or None for current legislature. Note that older legislatures have problems for some processing later on. format (str): The format to retrieve data in. Valid values are \"json\" or \"xml\". Raises: AssertionError: If ptype is not \"depute\" or \"senateur\", or if legislature is not \"2007-2012\", \"2012-2017\", \"2017-2022\", or None. \"\"\" assert ptype in [ \"depute\" , \"senateur\" ] assert legislature in [ \"2007-2012\" , \"2012-2017\" , \"2017-2022\" , None ] self . legislature = legislature self . legislature_name = \"last\" if legislature is None else legislature self . format = format self . ptype = ptype self . ptype_plural = ptype + \"s\" self . prefix = \"www\" if legislature is None else legislature self . base_url = f \"https:// { self . prefix } .nos { self . ptype_plural } .fr\" # Fetches the list of deputies self . parlementaires () # creates self.parlementaires_list # Create the dataframe of deputies and the name of the groups self . get_deputies_df () # creates self.deputies_df and self.groups and self.deputies @memoize def parlementaires ( self , active = None ): \"\"\"Retrieves a list of parliamentaries. Args: active (bool): Whether to retrieve active parliamentaries or not. Returns: parlementaires_list (list): A list of parliamentaries. This also creates the attribute self.parlementaires_list. Creates: self.parlementaires_list : A list of all parliamentaries \"\"\" # Build the URL based on the active parameter if active is None : url = f \" { self . base_url } / { self . ptype_plural } / { self . format } \" else : url = f \" { self . base_url } / { self . ptype_plural } /enmandat/ { self . format } \" # Retrieve the data from the API data = requests . get ( url ) . json () # Extract the parliamentaries from the response and return them self . parlementaires_list = [ depute [ self . ptype ] for depute in data [ self . ptype_plural ]] return self . parlementaires_list def search_parlementaires ( self , q , field = \"nom\" , limit = 5 ): \"\"\"Fuzzy searches for a parliamentarian in the deputies list. Args: q (str): The query to search for. field (str): The field of the parliamentarian data to search in. limit (int): The maximum number of results to return. Returns: name (str): The exact name of the parliamentarian in the list. \"\"\" # Search for parliamentarians and return the best matches name = extractBests ( q , self . parlementaires_list , processor = lambda x : x [ field ] if type ( x ) == dict else x , limit = limit , ) return name def get_deputies_df ( self ): \"\"\"Retrieves a DataFrame of deputies information from the API. Returns: deputies_df (pandas.DataFrame): A DataFrame containing information about deputies. Creates: self.deputies_df : A DataFrame containing information about deputies self.groups : groups of the deputies self.deputies : Deputies information \"\"\" # Retrieve deputies data from the API deputies_json = self . parlementaires_list # Convert the JSON data to a DataFrame cols_to_drop = [ \"sites_web\" , \"emails\" , \"adresses\" , \"collaborateurs\" , \"anciens_autres_mandats\" , \"autres_mandats\" , \"url_an\" , \"id_an\" , \"url_nosdeputes\" , \"url_nosdeputes_api\" , \"twitter\" , ] deputies_df = pd . json_normalize ( deputies_json ) deputies_df = deputies_df . drop ( columns = cols_to_drop , errors = \"ignore\" ) deputies_df [ \"legislature\" ] = self . legislature_name self . deputies_df = deputies_df self . all_groups = deputies_df [ \"groupe_sigle\" ] . unique () self . deputies = deputies_df [ \"nom\" ] . unique () return deputies_df @staticmethod async def urls_response ( session , url , count_param , page_param ): \"\"\"Retrieves the json response from a url. Args: session (aiohttp.ClientSession): The session to use for the request url (str): The url to retrieve the response from count_param (str): The parameter for the number of results in the url page_param (str): The parameter for the page number in the url Returns: response (dict): The json response from the url. Returns an empty dict if there is an error. The dict contains keys (start, end, last_result, results) \"\"\" async with session . get ( url + count_param + page_param ) as resp : try : tmp = await resp . json ( content_type = None ) except Exception as e : print ( e ) return { \"start\" : 1 , \"end\" : 0 , \"last_result\" : 0 , \"results\" : [], } return tmp async def async_get_deputy_interventions_urls ( self , dep_name , all_pages = False , max_interventions = 1000 , count = 500 , last_int = 500 , sort = 0 , page = 1 , ): \"\"\"Retrieves the urls of the interventions of a parliamentarian. Args: dep_name (str): The name of the parliamentarian. all_pages (bool, optional): Whether to retrieve all pages of results. max_interventions (int, optional): Maximum number of interventions to retrieve. count (int, optional): The number of interventions to retrieve per page. last_int (int, optional): The last intervention number to retrieve. sort (int, optional): The sort order of the interventions, by pertinence or by date. page (int, optional): The page of results to retrieve. Returns: response (dict): A json object containing the links to the interventions. \"\"\" if all_pages or max_interventions : page = 1 # Remove accents and special characters from the name and replace spaces with + name = self . search_parlementaires ( dep_name )[ 0 ][ 0 ][ \"nom\" ] slug_name = re . sub ( \" \" , \"+\" , unidecode . unidecode ( name . lower ())) # Build the URL for the search url = f \" { self . base_url } /recherche?object_name=Intervention&format= { self . format } \" url += f \"&tag=parlementaire%3D { slug_name } &sort= { sort } \" page_param = f \"&page= { page } \" count_param = f \"&count= { count } \" if last_int is None : tick = requests . get ( url + \"&count=0&page=1\" ) . json ()[ \"last_result\" ] if all_pages : last_int = tick else : last_int = min ( max_interventions , tick ) response = { \"start\" : 1 , \"end\" : 0 , \"last_result\" : last_int , \"results\" : []} # Retrieve the search results async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for page in range ( 1 , last_int // count + 2 ): page_param = f \"&page= { page } \" tasks . append ( asyncio . ensure_future ( self . urls_response ( session , url , count_param , page_param )) ) responses = await asyncio . gather ( * tasks ) for tmp in responses : response [ \"results\" ] += tmp [ \"results\" ] response [ \"end\" ] = tmp [ \"end\" ] response [ \"dep_name\" ] = dep_name response [ \"slug_name\" ] = slug_name response [ \"legislature\" ] = self . legislature_name return response async def async_get_all_interventions_urls ( self , all_pages = True , max_interventions = 1000 , sort = 0 , count = 500 , object_name = \"Intervention\" , page = 1 , verbose = True , save = \"./data/interventions_urls_by_deputies.json\" , ): \"\"\"Retrieves the URLs for all interventions by each deputy. Args: all_pages (bool, optional): Whether to retrieve all pages of results or just the first page. max_interventions (int, optional): Maximum number of interventions to retrieve. sort (int, optional): An integer representing the sorting method to use. count (int, optional): The number of results to retrieve per page. object_name (str, optional): The name of the object to retrieve. page (int, optional): The page number to start at. verbose (bool, optional): Whether to display a progress bar. save (str, optional): The file path to save the results to. Returns: deputies_list (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. \"\"\" deputies_list = {} async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for dep in self . deputies : tmp = asyncio . ensure_future ( self . async_get_deputy_interventions_urls ( dep_name = dep , session = session , count = count , object_name = object_name , sort = sort , page = page , all_pages = all_pages , max_interventions = max_interventions , ) ) tasks . append ( tmp ) responses = await asyncio . gather ( * tasks ) for dep , response in zip ( self . deputies , responses ): deputies_list [ dep ] = response # Save the results to a file if save path is provided if save : path = Path ( save ) path_list = list ( path . parts ) path_list . insert ( 2 , self . legislature_name ) save_path = Path ( \"\" ) . joinpath ( * path_list [: - 1 ]) save_path . mkdir ( parents = True , exist_ok = True ) with open ( save_path / path_list [ - 1 ], \"w\" ) as f : json . dump ( deputies_list , f ) self . deputies_list_file = str ( save_path / path_list [ - 1 ]) return deputies_list @staticmethod async def process_response ( session , url ): \"\"\"Given a url, this function calls the url and returns the json object associated with the url. Args: session : The aiohttp session object url : The url to call Returns: intervention (dict): json object containing the intervention details \"\"\" async with session . get ( url ) as resp : action_item = await resp . json ( content_type = None ) return action_item [ \"intervention\" ] async def async_fetch_interventions_of_deputy ( self , dep_urls , slug_name = None , max_interventions = 1000 , verbose = True , save = \"./data/\" ): \"\"\"Retrieves the interventions of a parliamentarian. Args: dep_urls (dict): A dictionary containing the links to the interventions. slug_name (str): The slug name of the parliamentarian. max_interventions (int): The maximum number of interventions to retrieve. verbose (bool): Whether to display a progress bar. save (str):The file path to save the results to. Returns: interventions (dict): A dictionary containing the interventions. \"\"\" # Initialize an empty list to store the interventions interventions = { \"start\" : dep_urls [ \"start\" ], \"end\" : dep_urls [ \"end\" ], \"last_result\" : dep_urls [ \"last_result\" ], \"interventions\" : [], } inters = ( dep_urls [ \"results\" ][: max_interventions ] if max_interventions else dep_urls [ \"results\" ] ) urls = [ intervention [ \"document_url\" ] for intervention in inters ] pbar = tqdm ( urls , leave = False ) if verbose else urls async with aiohttp . ClientSession ( trust_env = True ) as session : # Fetch the interventions from the API tasks = [] for url in pbar : tmp = asyncio . ensure_future ( self . process_response ( session , url )) tasks . append ( tmp ) interventions [ \"interventions\" ] = await asyncio . gather ( * tasks ) if save and slug_name : path = Path ( save ) / self . legislature_name / \"interventions\" path . mkdir ( parents = True , exist_ok = True ) with open ( path / f \" { slug_name } .json\" , \"w\" ) as f : json . dump ( interventions , f ) return interventions async def async_fetch_all_interventions ( self , urls , save = \"./data/\" , max_interventions = 1000 ): \"\"\"Retrieves all interventions for each deputy and saves them to a file for each deputy. Args: urls (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. save (str): The file path to save the results to. max_interventions (int): The maximum number of interventions to fetch for each deputy. Returns: interventions_dict (dict): A dictionary where the keys are the deputy names and the values are lists of their interventions. \"\"\" deps = self . deputies slugs = [ self . deputies_df [ self . deputies_df [ \"nom\" ] == dep ][ \"slug\" ] . values [ 0 ] for dep in deps ] files = glob ( f \" { save } / { self . legislature_name } /interventions/*.json\" ) files = [ file . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for file in files ] idx = [ i for i , slug in enumerate ( slugs ) if slug not in files ] deps = [ deps [ i ] for i in idx ] slugs = [ slugs [ i ] for i in idx ] print ( f \"Fetching { len ( idx ) } deputies' interventions... Instead of { len ( self . deputies ) } \" ) interventions_dict = {} for dep , slug in tqdm ( zip ( deps , slugs ), total = len ( idx )): try : interventions_dict [ dep ] = await self . async_fetch_interventions_of_deputy ( dep_urls = urls [ dep ], slug_name = slug , max_interventions = max_interventions , verbose = False , save = save , ) except Exception as e : print ( dep , e ) time . sleep ( 60 ) return interventions_dict __init__ ( ptype = 'depute' , legislature = '2017-2022' , format = 'json' ) \u00b6 Initializes the API object. Parameters: Name Type Description Default ptype str The type of parliamentarian to retrieve data for. Valid values are \"depute\" (deputy) or \"senateur\" (senator). 'depute' legislature str The legislature to retrieve data for. Valid values are \"2007-2012\" or \"2012-2017\", \"2017-2022\", or None for current legislature. Note that older legislatures have problems for some processing later on. '2017-2022' format str The format to retrieve data in. Valid values are \"json\" or \"xml\". 'json' Raises: Type Description AssertionError If ptype is not \"depute\" or \"senateur\", or if legislature is not \"2007-2012\", \"2012-2017\", \"2017-2022\", or None. Source code in nlp_assemblee/api.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , ptype = \"depute\" , legislature = \"2017-2022\" , format = \"json\" ): \"\"\"Initializes the API object. Args: ptype (str): The type of parliamentarian to retrieve data for. Valid values are \"depute\" (deputy) or \"senateur\" (senator). legislature (str): The legislature to retrieve data for. Valid values are \"2007-2012\" or \"2012-2017\", \"2017-2022\", or None for current legislature. Note that older legislatures have problems for some processing later on. format (str): The format to retrieve data in. Valid values are \"json\" or \"xml\". Raises: AssertionError: If ptype is not \"depute\" or \"senateur\", or if legislature is not \"2007-2012\", \"2012-2017\", \"2017-2022\", or None. \"\"\" assert ptype in [ \"depute\" , \"senateur\" ] assert legislature in [ \"2007-2012\" , \"2012-2017\" , \"2017-2022\" , None ] self . legislature = legislature self . legislature_name = \"last\" if legislature is None else legislature self . format = format self . ptype = ptype self . ptype_plural = ptype + \"s\" self . prefix = \"www\" if legislature is None else legislature self . base_url = f \"https:// { self . prefix } .nos { self . ptype_plural } .fr\" # Fetches the list of deputies self . parlementaires () # creates self.parlementaires_list # Create the dataframe of deputies and the name of the groups self . get_deputies_df () # creates self.deputies_df and self.groups and self.deputies async_fetch_all_interventions ( urls , save = './data/' , max_interventions = 1000 ) async \u00b6 Retrieves all interventions for each deputy and saves them to a file for each deputy. Parameters: Name Type Description Default urls dict A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. required save str The file path to save the results to. './data/' max_interventions int The maximum number of interventions to fetch for each deputy. 1000 Returns: Name Type Description interventions_dict dict A dictionary where the keys are the deputy names and the values are lists of their interventions. Source code in nlp_assemblee/api.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 async def async_fetch_all_interventions ( self , urls , save = \"./data/\" , max_interventions = 1000 ): \"\"\"Retrieves all interventions for each deputy and saves them to a file for each deputy. Args: urls (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. save (str): The file path to save the results to. max_interventions (int): The maximum number of interventions to fetch for each deputy. Returns: interventions_dict (dict): A dictionary where the keys are the deputy names and the values are lists of their interventions. \"\"\" deps = self . deputies slugs = [ self . deputies_df [ self . deputies_df [ \"nom\" ] == dep ][ \"slug\" ] . values [ 0 ] for dep in deps ] files = glob ( f \" { save } / { self . legislature_name } /interventions/*.json\" ) files = [ file . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for file in files ] idx = [ i for i , slug in enumerate ( slugs ) if slug not in files ] deps = [ deps [ i ] for i in idx ] slugs = [ slugs [ i ] for i in idx ] print ( f \"Fetching { len ( idx ) } deputies' interventions... Instead of { len ( self . deputies ) } \" ) interventions_dict = {} for dep , slug in tqdm ( zip ( deps , slugs ), total = len ( idx )): try : interventions_dict [ dep ] = await self . async_fetch_interventions_of_deputy ( dep_urls = urls [ dep ], slug_name = slug , max_interventions = max_interventions , verbose = False , save = save , ) except Exception as e : print ( dep , e ) time . sleep ( 60 ) return interventions_dict async_fetch_interventions_of_deputy ( dep_urls , slug_name = None , max_interventions = 1000 , verbose = True , save = './data/' ) async \u00b6 Retrieves the interventions of a parliamentarian. Parameters: Name Type Description Default dep_urls dict A dictionary containing the links to the interventions. required slug_name str The slug name of the parliamentarian. None max_interventions int The maximum number of interventions to retrieve. 1000 verbose bool Whether to display a progress bar. True save str The file path to save the results to. './data/' Returns: Name Type Description interventions dict A dictionary containing the interventions. Source code in nlp_assemblee/api.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 async def async_fetch_interventions_of_deputy ( self , dep_urls , slug_name = None , max_interventions = 1000 , verbose = True , save = \"./data/\" ): \"\"\"Retrieves the interventions of a parliamentarian. Args: dep_urls (dict): A dictionary containing the links to the interventions. slug_name (str): The slug name of the parliamentarian. max_interventions (int): The maximum number of interventions to retrieve. verbose (bool): Whether to display a progress bar. save (str):The file path to save the results to. Returns: interventions (dict): A dictionary containing the interventions. \"\"\" # Initialize an empty list to store the interventions interventions = { \"start\" : dep_urls [ \"start\" ], \"end\" : dep_urls [ \"end\" ], \"last_result\" : dep_urls [ \"last_result\" ], \"interventions\" : [], } inters = ( dep_urls [ \"results\" ][: max_interventions ] if max_interventions else dep_urls [ \"results\" ] ) urls = [ intervention [ \"document_url\" ] for intervention in inters ] pbar = tqdm ( urls , leave = False ) if verbose else urls async with aiohttp . ClientSession ( trust_env = True ) as session : # Fetch the interventions from the API tasks = [] for url in pbar : tmp = asyncio . ensure_future ( self . process_response ( session , url )) tasks . append ( tmp ) interventions [ \"interventions\" ] = await asyncio . gather ( * tasks ) if save and slug_name : path = Path ( save ) / self . legislature_name / \"interventions\" path . mkdir ( parents = True , exist_ok = True ) with open ( path / f \" { slug_name } .json\" , \"w\" ) as f : json . dump ( interventions , f ) return interventions async_get_all_interventions_urls ( all_pages = True , max_interventions = 1000 , sort = 0 , count = 500 , object_name = 'Intervention' , page = 1 , verbose = True , save = './data/interventions_urls_by_deputies.json' ) async \u00b6 Retrieves the URLs for all interventions by each deputy. Parameters: Name Type Description Default all_pages bool Whether to retrieve all pages of results or just the first page. True max_interventions int Maximum number of interventions to retrieve. 1000 sort int An integer representing the sorting method to use. 0 count int The number of results to retrieve per page. 500 object_name str The name of the object to retrieve. 'Intervention' page int The page number to start at. 1 verbose bool Whether to display a progress bar. True save str The file path to save the results to. './data/interventions_urls_by_deputies.json' Returns: Name Type Description deputies_list dict A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. Source code in nlp_assemblee/api.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 async def async_get_all_interventions_urls ( self , all_pages = True , max_interventions = 1000 , sort = 0 , count = 500 , object_name = \"Intervention\" , page = 1 , verbose = True , save = \"./data/interventions_urls_by_deputies.json\" , ): \"\"\"Retrieves the URLs for all interventions by each deputy. Args: all_pages (bool, optional): Whether to retrieve all pages of results or just the first page. max_interventions (int, optional): Maximum number of interventions to retrieve. sort (int, optional): An integer representing the sorting method to use. count (int, optional): The number of results to retrieve per page. object_name (str, optional): The name of the object to retrieve. page (int, optional): The page number to start at. verbose (bool, optional): Whether to display a progress bar. save (str, optional): The file path to save the results to. Returns: deputies_list (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. \"\"\" deputies_list = {} async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for dep in self . deputies : tmp = asyncio . ensure_future ( self . async_get_deputy_interventions_urls ( dep_name = dep , session = session , count = count , object_name = object_name , sort = sort , page = page , all_pages = all_pages , max_interventions = max_interventions , ) ) tasks . append ( tmp ) responses = await asyncio . gather ( * tasks ) for dep , response in zip ( self . deputies , responses ): deputies_list [ dep ] = response # Save the results to a file if save path is provided if save : path = Path ( save ) path_list = list ( path . parts ) path_list . insert ( 2 , self . legislature_name ) save_path = Path ( \"\" ) . joinpath ( * path_list [: - 1 ]) save_path . mkdir ( parents = True , exist_ok = True ) with open ( save_path / path_list [ - 1 ], \"w\" ) as f : json . dump ( deputies_list , f ) self . deputies_list_file = str ( save_path / path_list [ - 1 ]) return deputies_list async_get_deputy_interventions_urls ( dep_name , all_pages = False , max_interventions = 1000 , count = 500 , last_int = 500 , sort = 0 , page = 1 ) async \u00b6 Retrieves the urls of the interventions of a parliamentarian. Parameters: Name Type Description Default dep_name str The name of the parliamentarian. required all_pages bool Whether to retrieve all pages of results. False max_interventions int Maximum number of interventions to retrieve. 1000 count int The number of interventions to retrieve per page. 500 last_int int The last intervention number to retrieve. 500 sort int The sort order of the interventions, by pertinence or by date. 0 page int The page of results to retrieve. 1 Returns: Name Type Description response dict A json object containing the links to the interventions. Source code in nlp_assemblee/api.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 async def async_get_deputy_interventions_urls ( self , dep_name , all_pages = False , max_interventions = 1000 , count = 500 , last_int = 500 , sort = 0 , page = 1 , ): \"\"\"Retrieves the urls of the interventions of a parliamentarian. Args: dep_name (str): The name of the parliamentarian. all_pages (bool, optional): Whether to retrieve all pages of results. max_interventions (int, optional): Maximum number of interventions to retrieve. count (int, optional): The number of interventions to retrieve per page. last_int (int, optional): The last intervention number to retrieve. sort (int, optional): The sort order of the interventions, by pertinence or by date. page (int, optional): The page of results to retrieve. Returns: response (dict): A json object containing the links to the interventions. \"\"\" if all_pages or max_interventions : page = 1 # Remove accents and special characters from the name and replace spaces with + name = self . search_parlementaires ( dep_name )[ 0 ][ 0 ][ \"nom\" ] slug_name = re . sub ( \" \" , \"+\" , unidecode . unidecode ( name . lower ())) # Build the URL for the search url = f \" { self . base_url } /recherche?object_name=Intervention&format= { self . format } \" url += f \"&tag=parlementaire%3D { slug_name } &sort= { sort } \" page_param = f \"&page= { page } \" count_param = f \"&count= { count } \" if last_int is None : tick = requests . get ( url + \"&count=0&page=1\" ) . json ()[ \"last_result\" ] if all_pages : last_int = tick else : last_int = min ( max_interventions , tick ) response = { \"start\" : 1 , \"end\" : 0 , \"last_result\" : last_int , \"results\" : []} # Retrieve the search results async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for page in range ( 1 , last_int // count + 2 ): page_param = f \"&page= { page } \" tasks . append ( asyncio . ensure_future ( self . urls_response ( session , url , count_param , page_param )) ) responses = await asyncio . gather ( * tasks ) for tmp in responses : response [ \"results\" ] += tmp [ \"results\" ] response [ \"end\" ] = tmp [ \"end\" ] response [ \"dep_name\" ] = dep_name response [ \"slug_name\" ] = slug_name response [ \"legislature\" ] = self . legislature_name return response get_deputies_df () \u00b6 Retrieves a DataFrame of deputies information from the API. Returns: Name Type Description deputies_df pandas . DataFrame A DataFrame containing information about deputies. Creates self.deputies_df : A DataFrame containing information about deputies self.groups : groups of the deputies self.deputies : Deputies information Source code in nlp_assemblee/api.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def get_deputies_df ( self ): \"\"\"Retrieves a DataFrame of deputies information from the API. Returns: deputies_df (pandas.DataFrame): A DataFrame containing information about deputies. Creates: self.deputies_df : A DataFrame containing information about deputies self.groups : groups of the deputies self.deputies : Deputies information \"\"\" # Retrieve deputies data from the API deputies_json = self . parlementaires_list # Convert the JSON data to a DataFrame cols_to_drop = [ \"sites_web\" , \"emails\" , \"adresses\" , \"collaborateurs\" , \"anciens_autres_mandats\" , \"autres_mandats\" , \"url_an\" , \"id_an\" , \"url_nosdeputes\" , \"url_nosdeputes_api\" , \"twitter\" , ] deputies_df = pd . json_normalize ( deputies_json ) deputies_df = deputies_df . drop ( columns = cols_to_drop , errors = \"ignore\" ) deputies_df [ \"legislature\" ] = self . legislature_name self . deputies_df = deputies_df self . all_groups = deputies_df [ \"groupe_sigle\" ] . unique () self . deputies = deputies_df [ \"nom\" ] . unique () return deputies_df parlementaires ( active = None ) \u00b6 Retrieves a list of parliamentaries. Parameters: Name Type Description Default active bool Whether to retrieve active parliamentaries or not. None Returns: Name Type Description parlementaires_list list A list of parliamentaries. This also creates the attribute self.parlementaires_list. Creates self.parlementaires_list : A list of all parliamentaries Source code in nlp_assemblee/api.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @memoize def parlementaires ( self , active = None ): \"\"\"Retrieves a list of parliamentaries. Args: active (bool): Whether to retrieve active parliamentaries or not. Returns: parlementaires_list (list): A list of parliamentaries. This also creates the attribute self.parlementaires_list. Creates: self.parlementaires_list : A list of all parliamentaries \"\"\" # Build the URL based on the active parameter if active is None : url = f \" { self . base_url } / { self . ptype_plural } / { self . format } \" else : url = f \" { self . base_url } / { self . ptype_plural } /enmandat/ { self . format } \" # Retrieve the data from the API data = requests . get ( url ) . json () # Extract the parliamentaries from the response and return them self . parlementaires_list = [ depute [ self . ptype ] for depute in data [ self . ptype_plural ]] return self . parlementaires_list process_response ( session , url ) async staticmethod \u00b6 Given a url, this function calls the url and returns the json object associated with the url. Parameters: Name Type Description Default session The aiohttp session object required url The url to call required Returns: Name Type Description intervention dict json object containing the intervention details Source code in nlp_assemblee/api.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 @staticmethod async def process_response ( session , url ): \"\"\"Given a url, this function calls the url and returns the json object associated with the url. Args: session : The aiohttp session object url : The url to call Returns: intervention (dict): json object containing the intervention details \"\"\" async with session . get ( url ) as resp : action_item = await resp . json ( content_type = None ) return action_item [ \"intervention\" ] search_parlementaires ( q , field = 'nom' , limit = 5 ) \u00b6 Fuzzy searches for a parliamentarian in the deputies list. Parameters: Name Type Description Default q str The query to search for. required field str The field of the parliamentarian data to search in. 'nom' limit int The maximum number of results to return. 5 Returns: Name Type Description name str The exact name of the parliamentarian in the list. Source code in nlp_assemblee/api.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def search_parlementaires ( self , q , field = \"nom\" , limit = 5 ): \"\"\"Fuzzy searches for a parliamentarian in the deputies list. Args: q (str): The query to search for. field (str): The field of the parliamentarian data to search in. limit (int): The maximum number of results to return. Returns: name (str): The exact name of the parliamentarian in the list. \"\"\" # Search for parliamentarians and return the best matches name = extractBests ( q , self . parlementaires_list , processor = lambda x : x [ field ] if type ( x ) == dict else x , limit = limit , ) return name urls_response ( session , url , count_param , page_param ) async staticmethod \u00b6 Retrieves the json response from a url. Parameters: Name Type Description Default session aiohttp . ClientSession The session to use for the request required url str The url to retrieve the response from required count_param str The parameter for the number of results in the url required page_param str The parameter for the page number in the url required Returns: Name Type Description response dict The json response from the url. Returns an empty dict if there is an error. The dict contains keys (start, end, last_result, results) Source code in nlp_assemblee/api.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 @staticmethod async def urls_response ( session , url , count_param , page_param ): \"\"\"Retrieves the json response from a url. Args: session (aiohttp.ClientSession): The session to use for the request url (str): The url to retrieve the response from count_param (str): The parameter for the number of results in the url page_param (str): The parameter for the page number in the url Returns: response (dict): The json response from the url. Returns an empty dict if there is an error. The dict contains keys (start, end, last_result, results) \"\"\" async with session . get ( url + count_param + page_param ) as resp : try : tmp = await resp . json ( content_type = None ) except Exception as e : print ( e ) return { \"start\" : 1 , \"end\" : 0 , \"last_result\" : 0 , \"results\" : [], } return tmp","title":"API"},{"location":"api/api/#getting-the-data","text":"For this project, we used data made openly available by assemblee-nationale.fr/ . The site nosdeputes.fr wrote a API to simplify the acces to the data. We used this website to fetch the textual data used in the project.","title":"Getting the data"},{"location":"api/api/#wrapper-function","text":"Wrapper function We defined a wrapper function to simplify the use of the API. It fetches all the urls, interventions and the deputies information for a given legislature. It is used to fetch the data in the notebook . Fetch data for a given legislature, including interventions and deputies data. Parameters: Name Type Description Default legislature str the legislature to fetch data for. '2017-2022' root_dir str the root directory to save data. './data/' max_interventions int the maximum number of interventions to fetch. 500 Returns: Name Type Description uresults tuple containing the urls of the interventions, the interventions data and the deputies data Source code in nlp_assemblee/api.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 async def fetch_data_for_legislature ( legislature = \"2017-2022\" , root_dir = \"./data/\" , max_interventions = 500 ): \"\"\"Fetch data for a given legislature, including interventions and deputies data. Args: legislature (str): the legislature to fetch data for. root_dir (str): the root directory to save data. max_interventions (int): the maximum number of interventions to fetch. Returns: uresults (tuple): containing the urls of the interventions, the interventions data and the deputies data \"\"\" api = CPCApi ( legislature = legislature ) deps_df = api . deputies_df urls = await api . async_get_all_interventions_urls ( all_pages = False , max_interventions = max_interventions , sort = 0 , count = 500 , object_name = \"Intervention\" , page = 1 , verbose = True , save = f \" { root_dir } /interventions_urls_by_deputies.json\" , ) interventions = await api . async_fetch_all_interventions ( urls , save = root_dir , max_interventions = max_interventions ) return urls , interventions , deps_df","title":"Wrapper function"},{"location":"api/api/#cpcapi","text":"API class The wrapper function uses an API class inspired by the GitHub of nosdeputes.fr . Bases: object The CPCApi class provides an interface for interacting with nosdeputes.fr's API. It allows users to retrieve information about parliamentarians, as well as the interventions they have made. The class provides asynchronous methods for fetching data, as well as methods for processing and saving the data. Additionally, the class contains methods for handling pagination and error handling. The class also provides a way to save the result of the request to a folder or file. The class uses aiohttp, asyncio and json library to make the request, parse the json and save the result to a file. Source code in nlp_assemblee/api.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 class CPCApi ( object ): \"\"\"The CPCApi class provides an interface for interacting with nosdeputes.fr's API. It allows users to retrieve information about parliamentarians, as well as the interventions they have made. The class provides asynchronous methods for fetching data, as well as methods for processing and saving the data. Additionally, the class contains methods for handling pagination and error handling. The class also provides a way to save the result of the request to a folder or file. The class uses aiohttp, asyncio and json library to make the request, parse the json and save the result to a file. \"\"\" def __init__ ( self , ptype = \"depute\" , legislature = \"2017-2022\" , format = \"json\" ): \"\"\"Initializes the API object. Args: ptype (str): The type of parliamentarian to retrieve data for. Valid values are \"depute\" (deputy) or \"senateur\" (senator). legislature (str): The legislature to retrieve data for. Valid values are \"2007-2012\" or \"2012-2017\", \"2017-2022\", or None for current legislature. Note that older legislatures have problems for some processing later on. format (str): The format to retrieve data in. Valid values are \"json\" or \"xml\". Raises: AssertionError: If ptype is not \"depute\" or \"senateur\", or if legislature is not \"2007-2012\", \"2012-2017\", \"2017-2022\", or None. \"\"\" assert ptype in [ \"depute\" , \"senateur\" ] assert legislature in [ \"2007-2012\" , \"2012-2017\" , \"2017-2022\" , None ] self . legislature = legislature self . legislature_name = \"last\" if legislature is None else legislature self . format = format self . ptype = ptype self . ptype_plural = ptype + \"s\" self . prefix = \"www\" if legislature is None else legislature self . base_url = f \"https:// { self . prefix } .nos { self . ptype_plural } .fr\" # Fetches the list of deputies self . parlementaires () # creates self.parlementaires_list # Create the dataframe of deputies and the name of the groups self . get_deputies_df () # creates self.deputies_df and self.groups and self.deputies @memoize def parlementaires ( self , active = None ): \"\"\"Retrieves a list of parliamentaries. Args: active (bool): Whether to retrieve active parliamentaries or not. Returns: parlementaires_list (list): A list of parliamentaries. This also creates the attribute self.parlementaires_list. Creates: self.parlementaires_list : A list of all parliamentaries \"\"\" # Build the URL based on the active parameter if active is None : url = f \" { self . base_url } / { self . ptype_plural } / { self . format } \" else : url = f \" { self . base_url } / { self . ptype_plural } /enmandat/ { self . format } \" # Retrieve the data from the API data = requests . get ( url ) . json () # Extract the parliamentaries from the response and return them self . parlementaires_list = [ depute [ self . ptype ] for depute in data [ self . ptype_plural ]] return self . parlementaires_list def search_parlementaires ( self , q , field = \"nom\" , limit = 5 ): \"\"\"Fuzzy searches for a parliamentarian in the deputies list. Args: q (str): The query to search for. field (str): The field of the parliamentarian data to search in. limit (int): The maximum number of results to return. Returns: name (str): The exact name of the parliamentarian in the list. \"\"\" # Search for parliamentarians and return the best matches name = extractBests ( q , self . parlementaires_list , processor = lambda x : x [ field ] if type ( x ) == dict else x , limit = limit , ) return name def get_deputies_df ( self ): \"\"\"Retrieves a DataFrame of deputies information from the API. Returns: deputies_df (pandas.DataFrame): A DataFrame containing information about deputies. Creates: self.deputies_df : A DataFrame containing information about deputies self.groups : groups of the deputies self.deputies : Deputies information \"\"\" # Retrieve deputies data from the API deputies_json = self . parlementaires_list # Convert the JSON data to a DataFrame cols_to_drop = [ \"sites_web\" , \"emails\" , \"adresses\" , \"collaborateurs\" , \"anciens_autres_mandats\" , \"autres_mandats\" , \"url_an\" , \"id_an\" , \"url_nosdeputes\" , \"url_nosdeputes_api\" , \"twitter\" , ] deputies_df = pd . json_normalize ( deputies_json ) deputies_df = deputies_df . drop ( columns = cols_to_drop , errors = \"ignore\" ) deputies_df [ \"legislature\" ] = self . legislature_name self . deputies_df = deputies_df self . all_groups = deputies_df [ \"groupe_sigle\" ] . unique () self . deputies = deputies_df [ \"nom\" ] . unique () return deputies_df @staticmethod async def urls_response ( session , url , count_param , page_param ): \"\"\"Retrieves the json response from a url. Args: session (aiohttp.ClientSession): The session to use for the request url (str): The url to retrieve the response from count_param (str): The parameter for the number of results in the url page_param (str): The parameter for the page number in the url Returns: response (dict): The json response from the url. Returns an empty dict if there is an error. The dict contains keys (start, end, last_result, results) \"\"\" async with session . get ( url + count_param + page_param ) as resp : try : tmp = await resp . json ( content_type = None ) except Exception as e : print ( e ) return { \"start\" : 1 , \"end\" : 0 , \"last_result\" : 0 , \"results\" : [], } return tmp async def async_get_deputy_interventions_urls ( self , dep_name , all_pages = False , max_interventions = 1000 , count = 500 , last_int = 500 , sort = 0 , page = 1 , ): \"\"\"Retrieves the urls of the interventions of a parliamentarian. Args: dep_name (str): The name of the parliamentarian. all_pages (bool, optional): Whether to retrieve all pages of results. max_interventions (int, optional): Maximum number of interventions to retrieve. count (int, optional): The number of interventions to retrieve per page. last_int (int, optional): The last intervention number to retrieve. sort (int, optional): The sort order of the interventions, by pertinence or by date. page (int, optional): The page of results to retrieve. Returns: response (dict): A json object containing the links to the interventions. \"\"\" if all_pages or max_interventions : page = 1 # Remove accents and special characters from the name and replace spaces with + name = self . search_parlementaires ( dep_name )[ 0 ][ 0 ][ \"nom\" ] slug_name = re . sub ( \" \" , \"+\" , unidecode . unidecode ( name . lower ())) # Build the URL for the search url = f \" { self . base_url } /recherche?object_name=Intervention&format= { self . format } \" url += f \"&tag=parlementaire%3D { slug_name } &sort= { sort } \" page_param = f \"&page= { page } \" count_param = f \"&count= { count } \" if last_int is None : tick = requests . get ( url + \"&count=0&page=1\" ) . json ()[ \"last_result\" ] if all_pages : last_int = tick else : last_int = min ( max_interventions , tick ) response = { \"start\" : 1 , \"end\" : 0 , \"last_result\" : last_int , \"results\" : []} # Retrieve the search results async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for page in range ( 1 , last_int // count + 2 ): page_param = f \"&page= { page } \" tasks . append ( asyncio . ensure_future ( self . urls_response ( session , url , count_param , page_param )) ) responses = await asyncio . gather ( * tasks ) for tmp in responses : response [ \"results\" ] += tmp [ \"results\" ] response [ \"end\" ] = tmp [ \"end\" ] response [ \"dep_name\" ] = dep_name response [ \"slug_name\" ] = slug_name response [ \"legislature\" ] = self . legislature_name return response async def async_get_all_interventions_urls ( self , all_pages = True , max_interventions = 1000 , sort = 0 , count = 500 , object_name = \"Intervention\" , page = 1 , verbose = True , save = \"./data/interventions_urls_by_deputies.json\" , ): \"\"\"Retrieves the URLs for all interventions by each deputy. Args: all_pages (bool, optional): Whether to retrieve all pages of results or just the first page. max_interventions (int, optional): Maximum number of interventions to retrieve. sort (int, optional): An integer representing the sorting method to use. count (int, optional): The number of results to retrieve per page. object_name (str, optional): The name of the object to retrieve. page (int, optional): The page number to start at. verbose (bool, optional): Whether to display a progress bar. save (str, optional): The file path to save the results to. Returns: deputies_list (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. \"\"\" deputies_list = {} async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for dep in self . deputies : tmp = asyncio . ensure_future ( self . async_get_deputy_interventions_urls ( dep_name = dep , session = session , count = count , object_name = object_name , sort = sort , page = page , all_pages = all_pages , max_interventions = max_interventions , ) ) tasks . append ( tmp ) responses = await asyncio . gather ( * tasks ) for dep , response in zip ( self . deputies , responses ): deputies_list [ dep ] = response # Save the results to a file if save path is provided if save : path = Path ( save ) path_list = list ( path . parts ) path_list . insert ( 2 , self . legislature_name ) save_path = Path ( \"\" ) . joinpath ( * path_list [: - 1 ]) save_path . mkdir ( parents = True , exist_ok = True ) with open ( save_path / path_list [ - 1 ], \"w\" ) as f : json . dump ( deputies_list , f ) self . deputies_list_file = str ( save_path / path_list [ - 1 ]) return deputies_list @staticmethod async def process_response ( session , url ): \"\"\"Given a url, this function calls the url and returns the json object associated with the url. Args: session : The aiohttp session object url : The url to call Returns: intervention (dict): json object containing the intervention details \"\"\" async with session . get ( url ) as resp : action_item = await resp . json ( content_type = None ) return action_item [ \"intervention\" ] async def async_fetch_interventions_of_deputy ( self , dep_urls , slug_name = None , max_interventions = 1000 , verbose = True , save = \"./data/\" ): \"\"\"Retrieves the interventions of a parliamentarian. Args: dep_urls (dict): A dictionary containing the links to the interventions. slug_name (str): The slug name of the parliamentarian. max_interventions (int): The maximum number of interventions to retrieve. verbose (bool): Whether to display a progress bar. save (str):The file path to save the results to. Returns: interventions (dict): A dictionary containing the interventions. \"\"\" # Initialize an empty list to store the interventions interventions = { \"start\" : dep_urls [ \"start\" ], \"end\" : dep_urls [ \"end\" ], \"last_result\" : dep_urls [ \"last_result\" ], \"interventions\" : [], } inters = ( dep_urls [ \"results\" ][: max_interventions ] if max_interventions else dep_urls [ \"results\" ] ) urls = [ intervention [ \"document_url\" ] for intervention in inters ] pbar = tqdm ( urls , leave = False ) if verbose else urls async with aiohttp . ClientSession ( trust_env = True ) as session : # Fetch the interventions from the API tasks = [] for url in pbar : tmp = asyncio . ensure_future ( self . process_response ( session , url )) tasks . append ( tmp ) interventions [ \"interventions\" ] = await asyncio . gather ( * tasks ) if save and slug_name : path = Path ( save ) / self . legislature_name / \"interventions\" path . mkdir ( parents = True , exist_ok = True ) with open ( path / f \" { slug_name } .json\" , \"w\" ) as f : json . dump ( interventions , f ) return interventions async def async_fetch_all_interventions ( self , urls , save = \"./data/\" , max_interventions = 1000 ): \"\"\"Retrieves all interventions for each deputy and saves them to a file for each deputy. Args: urls (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. save (str): The file path to save the results to. max_interventions (int): The maximum number of interventions to fetch for each deputy. Returns: interventions_dict (dict): A dictionary where the keys are the deputy names and the values are lists of their interventions. \"\"\" deps = self . deputies slugs = [ self . deputies_df [ self . deputies_df [ \"nom\" ] == dep ][ \"slug\" ] . values [ 0 ] for dep in deps ] files = glob ( f \" { save } / { self . legislature_name } /interventions/*.json\" ) files = [ file . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for file in files ] idx = [ i for i , slug in enumerate ( slugs ) if slug not in files ] deps = [ deps [ i ] for i in idx ] slugs = [ slugs [ i ] for i in idx ] print ( f \"Fetching { len ( idx ) } deputies' interventions... Instead of { len ( self . deputies ) } \" ) interventions_dict = {} for dep , slug in tqdm ( zip ( deps , slugs ), total = len ( idx )): try : interventions_dict [ dep ] = await self . async_fetch_interventions_of_deputy ( dep_urls = urls [ dep ], slug_name = slug , max_interventions = max_interventions , verbose = False , save = save , ) except Exception as e : print ( dep , e ) time . sleep ( 60 ) return interventions_dict","title":"CPCApi"},{"location":"api/api/#nlp_assemblee.api.CPCApi.__init__","text":"Initializes the API object. Parameters: Name Type Description Default ptype str The type of parliamentarian to retrieve data for. Valid values are \"depute\" (deputy) or \"senateur\" (senator). 'depute' legislature str The legislature to retrieve data for. Valid values are \"2007-2012\" or \"2012-2017\", \"2017-2022\", or None for current legislature. Note that older legislatures have problems for some processing later on. '2017-2022' format str The format to retrieve data in. Valid values are \"json\" or \"xml\". 'json' Raises: Type Description AssertionError If ptype is not \"depute\" or \"senateur\", or if legislature is not \"2007-2012\", \"2012-2017\", \"2017-2022\", or None. Source code in nlp_assemblee/api.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , ptype = \"depute\" , legislature = \"2017-2022\" , format = \"json\" ): \"\"\"Initializes the API object. Args: ptype (str): The type of parliamentarian to retrieve data for. Valid values are \"depute\" (deputy) or \"senateur\" (senator). legislature (str): The legislature to retrieve data for. Valid values are \"2007-2012\" or \"2012-2017\", \"2017-2022\", or None for current legislature. Note that older legislatures have problems for some processing later on. format (str): The format to retrieve data in. Valid values are \"json\" or \"xml\". Raises: AssertionError: If ptype is not \"depute\" or \"senateur\", or if legislature is not \"2007-2012\", \"2012-2017\", \"2017-2022\", or None. \"\"\" assert ptype in [ \"depute\" , \"senateur\" ] assert legislature in [ \"2007-2012\" , \"2012-2017\" , \"2017-2022\" , None ] self . legislature = legislature self . legislature_name = \"last\" if legislature is None else legislature self . format = format self . ptype = ptype self . ptype_plural = ptype + \"s\" self . prefix = \"www\" if legislature is None else legislature self . base_url = f \"https:// { self . prefix } .nos { self . ptype_plural } .fr\" # Fetches the list of deputies self . parlementaires () # creates self.parlementaires_list # Create the dataframe of deputies and the name of the groups self . get_deputies_df () # creates self.deputies_df and self.groups and self.deputies","title":"__init__()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.async_fetch_all_interventions","text":"Retrieves all interventions for each deputy and saves them to a file for each deputy. Parameters: Name Type Description Default urls dict A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. required save str The file path to save the results to. './data/' max_interventions int The maximum number of interventions to fetch for each deputy. 1000 Returns: Name Type Description interventions_dict dict A dictionary where the keys are the deputy names and the values are lists of their interventions. Source code in nlp_assemblee/api.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 async def async_fetch_all_interventions ( self , urls , save = \"./data/\" , max_interventions = 1000 ): \"\"\"Retrieves all interventions for each deputy and saves them to a file for each deputy. Args: urls (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. save (str): The file path to save the results to. max_interventions (int): The maximum number of interventions to fetch for each deputy. Returns: interventions_dict (dict): A dictionary where the keys are the deputy names and the values are lists of their interventions. \"\"\" deps = self . deputies slugs = [ self . deputies_df [ self . deputies_df [ \"nom\" ] == dep ][ \"slug\" ] . values [ 0 ] for dep in deps ] files = glob ( f \" { save } / { self . legislature_name } /interventions/*.json\" ) files = [ file . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for file in files ] idx = [ i for i , slug in enumerate ( slugs ) if slug not in files ] deps = [ deps [ i ] for i in idx ] slugs = [ slugs [ i ] for i in idx ] print ( f \"Fetching { len ( idx ) } deputies' interventions... Instead of { len ( self . deputies ) } \" ) interventions_dict = {} for dep , slug in tqdm ( zip ( deps , slugs ), total = len ( idx )): try : interventions_dict [ dep ] = await self . async_fetch_interventions_of_deputy ( dep_urls = urls [ dep ], slug_name = slug , max_interventions = max_interventions , verbose = False , save = save , ) except Exception as e : print ( dep , e ) time . sleep ( 60 ) return interventions_dict","title":"async_fetch_all_interventions()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.async_fetch_interventions_of_deputy","text":"Retrieves the interventions of a parliamentarian. Parameters: Name Type Description Default dep_urls dict A dictionary containing the links to the interventions. required slug_name str The slug name of the parliamentarian. None max_interventions int The maximum number of interventions to retrieve. 1000 verbose bool Whether to display a progress bar. True save str The file path to save the results to. './data/' Returns: Name Type Description interventions dict A dictionary containing the interventions. Source code in nlp_assemblee/api.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 async def async_fetch_interventions_of_deputy ( self , dep_urls , slug_name = None , max_interventions = 1000 , verbose = True , save = \"./data/\" ): \"\"\"Retrieves the interventions of a parliamentarian. Args: dep_urls (dict): A dictionary containing the links to the interventions. slug_name (str): The slug name of the parliamentarian. max_interventions (int): The maximum number of interventions to retrieve. verbose (bool): Whether to display a progress bar. save (str):The file path to save the results to. Returns: interventions (dict): A dictionary containing the interventions. \"\"\" # Initialize an empty list to store the interventions interventions = { \"start\" : dep_urls [ \"start\" ], \"end\" : dep_urls [ \"end\" ], \"last_result\" : dep_urls [ \"last_result\" ], \"interventions\" : [], } inters = ( dep_urls [ \"results\" ][: max_interventions ] if max_interventions else dep_urls [ \"results\" ] ) urls = [ intervention [ \"document_url\" ] for intervention in inters ] pbar = tqdm ( urls , leave = False ) if verbose else urls async with aiohttp . ClientSession ( trust_env = True ) as session : # Fetch the interventions from the API tasks = [] for url in pbar : tmp = asyncio . ensure_future ( self . process_response ( session , url )) tasks . append ( tmp ) interventions [ \"interventions\" ] = await asyncio . gather ( * tasks ) if save and slug_name : path = Path ( save ) / self . legislature_name / \"interventions\" path . mkdir ( parents = True , exist_ok = True ) with open ( path / f \" { slug_name } .json\" , \"w\" ) as f : json . dump ( interventions , f ) return interventions","title":"async_fetch_interventions_of_deputy()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.async_get_all_interventions_urls","text":"Retrieves the URLs for all interventions by each deputy. Parameters: Name Type Description Default all_pages bool Whether to retrieve all pages of results or just the first page. True max_interventions int Maximum number of interventions to retrieve. 1000 sort int An integer representing the sorting method to use. 0 count int The number of results to retrieve per page. 500 object_name str The name of the object to retrieve. 'Intervention' page int The page number to start at. 1 verbose bool Whether to display a progress bar. True save str The file path to save the results to. './data/interventions_urls_by_deputies.json' Returns: Name Type Description deputies_list dict A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. Source code in nlp_assemblee/api.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 async def async_get_all_interventions_urls ( self , all_pages = True , max_interventions = 1000 , sort = 0 , count = 500 , object_name = \"Intervention\" , page = 1 , verbose = True , save = \"./data/interventions_urls_by_deputies.json\" , ): \"\"\"Retrieves the URLs for all interventions by each deputy. Args: all_pages (bool, optional): Whether to retrieve all pages of results or just the first page. max_interventions (int, optional): Maximum number of interventions to retrieve. sort (int, optional): An integer representing the sorting method to use. count (int, optional): The number of results to retrieve per page. object_name (str, optional): The name of the object to retrieve. page (int, optional): The page number to start at. verbose (bool, optional): Whether to display a progress bar. save (str, optional): The file path to save the results to. Returns: deputies_list (dict): A dictionary where the keys are deputy names and the values are lists of URLs for their interventions. \"\"\" deputies_list = {} async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for dep in self . deputies : tmp = asyncio . ensure_future ( self . async_get_deputy_interventions_urls ( dep_name = dep , session = session , count = count , object_name = object_name , sort = sort , page = page , all_pages = all_pages , max_interventions = max_interventions , ) ) tasks . append ( tmp ) responses = await asyncio . gather ( * tasks ) for dep , response in zip ( self . deputies , responses ): deputies_list [ dep ] = response # Save the results to a file if save path is provided if save : path = Path ( save ) path_list = list ( path . parts ) path_list . insert ( 2 , self . legislature_name ) save_path = Path ( \"\" ) . joinpath ( * path_list [: - 1 ]) save_path . mkdir ( parents = True , exist_ok = True ) with open ( save_path / path_list [ - 1 ], \"w\" ) as f : json . dump ( deputies_list , f ) self . deputies_list_file = str ( save_path / path_list [ - 1 ]) return deputies_list","title":"async_get_all_interventions_urls()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.async_get_deputy_interventions_urls","text":"Retrieves the urls of the interventions of a parliamentarian. Parameters: Name Type Description Default dep_name str The name of the parliamentarian. required all_pages bool Whether to retrieve all pages of results. False max_interventions int Maximum number of interventions to retrieve. 1000 count int The number of interventions to retrieve per page. 500 last_int int The last intervention number to retrieve. 500 sort int The sort order of the interventions, by pertinence or by date. 0 page int The page of results to retrieve. 1 Returns: Name Type Description response dict A json object containing the links to the interventions. Source code in nlp_assemblee/api.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 async def async_get_deputy_interventions_urls ( self , dep_name , all_pages = False , max_interventions = 1000 , count = 500 , last_int = 500 , sort = 0 , page = 1 , ): \"\"\"Retrieves the urls of the interventions of a parliamentarian. Args: dep_name (str): The name of the parliamentarian. all_pages (bool, optional): Whether to retrieve all pages of results. max_interventions (int, optional): Maximum number of interventions to retrieve. count (int, optional): The number of interventions to retrieve per page. last_int (int, optional): The last intervention number to retrieve. sort (int, optional): The sort order of the interventions, by pertinence or by date. page (int, optional): The page of results to retrieve. Returns: response (dict): A json object containing the links to the interventions. \"\"\" if all_pages or max_interventions : page = 1 # Remove accents and special characters from the name and replace spaces with + name = self . search_parlementaires ( dep_name )[ 0 ][ 0 ][ \"nom\" ] slug_name = re . sub ( \" \" , \"+\" , unidecode . unidecode ( name . lower ())) # Build the URL for the search url = f \" { self . base_url } /recherche?object_name=Intervention&format= { self . format } \" url += f \"&tag=parlementaire%3D { slug_name } &sort= { sort } \" page_param = f \"&page= { page } \" count_param = f \"&count= { count } \" if last_int is None : tick = requests . get ( url + \"&count=0&page=1\" ) . json ()[ \"last_result\" ] if all_pages : last_int = tick else : last_int = min ( max_interventions , tick ) response = { \"start\" : 1 , \"end\" : 0 , \"last_result\" : last_int , \"results\" : []} # Retrieve the search results async with aiohttp . ClientSession ( trust_env = True ) as session : tasks = [] for page in range ( 1 , last_int // count + 2 ): page_param = f \"&page= { page } \" tasks . append ( asyncio . ensure_future ( self . urls_response ( session , url , count_param , page_param )) ) responses = await asyncio . gather ( * tasks ) for tmp in responses : response [ \"results\" ] += tmp [ \"results\" ] response [ \"end\" ] = tmp [ \"end\" ] response [ \"dep_name\" ] = dep_name response [ \"slug_name\" ] = slug_name response [ \"legislature\" ] = self . legislature_name return response","title":"async_get_deputy_interventions_urls()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.get_deputies_df","text":"Retrieves a DataFrame of deputies information from the API. Returns: Name Type Description deputies_df pandas . DataFrame A DataFrame containing information about deputies. Creates self.deputies_df : A DataFrame containing information about deputies self.groups : groups of the deputies self.deputies : Deputies information Source code in nlp_assemblee/api.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def get_deputies_df ( self ): \"\"\"Retrieves a DataFrame of deputies information from the API. Returns: deputies_df (pandas.DataFrame): A DataFrame containing information about deputies. Creates: self.deputies_df : A DataFrame containing information about deputies self.groups : groups of the deputies self.deputies : Deputies information \"\"\" # Retrieve deputies data from the API deputies_json = self . parlementaires_list # Convert the JSON data to a DataFrame cols_to_drop = [ \"sites_web\" , \"emails\" , \"adresses\" , \"collaborateurs\" , \"anciens_autres_mandats\" , \"autres_mandats\" , \"url_an\" , \"id_an\" , \"url_nosdeputes\" , \"url_nosdeputes_api\" , \"twitter\" , ] deputies_df = pd . json_normalize ( deputies_json ) deputies_df = deputies_df . drop ( columns = cols_to_drop , errors = \"ignore\" ) deputies_df [ \"legislature\" ] = self . legislature_name self . deputies_df = deputies_df self . all_groups = deputies_df [ \"groupe_sigle\" ] . unique () self . deputies = deputies_df [ \"nom\" ] . unique () return deputies_df","title":"get_deputies_df()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.parlementaires","text":"Retrieves a list of parliamentaries. Parameters: Name Type Description Default active bool Whether to retrieve active parliamentaries or not. None Returns: Name Type Description parlementaires_list list A list of parliamentaries. This also creates the attribute self.parlementaires_list. Creates self.parlementaires_list : A list of all parliamentaries Source code in nlp_assemblee/api.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @memoize def parlementaires ( self , active = None ): \"\"\"Retrieves a list of parliamentaries. Args: active (bool): Whether to retrieve active parliamentaries or not. Returns: parlementaires_list (list): A list of parliamentaries. This also creates the attribute self.parlementaires_list. Creates: self.parlementaires_list : A list of all parliamentaries \"\"\" # Build the URL based on the active parameter if active is None : url = f \" { self . base_url } / { self . ptype_plural } / { self . format } \" else : url = f \" { self . base_url } / { self . ptype_plural } /enmandat/ { self . format } \" # Retrieve the data from the API data = requests . get ( url ) . json () # Extract the parliamentaries from the response and return them self . parlementaires_list = [ depute [ self . ptype ] for depute in data [ self . ptype_plural ]] return self . parlementaires_list","title":"parlementaires()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.process_response","text":"Given a url, this function calls the url and returns the json object associated with the url. Parameters: Name Type Description Default session The aiohttp session object required url The url to call required Returns: Name Type Description intervention dict json object containing the intervention details Source code in nlp_assemblee/api.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 @staticmethod async def process_response ( session , url ): \"\"\"Given a url, this function calls the url and returns the json object associated with the url. Args: session : The aiohttp session object url : The url to call Returns: intervention (dict): json object containing the intervention details \"\"\" async with session . get ( url ) as resp : action_item = await resp . json ( content_type = None ) return action_item [ \"intervention\" ]","title":"process_response()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.search_parlementaires","text":"Fuzzy searches for a parliamentarian in the deputies list. Parameters: Name Type Description Default q str The query to search for. required field str The field of the parliamentarian data to search in. 'nom' limit int The maximum number of results to return. 5 Returns: Name Type Description name str The exact name of the parliamentarian in the list. Source code in nlp_assemblee/api.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def search_parlementaires ( self , q , field = \"nom\" , limit = 5 ): \"\"\"Fuzzy searches for a parliamentarian in the deputies list. Args: q (str): The query to search for. field (str): The field of the parliamentarian data to search in. limit (int): The maximum number of results to return. Returns: name (str): The exact name of the parliamentarian in the list. \"\"\" # Search for parliamentarians and return the best matches name = extractBests ( q , self . parlementaires_list , processor = lambda x : x [ field ] if type ( x ) == dict else x , limit = limit , ) return name","title":"search_parlementaires()"},{"location":"api/api/#nlp_assemblee.api.CPCApi.urls_response","text":"Retrieves the json response from a url. Parameters: Name Type Description Default session aiohttp . ClientSession The session to use for the request required url str The url to retrieve the response from required count_param str The parameter for the number of results in the url required page_param str The parameter for the page number in the url required Returns: Name Type Description response dict The json response from the url. Returns an empty dict if there is an error. The dict contains keys (start, end, last_result, results) Source code in nlp_assemblee/api.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 @staticmethod async def urls_response ( session , url , count_param , page_param ): \"\"\"Retrieves the json response from a url. Args: session (aiohttp.ClientSession): The session to use for the request url (str): The url to retrieve the response from count_param (str): The parameter for the number of results in the url page_param (str): The parameter for the page number in the url Returns: response (dict): The json response from the url. Returns an empty dict if there is an error. The dict contains keys (start, end, last_result, results) \"\"\" async with session . get ( url + count_param + page_param ) as resp : try : tmp = await resp . json ( content_type = None ) except Exception as e : print ( e ) return { \"start\" : 1 , \"end\" : 0 , \"last_result\" : 0 , \"results\" : [], } return tmp","title":"urls_response()"}]}