{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from nlp_assemblee.simple_datasets import get_dataloader, get_single_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "colors = sns.color_palette(\"deep\")\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier,\n",
    "        optimizer_type=\"Adam\",\n",
    "        learning_rate=1e-3,\n",
    "        optimizer_kwargs={},\n",
    "        criterion_type=\"CrossEntropyLoss\",\n",
    "        batch_size=256,\n",
    "        loader_kwargs={\n",
    "            \"root\": \"../../../data/\",\n",
    "            \"bert_type\": \"camembert\",\n",
    "            \"text_vars\": [\"intervention\"],\n",
    "            \"use_features\": False,\n",
    "            \"label_var\": \"label\",\n",
    "            \"num_workers\": 12,\n",
    "            \"prefetch_factor\": 4,\n",
    "            \"pin_memory\": True,\n",
    "        },\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.criterion_type = criterion_type\n",
    "        self.batch_size = batch_size\n",
    "        self.loader_kwargs = loader_kwargs\n",
    "        self.save_hyperparameters(ignore=[\"classifier\", \"criterion\"])\n",
    "\n",
    "        if criterion_type == \"CrossEntropyLoss\":\n",
    "            self.criterion = nn.functional.cross_entropy\n",
    "\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_type == \"SGD\":\n",
    "            optimizer = optim.SGD(\n",
    "                self.classifier.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n",
    "            )\n",
    "        elif self.optimizer_type == \"Adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                self.classifier.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n",
    "            )\n",
    "        return optimizer\n",
    "\n",
    "    def get_loss(self, batch, model_type=\"train\"):\n",
    "        x, y = batch\n",
    "        z = self.classifier(x)\n",
    "        loss = nn.CrossEntropyLoss()(z, y)\n",
    "        self.log(f\"{model_type}_loss\", loss, prog_bar=(model_type == \"val\"))\n",
    "\n",
    "        _, predicted = z.max(1)\n",
    "        accuracy = predicted.eq(y).sum().item() / y.size(0)\n",
    "        self.log(f\"{model_type}_accuracy\", accuracy, prog_bar=(model_type == \"val\"))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tain_loss = self.get_loss(batch, model_type=\"train\")\n",
    "        return tain_loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        val_loss = self.get_loss(val_batch, model_type=\"val\")\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        test_loss = self.get_loss(test_batch, model_type=\"test\")\n",
    "        return test_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        _, loader, _ = get_single_dataloader(\n",
    "            phase=\"train\", batch_size=self.batch_size, **self.loader_kwargs\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        _, loader, _ = get_single_dataloader(\n",
    "            phase=\"val\", batch_size=self.batch_size, **self.loader_kwargs\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        _, loader, _ = get_single_dataloader(\n",
    "            phase=\"test\", batch_size=self.batch_size, **self.loader_kwargs\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 3),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = x[\"text\"][\"intervention\"].to(device)\n",
    "        x_ = self.fc(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimplestBert()\n",
    "\n",
    "lit_model = LitModel(\n",
    "    net,\n",
    "    optimizer_type=\"Adam\",\n",
    "    learning_rate=5e-3,\n",
    "    optimizer_kwargs={},\n",
    "    criterion_type=\"CrossEntropyLoss\",\n",
    "    batch_size=256,\n",
    "    loader_kwargs={\n",
    "        \"root\": \"../../../data/\",\n",
    "        \"bert_type\": \"bert\",\n",
    "        \"text_vars\": [\"intervention\"],\n",
    "        \"use_features\": False,\n",
    "        \"label_var\": \"label\",\n",
    "        \"num_workers\": 12,\n",
    "        \"prefetch_factor\": 4,\n",
    "        \"pin_memory\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # profiler=\"simple\",\n",
    "    max_epochs=100,\n",
    "    default_root_dir=\"../../../\",\n",
    "    # fast_dev_run=True,\n",
    "    # overfit_batches=1,\n",
    "    # auto_scale_batch_size=\"binsearch\",\n",
    "    # auto_lr_find=True,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", check_finite=True, patience=10),\n",
    "        callbacks.ModelSummary(max_depth=-1),\n",
    "        callbacks.Timer(duration=\"00:03:00:00\", interval=\"epoch\"),  # Max three hours\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type         | Params\n",
      "-------------------------------------------------\n",
      "0 | classifier      | SimplestBert | 788 K \n",
      "1 | classifier.fc   | Sequential   | 788 K \n",
      "2 | classifier.fc.0 | Linear       | 393 K \n",
      "3 | classifier.fc.1 | LeakyReLU    | 0     \n",
      "4 | classifier.fc.2 | Linear       | 262 K \n",
      "5 | classifier.fc.3 | LeakyReLU    | 0     \n",
      "6 | classifier.fc.4 | Linear       | 131 K \n",
      "7 | classifier.fc.5 | LeakyReLU    | 0     \n",
      "8 | classifier.fc.6 | Linear       | 771   \n",
      "-------------------------------------------------\n",
      "788 K     Trainable params\n",
      "0         Non-trainable params\n",
      "788 K     Total params\n",
      "3.154     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07cf6c6b23443bda2a98d184eb2d670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fafbde82bc34dd6beca4eb7b01a7256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a9a01b03fd47d4a45535fff974cbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83204f3eceda46d9883fc0fafa29dd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca8cd8f42b546568fda9f6c0897b30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39ed88a14254b91a678c0368d1d9c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ad6e1f7b014d34af3a3de51de75d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bd7a2c5ad84024bc87ae3ca5cbbaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1520c9b9c25e4ff5819c8e28dbce8313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bfd01d95c9474b928734bae3504166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27ecb5b227b4e989569762569f24946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7290a4217ff441e803b560715217d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-11-7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290de4d201867099a7cec8aa5bca78d01a6c85d7bcab7b52f8e97114ad853450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
