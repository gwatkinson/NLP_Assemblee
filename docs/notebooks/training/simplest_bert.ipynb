{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from nlp_assemblee.simple_datasets import get_dataloader, get_single_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "colors = sns.color_palette(\"deep\")\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = x[\"text\"][\"intervention\"]\n",
    "        x_ = self.fc(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_loss(net, loader, length, criterion, prefix=\"val\"):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            targets = targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss_item = loss.item()\n",
    "            test_loss += loss_item * targets.size(0)\n",
    "            loss_list.append(loss_item)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_item = predicted.eq(targets).sum().item()\n",
    "            correct += correct_item\n",
    "            accuracy_list.append(correct_item)\n",
    "\n",
    "    res = {\n",
    "        f\"{prefix}_loss_list\": loss_list,\n",
    "        f\"{prefix}_loss\": test_loss / length,\n",
    "        f\"{prefix}_accuracy_list\": accuracy_list,\n",
    "        f\"{prefix}_accuracy\": correct / length,\n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, loss_baseline=0.98978976, accuracy_baseline=0.39, palette=\"deep\"):\n",
    "    n_epoch = len(results)\n",
    "\n",
    "    epoch_train_loss = [results[i][\"train_loss\"] for i in range(n_epoch)]\n",
    "    epoch_val_loss = [results[i][\"val_loss\"] for i in range(n_epoch)]\n",
    "    batch_train_loss = []\n",
    "    for i in range(n_epoch):\n",
    "        batch_train_loss.extend(results[i][\"train_loss_list\"])\n",
    "    batch_val_loss = []\n",
    "    for i in range(n_epoch):\n",
    "        batch_val_loss.extend(results[i][\"val_loss_list\"])\n",
    "\n",
    "    epoch_train_accuracy = [results[i][\"train_accuracy\"] for i in range(n_epoch)]\n",
    "    epoch_val_accuracy = [results[i][\"val_accuracy\"] for i in range(n_epoch)]\n",
    "    batch_train_accuracy = []\n",
    "    for i in range(n_epoch):\n",
    "        batch_train_accuracy.extend(results[i][\"train_accuracy_list\"])\n",
    "    batch_val_accuracy = []\n",
    "    for i in range(n_epoch):\n",
    "        batch_val_accuracy.extend(results[i][\"val_accuracy_list\"])\n",
    "\n",
    "    epoch_x_range = np.arange(1, n_epoch + 1)\n",
    "    batch_x_range = np.linspace(1, n_epoch + 1, len(batch_train_loss))\n",
    "\n",
    "    colors = sns.color_palette(palette)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].plot(epoch_x_range, epoch_train_loss, label=\"Train loss\", color=colors[0])\n",
    "    axs[0].plot(epoch_x_range, epoch_val_loss, label=\"Validation loss\", color=colors[2])\n",
    "    axs[0].plot(batch_x_range, batch_train_loss, linestyle=\"--\", color=colors[0])\n",
    "    axs[0].plot(batch_x_range, batch_val_loss, linestyle=\"--\", color=colors[2])\n",
    "    axs[0].axhline(y=loss_baseline, linestyle=\"-.\", label=\"Baseline\", color=colors[3])\n",
    "    axs[0].set(title=\"Evolution of the loss\", xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "\n",
    "    axs[1].plot(epoch_x_range, epoch_train_accuracy, label=\"Train accuracy\", color=colors[0])\n",
    "    axs[1].plot(epoch_x_range, epoch_val_accuracy, label=\"Validation accuracy\", color=colors[2])\n",
    "    axs[1].plot(batch_x_range, batch_train_accuracy, linestyle=\"--\", color=colors[0])\n",
    "    axs[1].plot(batch_x_range, batch_val_accuracy, linestyle=\"--\", color=colors[2])\n",
    "    axs[1].axhline(y=accuracy_baseline, linestyle=\"-.\", label=\"Baseline\", color=colors[3])\n",
    "    axs[1].set(title=\"Evolution of the accuracy\", xlabel=\"Epoch\", ylabel=\"Accuracy\")\n",
    "    axs[1].yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(\n",
    "    net,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    loaders,\n",
    "    lengths,\n",
    "    n_epoch=10,\n",
    "    resume_epoch=0,\n",
    "    results={},\n",
    "):\n",
    "    trainloader = loaders[\"train\"]\n",
    "    valloader = loaders[\"val\"]\n",
    "    train_length = lengths[\"train\"]\n",
    "    val_length = lengths[\"val\"]\n",
    "\n",
    "    try:\n",
    "        epoch_pbar = tqdm(total=n_epoch, leave=False)\n",
    "        for epoch in range(n_epoch):\n",
    "            net.train()\n",
    "            train_loss = 0.0\n",
    "            correct = 0\n",
    "\n",
    "            # Train loop\n",
    "            train_res = {\n",
    "                \"train_loss_list\": [],\n",
    "                \"train_accuracy_list\": [],\n",
    "            }\n",
    "\n",
    "            pbar = tqdm(leave=True, total=len(trainloader))\n",
    "            for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Prediction\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                # Loss and step\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Loss and accuracy logging\n",
    "                loss_item = loss.item()\n",
    "                train_loss += loss_item * labels.size(0)\n",
    "                train_res[\"train_loss_list\"].append(loss_item)\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_item = predicted.eq(labels).sum().item()\n",
    "                correct += correct_item\n",
    "                train_res[\"train_accuracy_list\"].append(correct_item / labels.size(0))\n",
    "\n",
    "                pbar.set_description(f\"Train Loss : {loss_item: .4f}\")\n",
    "                pbar.update()\n",
    "\n",
    "            train_res[\"train_loss\"] = train_loss / train_length\n",
    "            train_res[\"train_accuracy\"] = correct / train_length\n",
    "\n",
    "            # Test loss\n",
    "            val_res = compute_test_loss(net, valloader, val_length, criterion, prefix=\"val\")\n",
    "\n",
    "            results[resume_epoch + epoch] = {**train_res, **val_res}\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Train Loss : {train_res[\"train_loss\"]: .4f}'\n",
    "                + f' | Test Loss : {val_res[\"val_loss\"]: .4f}'\n",
    "                + f' | Train Accuracy : {train_res[\"train_accuracy\"]: .2%}'\n",
    "                + f' | Test Accuracy : {val_res[\"val_accuracy\"]: .2%}'\n",
    "            )\n",
    "            epoch_pbar.set_description(\n",
    "                f\"Epoch : {resume_epoch + epoch}/{resume_epoch + n_epoch}\"\n",
    "                + f' | Train Loss : {train_res[\"train_loss\"]: .4f}'\n",
    "                + f' | Test Loss : {val_res[\"val_loss\"]: .4f}'\n",
    "                + f' | Train Accuracy : {train_res[\"train_accuracy\"]: .2%}'\n",
    "                + f' | Test Accuracy : {val_res[\"val_accuracy\"]: .2%}'\n",
    "            )\n",
    "            epoch_pbar.update()\n",
    "\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        fig = plot_results(results)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, loaders, lengths = get_dataloader(\n",
    "    root=\"../../../data/\",\n",
    "    bert_type=\"bert\",\n",
    "    batch_size=256,\n",
    "    text_vars=[\"intervention\"],\n",
    "    use_features=False,\n",
    "    label_var=\"label\",\n",
    "    num_workers=12,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier,\n",
    "        optimizer_type=\"Adam\",\n",
    "        learning_rate=1e-3,\n",
    "        optimizer_kwargs={},\n",
    "        criterion_type=\"CrossEntropyLoss\",\n",
    "        batch_size=256,\n",
    "        loader_kwargs={\n",
    "            \"root\": \"../../../data/\",\n",
    "            \"bert_type\": \"bert\",\n",
    "            \"text_vars\": [\"intervention\"],\n",
    "            \"use_features\": False,\n",
    "            \"label_var\": \"label\",\n",
    "            \"num_workers\": 12,\n",
    "            \"prefetch_factor\": 4,\n",
    "            \"pin_memory\": True,\n",
    "        },\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.criterion_type = criterion_type\n",
    "        self.batch_size = batch_size\n",
    "        self.loader_kwargs = loader_kwargs\n",
    "        self.save_hyperparameters(ignore=[\"classifier\", \"criterion\"])\n",
    "\n",
    "        if criterion_type == \"CrossEntropyLoss\":\n",
    "            self.criterion = nn.functional.cross_entropy\n",
    "\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_type == \"SGD\":\n",
    "            optimizer = optim.SGD(\n",
    "                self.classifier.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n",
    "            )\n",
    "        elif self.optimizer_type == \"Adam\":\n",
    "            optimizer = optim.Adam(\n",
    "                self.classifier.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n",
    "            )\n",
    "        return optimizer\n",
    "\n",
    "    def get_loss(self, batch, model_type=\"train\"):\n",
    "        x, y = batch\n",
    "        z = self.classifier(x)\n",
    "        loss = nn.CrossEntropyLoss()(z, y)\n",
    "        self.log(f\"{model_type}_loss\", loss, prog_bar=(model_type == \"val\"))\n",
    "\n",
    "        _, predicted = z.max(1)\n",
    "        accuracy = predicted.eq(y).sum().item() / y.size(0)\n",
    "        self.log(f\"{model_type}_accuracy\", accuracy, prog_bar=(model_type == \"val\"))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tain_loss = self.get_loss(batch, model_type=\"train\")\n",
    "        return tain_loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        val_loss = self.get_loss(val_batch, model_type=\"val\")\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        test_loss = self.get_loss(test_batch, model_type=\"test\")\n",
    "        return test_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        _, loader, _ = get_single_dataloader(\n",
    "            phase=\"train\", batch_size=self.batch_size, **self.loader_kwargs\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        _, loader, _ = get_single_dataloader(\n",
    "            phase=\"val\", batch_size=self.batch_size, **self.loader_kwargs\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        _, loader, _ = get_single_dataloader(\n",
    "            phase=\"test\", batch_size=self.batch_size, **self.loader_kwargs\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimplestBert()\n",
    "net.train()\n",
    "\n",
    "lit_model = LitModel(\n",
    "    net,\n",
    "    optimizer_type=\"Adam\",\n",
    "    learning_rate=5e-3,\n",
    "    optimizer_kwargs={},\n",
    "    criterion_type=\"CrossEntropyLoss\",\n",
    "    batch_size=512,\n",
    "    loader_kwargs={\n",
    "        \"root\": \"../../../data/\",\n",
    "        \"bert_type\": \"bert\",\n",
    "        \"text_vars\": [\"intervention\"],\n",
    "        \"use_features\": False,\n",
    "        \"label_var\": \"label\",\n",
    "        \"num_workers\": 12,\n",
    "        \"prefetch_factor\": 4,\n",
    "        \"pin_memory\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    # profiler=\"simple\",\n",
    "    max_epochs=100,\n",
    "    default_root_dir=\"../../../\",\n",
    "    # fast_dev_run=True,\n",
    "    # overfit_batches=1,\n",
    "    # auto_scale_batch_size=\"binsearch\",\n",
    "    # auto_lr_find=True,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", check_finite=True, patience=10),\n",
    "        callbacks.ModelSummary(max_depth=-1),\n",
    "        callbacks.Timer(duration=\"00:03:00:00\", interval=\"epoch\"),  # Max three hours\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ../../../lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type         | Params\n",
      "-------------------------------------------------\n",
      "0 | classifier      | SimplestBert | 788 K \n",
      "1 | classifier.fc   | Sequential   | 788 K \n",
      "2 | classifier.fc.0 | Linear       | 393 K \n",
      "3 | classifier.fc.1 | LeakyReLU    | 0     \n",
      "4 | classifier.fc.2 | Linear       | 262 K \n",
      "5 | classifier.fc.3 | LeakyReLU    | 0     \n",
      "6 | classifier.fc.4 | Linear       | 131 K \n",
      "7 | classifier.fc.5 | LeakyReLU    | 0     \n",
      "8 | classifier.fc.6 | Linear       | 771   \n",
      "-------------------------------------------------\n",
      "788 K     Trainable params\n",
      "0         Non-trainable params\n",
      "788 K     Total params\n",
      "3.154     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a6a186222442928bfcc42996411d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fec3319ab74655ad954dc5e6b4c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760507f3e377465abb1b7d0c47a00556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e9117db7044310a8f4300b747651ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c445aa4d71124cbd85c46911b556d156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8de1e621be42a894cd047174c2ec7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed527ed1259846efa109a02a8997cd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf3bd179f3c47a1a03398fea705bd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae93aa14ec9e46b1ba8a949efcc949d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba2c3de50a04dbbbcea9080e0dd4300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697e30b642af41c8a961b36d043d2f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79700161391544969be5af1c4a2ed84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcaef35596540e2b37905f329eeffc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad20d3885be4e41892fff54dd42c728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad92fd40029435fbc735f91d6fb9959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533a79abaac44768a5fc7b9968e51035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d66f60fd8b74ddeb94b4986f9adb8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84a9b2881b44f17884ed58f89e24f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef7a5fe9f9c45ebacd0b39aa48fb19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b59fd588c4c9f8479b27ff9866335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50020d2f733645f1816a629216cdb63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2687bf0e3c2d431596da23c0ca644ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3084db3d6c6a425b94b5287d79ce1aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b614c0d2517840518009ddfd3066542d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dded3df14341cab2d9ffab6bfbbd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5653a9772bc647e9a0f0c612857fcff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f757b80074634843824496f501ce6634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc5ea4907f94ddea4a1a39de3abc177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c789aee40b354fcbb3c86e0a74819b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baf81950aac4d0691a0c460b11bd9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68c033f5cf14a7fae2d85b8302ad5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(lit_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-11-7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290de4d201867099a7cec8aa5bca78d01a6c85d7bcab7b52f8e97114ad853450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
